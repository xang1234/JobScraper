{"id":"MyCareersFuture-178","title":"Search engine access is not thread-safe under run_in_executor","description":"Requests call the shared SemanticSearchEngine via run_in_executor(None, ...) which uses the default ThreadPoolExecutor. The engine mutates _result_cache and _query_cache (cachetools.TTLCache, not thread-safe) and state flags (_loaded, _degraded) without locks. Concurrent searches can race or corrupt caches. 11 endpoints use run_in_executor. Recommended fix: use a single-thread ThreadPoolExecutor(max_workers=1) which serializes all engine access. This is the simplest approach with zero lock-bug risk, and FAISS/numpy still release the GIL so CPU work parallelizes at the native level. Ref: src/api/app.py:68,193,206,229,254,274,295,315,328,349,360","status":"closed","priority":1,"issue_type":"bug","owner":"davidten7@gmail.com","created_at":"2026-02-06T17:57:56.325086+08:00","created_by":"David Ten","updated_at":"2026-02-06T19:49:33.954634+08:00","closed_at":"2026-02-06T19:49:33.954634+08:00","close_reason":"Replaced default ThreadPoolExecutor (None) with a dedicated single-worker _engine_executor = ThreadPoolExecutor(max_workers=1). All 11 run_in_executor calls now serialize through this executor, preventing concurrent mutation of TTLCache and other non-thread-safe engine state."}
{"id":"MyCareersFuture-1xj","title":"Scraper exits on transient 429s instead of backing off","description":"In MCFScraper.scrape() (the search-based scraper), MCFRateLimitError is a subclass of MCFAPIError. The except MCFAPIError block at line 176 catches both, causing the scraper to break out of the fetch loop on any 429. This stops collection on temporary rate limits instead of backing off and retrying. Note: the HistoricalScraper already handles MCFRateLimitError separately with proper backoff (lines 338, 544, 629) — this bug is only in MCFScraper. Fix: catch MCFRateLimitError before MCFAPIError and implement backoff/retry. Ref: src/mcf/scraper.py:170-178","status":"closed","priority":1,"issue_type":"bug","owner":"davidten7@gmail.com","created_at":"2026-02-06T17:58:00.652839+08:00","created_by":"David Ten","updated_at":"2026-02-06T19:53:43.749981+08:00","closed_at":"2026-02-06T19:53:43.749981+08:00","close_reason":"Caught MCFRateLimitError before MCFAPIError in MCFScraper.scrape() to back off and retry instead of exiting the loop"}
{"id":"MyCareersFuture-2jv","title":"Set up test infrastructure and fixtures","description":"# Task: Set Up Test Infrastructure and Fixtures\n\n## What\nCreate test infrastructure including:\n- Pytest configuration\n- Test fixtures for database, embeddings, and FAISS indexes\n- Mock data generators\n- Integration test helpers\n\n## Files to Create\n\n### 1. tests/conftest.py\n```python\n\"\"\"\nShared pytest fixtures for all tests.\n\"\"\"\nimport pytest\nimport tempfile\nimport numpy as np\nfrom pathlib import Path\n\nfrom src.mcf.database import MCFDatabase\nfrom src.mcf.models import Job, Salary, Company, SalaryType\nfrom src.mcf.embeddings import EmbeddingGenerator, FAISSIndexManager\n\n@pytest.fixture\ndef temp_dir():\n    \"\"\"Provide a temporary directory for test files.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        yield Path(tmpdir)\n\n@pytest.fixture\ndef test_db(temp_dir):\n    \"\"\"Provide a test database with sample data.\"\"\"\n    db_path = temp_dir / \"test.db\"\n    db = MCFDatabase(str(db_path))\n    \n    # Insert sample jobs\n    for i, job in enumerate(generate_test_jobs(20)):\n        db.upsert_job(job)\n    \n    yield db\n\n@pytest.fixture\ndef test_embeddings(temp_dir, test_db):\n    \"\"\"Generate embeddings for test jobs.\"\"\"\n    generator = EmbeddingGenerator()\n    \n    jobs = test_db.get_all_jobs()\n    for job in jobs:\n        embedding = generator.generate_job_embedding(job)\n        test_db.upsert_embedding(job.uuid, \"job\", embedding)\n    \n    yield test_db\n\n@pytest.fixture\ndef test_index(temp_dir, test_embeddings):\n    \"\"\"Build FAISS index for test data.\"\"\"\n    index_dir = temp_dir / \"embeddings\"\n    manager = FAISSIndexManager(index_dir)\n    \n    uuids, embeddings = test_embeddings.get_all_embeddings(\"job\")\n    manager.build_job_index(embeddings, uuids)\n    manager.save()\n    \n    yield manager\n\n@pytest.fixture\ndef mock_search_engine(test_db, test_index):\n    \"\"\"Provide a configured search engine for integration tests.\"\"\"\n    from src.mcf.embeddings import SemanticSearchEngine\n    \n    engine = SemanticSearchEngine(\n        db_path=str(test_db.db_path),\n        index_dir=test_index.index_dir\n    )\n    \n    # Load synchronously for tests\n    import asyncio\n    asyncio.get_event_loop().run_until_complete(engine.load())\n    \n    yield engine\n```\n\n### 2. tests/factories.py\n```python\n\"\"\"\nTest data factories.\n\"\"\"\nimport uuid\nfrom datetime import date, timedelta\nimport random\n\nfrom src.mcf.models import Job, Salary, Company, SalaryType, JobMetadata, Skill, Category\n\n# Sample data pools\nTITLES = [\n    \"Data Scientist\", \"ML Engineer\", \"Software Engineer\", \n    \"Data Analyst\", \"Backend Developer\", \"DevOps Engineer\",\n    \"Product Manager\", \"UX Designer\", \"QA Engineer\",\n    \"Full Stack Developer\", \"Data Engineer\", \"AI Researcher\"\n]\n\nCOMPANIES = [\n    (\"Google\", \"T12345678A\"),\n    (\"Meta\", \"T23456789B\"),\n    (\"Amazon\", \"T34567890C\"),\n    (\"Microsoft\", \"T45678901D\"),\n    (\"Grab\", \"T56789012E\"),\n    (\"Shopee\", \"T67890123F\"),\n]\n\nSKILLS_POOL = [\n    \"Python\", \"Java\", \"JavaScript\", \"SQL\", \"AWS\",\n    \"Machine Learning\", \"TensorFlow\", \"PyTorch\", \"Docker\", \"Kubernetes\",\n    \"React\", \"Node.js\", \"PostgreSQL\", \"MongoDB\", \"Redis\",\n    \"Git\", \"CI/CD\", \"Agile\", \"Scrum\", \"REST API\"\n]\n\ndef generate_test_job(\n    title: str = None,\n    company: str = None,\n    salary_min: int = None,\n    salary_max: int = None,\n    skills: list[str] = None\n) -\u003e Job:\n    \"\"\"Generate a single test job with optional overrides.\"\"\"\n    job_uuid = str(uuid.uuid4())\n    \n    if title is None:\n        title = random.choice(TITLES)\n    \n    company_name, company_uen = random.choice(COMPANIES) if company is None else (company, None)\n    \n    if salary_min is None:\n        salary_min = random.randint(4000, 15000)\n    if salary_max is None:\n        salary_max = salary_min + random.randint(1000, 5000)\n    \n    if skills is None:\n        skills = random.sample(SKILLS_POOL, random.randint(3, 8))\n    \n    return Job(\n        uuid=job_uuid,\n        title=title,\n        description=f\"We are looking for a {title} to join our team at {company_name}.\",\n        salary=Salary(\n            minimum=salary_min,\n            maximum=salary_max,\n            type=SalaryType(salaryType=\"Monthly\")\n        ),\n        postedCompany=Company(name=company_name, uen=company_uen),\n        skills=[Skill(skill=s) for s in skills],\n        categories=[Category(category=\"Technology\")],\n        metadata=JobMetadata(\n            newPostingDate=(date.today() - timedelta(days=random.randint(0, 30))).isoformat(),\n            totalNumberJobApplication=random.randint(0, 100)\n        )\n    )\n\ndef generate_test_jobs(n: int) -\u003e list[Job]:\n    \"\"\"Generate n test jobs.\"\"\"\n    return [generate_test_job() for _ in range(n)]\n\ndef generate_similar_jobs(base_title: str, n: int) -\u003e list[Job]:\n    \"\"\"Generate n jobs with similar titles (for similarity testing).\"\"\"\n    variations = [\n        f\"Senior {base_title}\",\n        f\"Junior {base_title}\",\n        f\"{base_title} Lead\",\n        f\"{base_title} Manager\",\n        f\"Principal {base_title}\",\n    ]\n    \n    jobs = []\n    for i in range(n):\n        title = variations[i % len(variations)]\n        jobs.append(generate_test_job(title=title))\n    \n    return jobs\n```\n\n### 3. tests/test_integration.py\n```python\n\"\"\"\nIntegration tests for the full search pipeline.\n\"\"\"\nimport pytest\nfrom src.api.models import SearchRequest, SimilarJobsRequest\n\n@pytest.mark.asyncio\nasync def test_full_search_pipeline(mock_search_engine):\n    \"\"\"Test complete search flow: query → embedding → FAISS → results.\"\"\"\n    request = SearchRequest(query=\"data scientist\", limit=5)\n    response = await mock_search_engine.search(request)\n    \n    assert len(response.results) \u003c= 5\n    assert response.search_time_ms \u003e 0\n    assert all(r.similarity_score \u003e= 0 for r in response.results)\n\n@pytest.mark.asyncio\nasync def test_similar_jobs_pipeline(mock_search_engine, test_db):\n    \"\"\"Test similar jobs flow.\"\"\"\n    # Get a job UUID\n    jobs = test_db.search_jobs(limit=1)\n    job_uuid = jobs[0]['uuid']\n    \n    request = SimilarJobsRequest(job_uuid=job_uuid, limit=5)\n    response = await mock_search_engine.find_similar(request)\n    \n    # Should not include the source job itself\n    assert job_uuid not in [r.uuid for r in response.results]\n    assert len(response.results) \u003c= 5\n\n@pytest.mark.asyncio\nasync def test_filter_then_search(mock_search_engine):\n    \"\"\"Test SQL filtering combined with semantic search.\"\"\"\n    request = SearchRequest(\n        query=\"engineer\",\n        salary_min=10000,\n        limit=10\n    )\n    response = await mock_search_engine.search(request)\n    \n    # All results should meet salary filter\n    for r in response.results:\n        if r.salary_min:\n            assert r.salary_min \u003e= 10000\n```\n\n### 4. pytest.ini\n```ini\n[pytest]\ntestpaths = tests\nasyncio_mode = auto\npython_files = test_*.py\npython_functions = test_*\nmarkers =\n    slow: marks tests as slow (deselect with '-m \"not slow\"')\n    integration: marks tests as integration tests\n```\n\n## Running Tests\n\n```bash\n# Run all tests\npytest\n\n# Run with coverage\npytest --cov=src --cov-report=html\n\n# Run only fast tests\npytest -m \"not slow\"\n\n# Run specific test file\npytest tests/test_embeddings.py -v\n\n# Run with verbose output\npytest -v --tb=short\n```\n\n## Testing Strategy\n\n| Component | Test Type | Description |\n|-----------|-----------|-------------|\n| EmbeddingGenerator | Unit | Verify embedding shape, normalization |\n| FAISSIndexManager | Unit | Verify index build, save/load, search |\n| Database methods | Unit | Verify CRUD operations |\n| SemanticSearchEngine | Integration | Full search pipeline |\n| API endpoints | Integration | HTTP request/response |\n\n## Dependencies\n- Should be done early in Phase 1 to enable TDD\n- All subsequent tasks should include tests","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T20:18:53.561974+08:00","created_by":"David Ten","updated_at":"2026-02-06T00:45:20.925432+08:00","closed_at":"2026-02-06T00:45:20.925432+08:00","close_reason":"Created test infrastructure with pytest config, fixtures, factories, and 105 passing tests","dependencies":[{"issue_id":"MyCareersFuture-2jv","depends_on_id":"MyCareersFuture-qwl.1.1","type":"blocks","created_at":"2026-02-05T20:19:39.5516+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-2jv","depends_on_id":"MyCareersFuture-qwl.1","type":"parent-child","created_at":"2026-02-05T20:25:23.742682+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-54s","title":"Docker Compose Deployment for MCF Semantic Search","description":"# Docker Compose Deployment for MCF Semantic Search\n\n## Background \u0026 Motivation\n\nThe MCF Semantic Search system is feature-complete. All 6 development phases are closed:\n- Phase 1: Core embedding generation (sentence-transformers, all-MiniLM-L6-v2)\n- Phase 2: FAISS index building (IVFFlat for jobs, Flat for skills/companies)\n- Phase 3: SemanticSearchEngine with hybrid BM25+vector scoring\n- Phase 4: FastAPI REST API with async handlers, caching, rate limiting\n- Phase 5: Skill clustering/search and company multi-centroid similarity\n- Phase 6: React 19 frontend with Vite, Tailwind CSS 4, TanStack Query\n\nThe system currently runs via two manual processes: `python -m src.cli api-serve` (backend on :8000) and `npm run dev` in src/frontend/ (Vite dev server on :3000 with proxy). This is fragile, undocumented, and unsuitable for long-running deployment.\n\n## Why Docker Compose (Not Kubernetes, Not Bare Metal)\n\n**vs. Kubernetes**: Two services (API + static files) do not justify a container orchestrator. K8s adds etcd, kubelet, API server overhead — overkill for a homelab.\n\n**vs. Bare Metal**: The backend has heavy, version-sensitive dependencies: torch (\u003e=2.0,\u003c2.3), faiss-cpu (^1.7.4), sentence-transformers, numpy, scipy, scikit-learn. Installing these on bare metal risks conflicts with other Python projects. Docker provides hermetic isolation.\n\n**Docker Compose** is the sweet spot: declarative service definitions, health checks, restart policies, resource limits, log rotation — all in a single YAML file. Works identically on macOS (dev laptop) and Linux (homelab server).\n\n## Architecture: Two Containers + Shared Volume\n\n```\n┌─────────────────────────────────────────────────┐\n│  Docker Compose Network                         │\n│                                                 │\n│  ┌──────────────┐       ┌───────────────────┐   │\n│  │  frontend     │:3000 │   backend          │   │\n│  │  (nginx)      │─────▶│   (uvicorn)        │   │\n│  │  static files │/api/ │   FastAPI + FAISS   │   │\n│  │  + reverse    │proxy │   + sentence-xfmrs  │   │\n│  │  proxy        │      │   port 8000         │   │\n│  └──────────────┘       └──────┬──────────────┘   │\n│                                │                  │\n│               ┌────────────────┼────────┐         │\n│               ▼                ▼        ▼         │\n│          mcf-data         hf-cache               │\n│          (SQLite DB +     (embedding model       │\n│           FAISS indexes)   ~100MB, cached)       │\n└─────────────────────────────────────────────────┘\n```\n\n**Frontend (nginx)**: Serves the Vite-built React SPA as static files (~400KB). Reverse-proxies /api/*, /health, /docs to the backend. This is superior to serving static files from FastAPI because nginx is purpose-built for this (lower memory, higher throughput) and isolates the frontend from backend restarts.\n\n**Backend (uvicorn)**: Single-worker FastAPI process running the SemanticSearchEngine. Loads FAISS indexes into RAM at startup (~1.5GB for 100K jobs), lazy-loads the sentence-transformers model on first search (~200MB). Total memory: ~2-3GB. Uses run_in_executor to offload synchronous search operations to a thread pool.\n\n## Volume Strategy\n\nData is NOT baked into Docker images. It is mounted as volumes:\n- **mcf-data** → /app/data (SQLite DB at 542MB + FAISS indexes + skill clusters)\n- **hf-cache** → /app/.cache/huggingface (embedding model, downloaded once)\n\nNamed volumes for local dev (Docker-managed, portable). Bind mounts for homelab (direct host access for backups).\n\n## Running Indefinitely: Analysis\n\nThe system CAN run indefinitely with restart: unless-stopped. Threat model:\n\n| Threat | Severity | Mitigation |\n|--------|----------|------------|\n| Python memory fragmentation | Low | Docker memory limit (3G) + auto-restart on OOM |\n| Analytics table unbounded growth | Low | ~7MB/year at 100 queries/day; monthly prune keeps bounded |\n| Docker log disk exhaustion | None | json-file driver with max-size:10m, max-file:5 |\n| SQLite corruption on power loss | Very Low | Short transactions (analytics INSERT); weekly backup |\n| Stale FAISS after new data scrape | Manual | Restart backend after embed-generate |\n| HuggingFace model unavailable offline | One-time | Volume persists model; warm cache before going offline |\n| Container crash | Handled | restart: unless-stopped auto-recovers |\n| Host reboot | Handled | Docker starts on boot + restart policy |\n\n## Maintenance Schedule\n\n- **Weekly**: Backup SQLite DB (simple file copy — safe while running due to short write transactions)\n- **Monthly**: Prune search_analytics older than 90 days (DELETE WHERE searched_at \u003c datetime('now','-90 days'))\n- **Quarterly**: Rebuild images for security patches (docker compose build --no-cache); VACUUM database\n- **On new data**: Run embed-generate inside container → restart backend to reload FAISS indexes\n\n## Key Technical Decisions\n\n1. **CPU-only torch** (~200MB vs ~2GB): No GPU usage anywhere — FAISS is faiss-cpu, embedding inference is CPU-only\n2. **Single uvicorn worker**: Each worker duplicates FAISS indexes in RAM; 1 worker = ~2.5GB, 2 workers = ~5GB\n3. **python:3.11-slim** (not alpine): Alpine's musl libc breaks numpy/scipy/torch wheels\n4. **nginx:1.27-alpine** for frontend: Tiny image (~25MB), purpose-built for static serving\n5. **start-period=60s** on health check: FAISS loading takes 5-30s, model lazy-loads on first search\n\n## Files Created by This Epic\n\n| File | Purpose |\n|------|---------|\n| .dockerignore | Build context exclusions |\n| docker/backend.Dockerfile | Multi-stage Python build |\n| docker/frontend.Dockerfile | Multi-stage Node→nginx build |\n| docker/nginx.conf | Reverse proxy + SPA fallback |\n| docker-compose.yml | Base config (local dev) |\n| docker-compose.prod.yml | Homelab overrides |\n| docker/bootstrap-data.sh | Data seeding helper |\n\n## Exit Criteria\n\n- [ ] Both images build without errors\n- [ ] docker compose up starts healthy services\n- [ ] Search works end-to-end through nginx proxy at :3000\n- [ ] /health returns {\"status\":\"healthy\",\"index_loaded\":true,\"degraded\":false}\n- [ ] Production config applies resource limits and restart policies\n- [ ] Data persists across container restarts\n- [ ] System auto-recovers from forced container kill\n- [ ] Logs are rotated and capped","status":"closed","priority":1,"issue_type":"epic","owner":"davidten7@gmail.com","created_at":"2026-02-06T15:53:27.265726+08:00","created_by":"David Ten","updated_at":"2026-02-06T17:18:49.704998+08:00","closed_at":"2026-02-06T17:18:49.704998+08:00","close_reason":"All 8 subtasks complete. Docker Compose deployment verified E2E: images build, services start with health-gated ordering, search works through nginx, volumes persist, prod config applies resource limits and restart policies."}
{"id":"MyCareersFuture-54s.1","title":"Create .dockerignore for build context control","description":"# Create .dockerignore for Build Context Control\n\n## What\nCreate a .dockerignore file at the project root to control what files are sent to the Docker daemon during image builds.\n\n## Why This Matters\n\nWhen you run `docker build`, Docker sends the entire directory tree (the \"build context\") to the daemon before executing any Dockerfile instructions. Without .dockerignore, this project would send:\n- .git/ directory (~100MB+ of git history)\n- data/mcf_jobs.db (542MB SQLite database)\n- data/embeddings/ (FAISS indexes, potentially GBs)\n- src/frontend/node_modules/ (hundreds of MBs if installed locally)\n- __pycache__/ directories scattered throughout\n- .beads/ issue tracker data\n\nThis wastes build time (minutes copying GBs over the Docker socket), inflates the build cache, and risks accidentally baking sensitive or large runtime data into image layers.\n\n## What to Exclude and Why\n\n```\n# Version control — history not needed in containers\n.git\n\n# Python virtual environments — deps installed fresh in container\n.venv\nvenv\n\n# Python bytecode — regenerated at runtime\n__pycache__\n*.pyc\n.pytest_cache\n.mypy_cache\n\n# Runtime data — mounted as volumes, NOT baked into images\n# This is a CRITICAL design decision: images are stateless,\n# data lives in Docker volumes\ndata/\n\n# Frontend build artifacts — rebuilt in container\nsrc/frontend/node_modules\nsrc/frontend/dist\n\n# Legacy code — not used by the API/frontend\nsrc/legacy\n\n# OS and editor artifacts\n.DS_Store\n*.swp\n*.swo\n\n# Secrets — must never enter an image layer\n.env\n*.pem\n*.key\n\n# Issue tracker — not needed in container\n.beads\n.bv\n```\n\n## Critical Design Decision: data/ Excluded Entirely\n\nThe SQLite database and FAISS indexes are NOT included in the Docker image. They are mounted at runtime via Docker volumes. This means:\n\n1. **Images are reproducible and small** — no 542MB database baked in\n2. **Data persists across image rebuilds** — update code without losing data\n3. **Different environments mount different datasets** — dev vs prod data\n4. **First run requires bootstrapping** — see task 54s.7 for the data seeding workflow\n\nThis is the standard \"stateless container + persistent volume\" pattern. The container is disposable; the data volume is precious.\n\n## File Location\n`.dockerignore` at project root: `/Users/admin/Documents/Work/MyCareersFuture/.dockerignore`\n\n## Implementation Notes\n- Docker reads .dockerignore before processing any Dockerfile\n- Patterns follow .gitignore syntax (glob matching, ! for negation)\n- This file must exist before ANY docker build command works efficiently\n- Both backend and frontend Dockerfiles benefit from the same .dockerignore since both use `context: .` (project root) in docker-compose.yml","status":"closed","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-06T15:53:31.504085+08:00","created_by":"David Ten","updated_at":"2026-02-06T16:07:08.784614+08:00","closed_at":"2026-02-06T16:07:08.784614+08:00","close_reason":"Created .dockerignore and docker/nginx.conf per task specs","dependencies":[{"issue_id":"MyCareersFuture-54s.1","depends_on_id":"MyCareersFuture-54s","type":"parent-child","created_at":"2026-02-06T15:53:31.506447+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-54s.2","title":"Create backend Dockerfile (multi-stage, CPU-only torch)","description":"# Create Backend Dockerfile (Multi-Stage, CPU-Only Torch)\n\n## What\nCreate `docker/backend.Dockerfile` — a multi-stage Dockerfile that builds the FastAPI semantic search API into a production-ready container with minimal image size.\n\n## Why Multi-Stage Build\n\nThe build process needs compilers (gcc, g++) for native Python extensions (numpy, scipy, faiss-cpu), but the runtime does not. Multi-stage keeps the final image clean:\n\n**Stage 1 (builder)** — python:3.11-slim + gcc + g++\n- Install Poetry, export requirements.txt\n- Install CPU-only PyTorch from dedicated wheel index\n- Install all remaining Python dependencies\n\n**Stage 2 (runtime)** — python:3.11-slim (clean)\n- Copy ONLY the installed site-packages from Stage 1\n- Copy application source code (src/)\n- Install minimal runtime deps: libgomp1 (OpenMP for FAISS), curl (healthcheck)\n- No compilers, no Poetry, no build tools\n\n## Critical Decision: CPU-Only PyTorch\n\nThe standard `torch` pip package includes NVIDIA CUDA libraries, weighing ~2GB. This system uses NO GPU:\n- FAISS is the `faiss-cpu` variant (CPU-only vector search)\n- sentence-transformers runs embedding inference on CPU\n- No model training, only forward-pass inference on 384-dimensional vectors\n\nUsing `torch==2.2.2+cpu` from `https://download.pytorch.org/whl/cpu` saves ~1.5GB, bringing the image from ~4.5GB to ~3GB. The pyproject.toml constraint `torch \u003e=2.0.0,\u003c2.3.0` is satisfied by 2.2.2+cpu.\n\n## Why python:3.11-slim (Not 3.10, Not Alpine)\n\n- **3.11 over 3.10**: CPython 3.11 has 10-60% faster execution for CPU-bound code. The search engine is CPU-bound (FAISS search, BM25 scoring, numpy operations). pyproject.toml requires ^3.10, so 3.11 is compatible.\n- **slim over full**: ~150MB vs ~900MB base. Slim omits docs, man pages, and some dev headers. Sufficient for runtime since native extensions are pre-compiled in Stage 1.\n- **NOT alpine**: Alpine Linux uses musl libc instead of glibc. Pre-built wheels for numpy, scipy, torch, faiss-cpu expect glibc. On alpine, these packages compile from source (adding 20+ minutes to build time) or fail entirely. The slim variant uses Debian with glibc — all wheels install instantly.\n\n## Runtime Dependencies\n\n`libgomp1` — GNU OpenMP runtime library. FAISS uses OpenMP for multi-threaded parallel search across index partitions. Without it, FAISS crashes at import time with: `ImportError: libgomp.so.1: cannot open shared object file`.\n\n`curl` — Used by the Docker HEALTHCHECK command. Lighter than installing a Python health check script.\n\n## Health Check Configuration\n\n```dockerfile\nHEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \\\n    CMD curl -f http://localhost:8000/health || exit 1\n```\n\n**start-period=60s** is critical. During startup:\n1. uvicorn starts and calls the lifespan handler (~1s)\n2. SemanticSearchEngine.load() runs in a thread executor — loads FAISS indexes from disk (~5-30s depending on index size)\n3. On the FIRST actual search request, sentence-transformers lazy-loads the model (~10-15s)\n\nThe 60s start period means health check failures during this window don't count toward the 3 retries. Without it, the container would be marked unhealthy and restarted in a loop.\n\n## Single Worker: Memory vs. Concurrency Trade-off\n\n```dockerfile\nCMD [\"uvicorn\", \"src.api.app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--workers\", \"1\"]\n```\n\nEach uvicorn worker is a separate OS process with its own memory space:\n- FAISS job index: ~1.5GB in RAM (for 100K jobs × 384 dims × 4 bytes, plus IVF overhead)\n- sentence-transformers model: ~200MB\n- TTL caches, Python interpreter, etc.: ~300MB\n- **Total per worker: ~2-2.5GB**\n\nWith 2 workers: ~4-5GB. With 4 workers: ~8-10GB. On a homelab with 16-32GB total RAM, even 2 workers is expensive.\n\nConcurrency is still good with 1 worker because:\n- FastAPI is async — uvicorn's event loop handles many concurrent connections\n- Blocking search operations run in a thread pool executor (ThreadPoolExecutor)\n- Default thread pool size is min(32, os.cpu_count() + 4)\n- A search taking 200ms means the thread pool can handle ~5 searches/second per thread\n\n## Environment Variables\n\n```dockerfile\nENV MCF_DB_PATH=/app/data/mcf_jobs.db \\\n    MCF_INDEX_DIR=/app/data/embeddings \\\n    MCF_CORS_ORIGINS=* \\\n    MCF_RATE_LIMIT_RPM=100 \\\n    HF_HOME=/app/.cache/huggingface \\\n    TRANSFORMERS_CACHE=/app/.cache/huggingface\n```\n\n- `MCF_DB_PATH` / `MCF_INDEX_DIR`: Read by create_app() in src/api/app.py (lines 112-114)\n- `MCF_CORS_ORIGINS=*`: Permissive because nginx is the public-facing layer; direct backend access is only for debugging\n- `HF_HOME` + `TRANSFORMERS_CACHE`: Both point to the mounted volume. HuggingFace/transformers libraries check these env vars for the model cache directory. Setting both ensures the model downloads into the persistent volume, not a container-local path that disappears on rebuild.\n\n## Estimated Image Size: ~2.5-3GB\n\n| Component | Size |\n|-----------|------|\n| python:3.11-slim base | ~150MB |\n| torch (CPU-only) | ~200MB |\n| sentence-transformers | ~130MB |\n| faiss-cpu | ~50MB |\n| numpy + scipy + scikit-learn | ~350MB |\n| Other deps (FastAPI, uvicorn, httpx, pydantic, rich, pandas) | ~100MB |\n| Application code (src/) | ~1MB |\n\n## Key Source Files Referenced\n- `pyproject.toml` — dependency versions and constraints\n- `src/api/app.py` — uvicorn entry point (src.api.app:app), env var reading (lines 112-120), lifespan handler (lines 55-79)\n- `src/mcf/embeddings/search_engine.py` — memory usage patterns, load() behavior, lazy model loading\n- `src/mcf/embeddings/index_manager.py` — FAISS index loading, libgomp1 dependency\n\n## File Location\n`docker/backend.Dockerfile` at: `/Users/admin/Documents/Work/MyCareersFuture/docker/backend.Dockerfile`","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-06T15:53:32.416002+08:00","created_by":"David Ten","updated_at":"2026-02-06T16:10:54.581021+08:00","closed_at":"2026-02-06T16:10:54.581021+08:00","close_reason":"Created multi-stage Dockerfiles for backend (CPU-only torch) and frontend (node→nginx)","dependencies":[{"issue_id":"MyCareersFuture-54s.2","depends_on_id":"MyCareersFuture-54s","type":"parent-child","created_at":"2026-02-06T15:53:32.417284+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-54s.2","depends_on_id":"MyCareersFuture-54s.1","type":"blocks","created_at":"2026-02-06T15:53:44.373803+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-54s.3","title":"Create nginx reverse proxy and static serving config","description":"# Create nginx Reverse Proxy and Static Serving Config\n\n## What\nCreate `docker/nginx.conf` that configures nginx to serve the React frontend as static files and reverse-proxy API requests to the FastAPI backend container.\n\n## Why nginx Instead of Serving Static Files from FastAPI\n\nThe FastAPI backend process is memory-constrained (~2-3GB for FAISS indexes + embedding model). Adding static file serving (FastAPI's StaticFiles mount) would:\n- Increase the Python process memory footprint\n- Mix concerns: API logic + file serving in the same process\n- Lose nginx's highly-optimized static file serving (sendfile syscall, memory-mapped I/O, connection pooling)\n- Make the frontend unavailable during backend restarts or crashes\n\nnginx is purpose-built for exactly this: ~10-30MB memory footprint, handles thousands of concurrent connections for static files, efficiently proxies API traffic, and provides gzip compression + security headers.\n\n## Proxy Routes Must Match Vite Dev Proxy\n\nThe frontend's vite.config.ts defines dev-time proxies:\n```typescript\n// src/frontend/vite.config.ts\nproxy: {\n  '/api': 'http://localhost:8000',\n  '/health': 'http://localhost:8000',\n}\n```\n\nThe frontend's API client uses relative URLs (baseURL: '/' in src/frontend/src/services/api.ts). In development, Vite intercepts /api/* requests and proxies them to the backend. In production, nginx must replicate this EXACTLY:\n\n```nginx\nlocation /api/   → proxy_pass http://backend:8000   # All API endpoints\nlocation /health → proxy_pass http://backend:8000   # Health check\nlocation /docs   → proxy_pass http://backend:8000   # FastAPI Swagger UI (debugging)\nlocation /openapi.json → proxy_pass http://backend:8000  # OpenAPI schema\n```\n\nNote: `backend` resolves via Docker Compose's internal DNS (service name → container IP).\n\n## SPA Fallback for Client-Side Routing\n\n```nginx\nlocation / {\n    try_files $uri $uri/ /index.html;\n}\n```\n\nThe React frontend uses react-router-dom for client-side routing. If a user navigates to a deep link like `/search?q=python` directly (bookmarked, shared link, or page refresh), nginx must serve index.html so React can parse the URL and render the correct component. Without try_files, direct URL access returns 404.\n\nThe order matters: try_files first checks for an actual file ($uri), then a directory ($uri/), then falls back to index.html. This means /assets/index-abc123.js serves the real file, while /search serves the SPA.\n\n## Proxy Headers\n\n```nginx\nproxy_set_header Host $host;\nproxy_set_header X-Real-IP $remote_addr;\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\nproxy_set_header X-Forwarded-Proto $scheme;\n```\n\nThese headers pass the original client information through to the backend. The backend's RateLimitMiddleware reads X-Forwarded-For to identify clients by IP (otherwise all requests appear to come from the nginx container's IP).\n\n## Timeout Configuration\n\n```nginx\nproxy_read_timeout 30s;    # Generous for slow semantic searches\nproxy_connect_timeout 5s;  # Backend should accept connections quickly\n```\n\nTypical search time is 50-300ms. The 30s read timeout is deliberately generous to handle:\n- Cold start searches (first request triggers model loading: ~10-15s)\n- Complex searches with large result sets\n- Degraded mode keyword-only searches on large databases\n\nThe 5s connect timeout catches backend-down scenarios quickly.\n\n## Gzip Compression\n\n```nginx\ngzip on;\ngzip_types text/plain text/css application/json application/javascript text/xml;\ngzip_min_length 1000;\n```\n\nCompresses API JSON responses and static assets. The React build output is already minified but not gzipped — nginx handles this transparently. JSON search responses (containing job descriptions) can be large; gzip typically achieves 70-80% compression on JSON.\n\n## Security Headers\n\n```nginx\nadd_header X-Frame-Options DENY;            # Prevent embedding in iframes (clickjacking)\nadd_header X-Content-Type-Options nosniff;  # Prevent MIME type sniffing\nadd_header X-XSS-Protection \"1; mode=block\"; # Legacy XSS protection\n```\n\nBasic security hardening. Not a complete security setup (no CSP, no HSTS since we're on HTTP for homelab), but covers the most common vectors.\n\n## File Location\n`docker/nginx.conf` at: `/Users/admin/Documents/Work/MyCareersFuture/docker/nginx.conf`\n\n## Key Source Files Referenced\n- `src/frontend/vite.config.ts` — proxy routes to replicate (lines with proxy config)\n- `src/frontend/src/services/api.ts` — API paths (baseURL: '/', all endpoint paths)\n- `src/api/app.py` — endpoint routes (/api/search, /api/similar, /health, /docs)","status":"closed","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-06T15:53:33.290371+08:00","created_by":"David Ten","updated_at":"2026-02-06T16:07:08.840154+08:00","closed_at":"2026-02-06T16:07:08.840154+08:00","close_reason":"Created .dockerignore and docker/nginx.conf per task specs","dependencies":[{"issue_id":"MyCareersFuture-54s.3","depends_on_id":"MyCareersFuture-54s","type":"parent-child","created_at":"2026-02-06T15:53:33.294515+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-54s.4","title":"Create frontend Dockerfile (multi-stage node build to nginx)","description":"# Create Frontend Dockerfile (Multi-Stage Node Build → nginx)\n\n## What\nCreate `docker/frontend.Dockerfile` — a multi-stage Dockerfile that builds the React frontend with Vite and serves the output via nginx.\n\n## Stage 1: Build (node:22-alpine)\n\n```dockerfile\nFROM node:22-alpine AS builder\nWORKDIR /app\nCOPY src/frontend/package.json src/frontend/package-lock.json ./\nRUN npm ci\nCOPY src/frontend/ ./\nRUN npm run build\n```\n\n**Why node:22-alpine for build**: The project's .nvmrc specifies Node 22. Alpine is fine for the build stage — no native modules need glibc. Alpine saves ~800MB vs the full node image, and this stage is discarded after build.\n\n**Layer caching strategy**: Copy package.json + package-lock.json FIRST, then run npm ci, THEN copy source code. This is a critical Docker optimization:\n- If only source code changes (common case): npm ci layer is cached, only the build reruns (~10-20s)\n- If dependencies change: npm ci reruns (~30-60s), then build\n- Without this ordering, every source change triggers a full npm install\n\n**npm ci vs npm install**: `npm ci` is deterministic — it installs exactly what's in package-lock.json, removes node_modules first, and fails if lock file is out of sync. This ensures reproducible builds.\n\n**Build command**: `npm run build` runs `tsc -b \u0026\u0026 vite build`:\n1. `tsc -b` — TypeScript compilation (type checking only, Vite handles the actual bundling)\n2. `vite build` — Bundles, minifies, tree-shakes into dist/ with hashed filenames\n\n## Stage 2: Serve (nginx:1.27-alpine)\n\n```dockerfile\nFROM nginx:1.27-alpine AS runtime\nRUN rm /etc/nginx/conf.d/default.conf\nCOPY docker/nginx.conf /etc/nginx/conf.d/default.conf\nCOPY --from=builder /app/dist /usr/share/nginx/html\nEXPOSE 80\nHEALTHCHECK --interval=30s --timeout=5s --retries=3 \\\n    CMD curl -f http://localhost:80/ || exit 1\n```\n\n**Why nginx:1.27-alpine**: ~25MB image. Purpose-built for serving static files. Alpine is fine here — nginx has no glibc dependency issues.\n\n**Remove default.conf**: nginx ships with a default server block that would conflict with our custom config.\n\n**Health check**: Simple — if nginx can serve the index page, it's healthy. No start-period needed because nginx starts in \u003c1 second (unlike the backend's 30-60 second FAISS loading).\n\n## Build Context Path Consideration\n\nThe Dockerfile uses paths relative to the Docker build context (project root), NOT relative to the Dockerfile location:\n- `COPY src/frontend/package.json ./` — works because context is `.` (project root)\n- `COPY docker/nginx.conf /etc/nginx/conf.d/default.conf` — the nginx.conf is in docker/, accessible from project root context\n\nIn docker-compose.yml, this means:\n```yaml\nfrontend:\n  build:\n    context: .                         # Project root\n    dockerfile: docker/frontend.Dockerfile  # Path to Dockerfile\n```\n\n## Estimated Image Size: ~50MB\n\n| Component | Size |\n|-----------|------|\n| nginx:1.27-alpine base | ~25MB |\n| Built React app (JS + CSS + HTML) | ~400KB |\n| curl (for healthcheck) | ~5MB |\n| nginx config | ~1KB |\n\nThis is ~60x smaller than the backend image. Static file serving has minimal overhead.\n\n## What the Build Output Contains\n\nAfter `vite build`, the dist/ directory contains:\n- `index.html` — Entry point with hashed asset references\n- `assets/index-[hash].js` — Bundled React application (~200KB minified)\n- `assets/index-[hash].css` — Bundled Tailwind CSS (~50KB)\n- Hashed filenames enable aggressive browser caching (immutable content)\n\n## Dependency on nginx.conf (Task 54s.3)\n\nThis Dockerfile COPY's the nginx.conf created in task 54s.3. The nginx.conf must exist before this Dockerfile can build. This dependency is explicit in the beads graph.\n\n## File Location\n`docker/frontend.Dockerfile` at: `/Users/admin/Documents/Work/MyCareersFuture/docker/frontend.Dockerfile`\n\n## Key Source Files Referenced\n- `src/frontend/package.json` — scripts.build (\"tsc -b \u0026\u0026 vite build\"), dependencies\n- `src/frontend/.nvmrc` — Node 22 version pin\n- `src/frontend/vite.config.ts` — build output configuration\n- `docker/nginx.conf` — copied into the image at build time","status":"closed","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-06T15:53:34.166849+08:00","created_by":"David Ten","updated_at":"2026-02-06T16:10:54.637919+08:00","closed_at":"2026-02-06T16:10:54.637919+08:00","close_reason":"Created multi-stage Dockerfiles for backend (CPU-only torch) and frontend (node→nginx)","dependencies":[{"issue_id":"MyCareersFuture-54s.4","depends_on_id":"MyCareersFuture-54s","type":"parent-child","created_at":"2026-02-06T15:53:34.168677+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-54s.4","depends_on_id":"MyCareersFuture-54s.1","type":"blocks","created_at":"2026-02-06T15:53:44.966464+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-54s.4","depends_on_id":"MyCareersFuture-54s.3","type":"blocks","created_at":"2026-02-06T15:53:45.80172+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-54s.5","title":"Create base docker-compose.yml for local development","description":"# Create Base docker-compose.yml for Local Development\n\n## What\nCreate the primary `docker-compose.yml` at the project root defining both services, volumes, networking, and health checks. This is the base configuration used for local development and extended by the production override for homelab.\n\n## Service Definitions\n\n### backend\n```yaml\nbackend:\n  build:\n    context: .\n    dockerfile: docker/backend.Dockerfile\n  container_name: mcf-backend\n  ports:\n    - \"8000:8000\"\n  volumes:\n    - mcf-data:/app/data\n    - hf-cache:/app/.cache/huggingface\n  environment:\n    - MCF_DB_PATH=/app/data/mcf_jobs.db\n    - MCF_INDEX_DIR=/app/data/embeddings\n    - MCF_CORS_ORIGINS=*\n    - MCF_RATE_LIMIT_RPM=0\n  healthcheck:\n    test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n    interval: 30s\n    timeout: 10s\n    start_period: 60s\n    retries: 3\n```\n\n**Port 8000 exposed directly**: For local dev, direct backend access is useful for debugging (curl, Postman, FastAPI /docs). The production override may remove this if only nginx access is desired.\n\n**MCF_RATE_LIMIT_RPM=0**: Disables the per-IP rate limiter. During local development, rapid testing (refreshing the UI, running curl loops) would hit the default 100 RPM limit, causing confusing 429 errors. Rate limiting is re-enabled in the production override.\n\n**MCF_CORS_ORIGINS=***: Permissive CORS. In the Docker Compose setup, the frontend goes through nginx (same origin), so CORS headers are technically unnecessary. The wildcard ensures direct backend access from any origin during development works too.\n\n### frontend\n```yaml\nfrontend:\n  build:\n    context: .\n    dockerfile: docker/frontend.Dockerfile\n  container_name: mcf-frontend\n  ports:\n    - \"3000:80\"\n  depends_on:\n    backend:\n      condition: service_healthy\n```\n\n**Port 3000:80**: Maps container port 80 (nginx) to host port 3000, matching the Vite dev server port. This means `http://localhost:3000` works identically whether running via `npm run dev` or `docker compose up`.\n\n**depends_on with service_healthy**: This is crucial. Without it, the frontend starts immediately, and the first user request hits nginx, which proxies to a backend that's still loading FAISS indexes. The user sees a 502 Bad Gateway. With `condition: service_healthy`, Docker waits until the backend health check passes (FAISS loaded, engine ready) before starting nginx.\n\n## Volume Definitions\n\n```yaml\nvolumes:\n  mcf-data:\n    driver: local\n  hf-cache:\n    driver: local\n```\n\n**Named volumes** (not bind mounts) are the default for local dev because:\n- Portable across OSes (macOS Docker Desktop stores volumes in the VM)\n- Docker manages lifecycle (docker volume ls, docker volume rm)\n- No dependency on specific host paths existing\n- The production override switches to bind mounts for direct host access\n\n**mcf-data** contains:\n- `mcf_jobs.db` (542MB SQLite database — read+write, analytics logged on each search)\n- `embeddings/jobs.index` (FAISS IVFFlat index — read-only after build)\n- `embeddings/skills.index` (FAISS Flat index — read-only)\n- `embeddings/companies.index` (FAISS Flat index — read-only)\n- `embeddings/*.pkl` (skill clusters, company centroids — read-only)\n- `embeddings/*.npy` (UUID mappings — read-only)\n\n**hf-cache** contains:\n- HuggingFace model files for all-MiniLM-L6-v2 (~100MB)\n- Downloaded automatically on first search request\n- Persists across container rebuilds (avoids re-download)\n\n## Networking\n\nDocker Compose creates a default bridge network automatically. Both containers join it. The frontend container resolves `backend` by service name — this is why nginx.conf uses `proxy_pass http://backend:8000`. No explicit network configuration needed.\n\n## Usage Commands\n\n```bash\n# Start everything (builds if needed)\ndocker compose up --build\n\n# Start in background\ndocker compose up -d --build\n\n# View logs\ndocker compose logs -f backend\ndocker compose logs -f frontend\n\n# Stop everything\ndocker compose down\n\n# Stop and remove volumes (DESTROYS DATA)\ndocker compose down -v\n```\n\n## File Location\n`docker-compose.yml` at project root: `/Users/admin/Documents/Work/MyCareersFuture/docker-compose.yml`\n\n## Key Source Files Referenced\n- `docker/backend.Dockerfile` — backend build instructions\n- `docker/frontend.Dockerfile` — frontend build instructions\n- `src/api/app.py` — environment variable names and defaults (lines 112-120)","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-06T15:53:34.853497+08:00","created_by":"David Ten","updated_at":"2026-02-06T16:15:33.556161+08:00","closed_at":"2026-02-06T16:15:33.556161+08:00","close_reason":"Created docker-compose.yml with backend+frontend services, named volumes, and health-gated startup","dependencies":[{"issue_id":"MyCareersFuture-54s.5","depends_on_id":"MyCareersFuture-54s","type":"parent-child","created_at":"2026-02-06T15:53:34.855022+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-54s.5","depends_on_id":"MyCareersFuture-54s.2","type":"blocks","created_at":"2026-02-06T15:53:46.453766+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-54s.5","depends_on_id":"MyCareersFuture-54s.4","type":"blocks","created_at":"2026-02-06T15:53:47.061725+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-54s.6","title":"Create docker-compose.prod.yml homelab override","description":"# Create docker-compose.prod.yml Homelab Override\n\n## What\nCreate `docker-compose.prod.yml` with production-specific overrides for homelab deployment. Used alongside the base file:\n\n```bash\ndocker compose -f docker-compose.yml -f docker-compose.prod.yml up -d --build\n```\n\n## The Docker Compose Override Pattern\n\nDocker Compose merges multiple -f files using deep merge:\n- New keys are added to the base\n- Existing scalar values are replaced\n- Lists are replaced entirely (not appended)\n\nThis keeps the base docker-compose.yml clean and focused on dev, while the override layers on production concerns (restart policies, resource limits, log rotation, bind mounts) without duplicating the entire configuration.\n\n## What This Override Adds\n\n### 1. Restart Policies\n\n```yaml\nrestart: unless-stopped\n```\n\nApplied to both services. This is the key to \"running indefinitely.\" The container auto-restarts on:\n- **Process crash**: Unhandled Python exception, segfault, OOM kill\n- **Docker daemon restart**: e.g., after Docker Desktop update\n- **Host reboot**: If Docker is configured to start on boot (systemctl enable docker)\n\nThe ONLY way a container stays stopped is `docker compose stop` or `docker compose down`. This is intentional — manual shutdown should be respected, but crashes should auto-recover.\n\n`unless-stopped` vs `always`: The `always` policy restarts even after explicit stop (on daemon restart). `unless-stopped` remembers that you manually stopped it. For a homelab, `unless-stopped` is the right choice — you don't want a service you deliberately stopped to resurrect on reboot.\n\n### 2. Resource Limits\n\n```yaml\nbackend:\n  deploy:\n    resources:\n      limits:\n        memory: 3G\n        cpus: \"2.0\"\n      reservations:\n        memory: 2G\n        cpus: \"0.5\"\n```\n\n**Memory limit 3GB — rationale:**\n- FAISS job index in RAM: ~1.5GB (100K jobs × 384 dims × 4 bytes + IVF overhead)\n- sentence-transformers model: ~200MB\n- Python interpreter + all imported modules: ~300MB\n- TTL caches (queries: 1000 × 1.5KB = ~1.5MB, results: 200 × 25KB = ~5MB): ~7MB\n- Operating headroom for search operations: ~200MB\n- **Typical total: ~2.2GB**\n- 3GB limit provides ~800MB headroom for:\n  - Python memory fragmentation over weeks/months (CPython doesn't always return freed memory to OS)\n  - Burst allocations during complex searches\n  - Temporary numpy array allocations\n\nIf the process exceeds 3GB, Docker OOM-kills it. With `restart: unless-stopped`, it immediately restarts and reloads cleanly. This is a feature, not a bug — it's a safety valve against unbounded memory growth.\n\n**CPU limit 2 cores**: FAISS search and sentence-transformers inference are CPU-intensive. 2 cores allows parallel FAISS search (OpenMP threads) while leaving other cores for the host and nginx. Adjust based on your homelab's total cores.\n\n**Frontend limits**: 128MB memory and 0.5 CPU is generous for nginx serving static files. nginx typically uses 10-30MB. The limits prevent runaway behavior in edge cases (e.g., very large access logs buffered in memory).\n\n### 3. Log Rotation\n\n```yaml\nlogging:\n  driver: json-file\n  options:\n    max-size: \"10m\"\n    max-file: \"5\"\n```\n\nWithout this, Docker's default json-file logging grows without bound. On a homelab running for months:\n- Backend logs: ~1-5MB/day (search request logging, startup messages)\n- Frontend logs: ~0.5-2MB/day (nginx access log)\n\nWith rotation: 10MB per file × 5 files = 50MB cap per service. Total: 100MB max for both services combined. Old logs are automatically deleted.\n\n### 4. Bind Mount Volumes\n\n```yaml\nvolumes:\n  mcf-data:\n    driver: local\n    driver_opts:\n      type: none\n      o: bind\n      device: /opt/mcf/data    # ← Customize this path\n  hf-cache:\n    driver: local\n    driver_opts:\n      type: none\n      o: bind\n      device: /opt/mcf/hf-cache\n```\n\nIn production, bind mounts replace named volumes. The data lives at a known host path, enabling:\n- **Direct backup**: `cp /opt/mcf/data/mcf_jobs.db /backups/` (no docker cp needed)\n- **Inspection**: `ls -la /opt/mcf/data/embeddings/` from the host\n- **Integration**: Host-level backup systems (cron + restic/borgbackup/rsync)\n- **Migration**: Copy the directory to move to another server\n\nThe host paths must exist before starting: `mkdir -p /opt/mcf/data/embeddings /opt/mcf/hf-cache`\n\n### 5. Rate Limiting Re-enabled\n\nThe base compose sets MCF_RATE_LIMIT_RPM=0 (disabled for dev). The prod override should either omit this (letting the Dockerfile default of 100 take effect) or explicitly set it. If the homelab is network-accessible, rate limiting prevents abuse.\n\n## Usage\n\n```bash\n# Start in production mode (background)\ndocker compose -f docker-compose.yml -f docker-compose.prod.yml up -d --build\n\n# Check resource usage\ndocker stats --no-stream\n\n# View logs\ndocker compose -f docker-compose.yml -f docker-compose.prod.yml logs -f backend\n\n# Stop (containers stay stopped until manually started)\ndocker compose -f docker-compose.yml -f docker-compose.prod.yml down\n```\n\nTip: Create a shell alias for the long compose command:\n```bash\nalias mcf-compose='docker compose -f docker-compose.yml -f docker-compose.prod.yml'\n# Then: mcf-compose up -d, mcf-compose logs, etc.\n```\n\n## File Location\n`docker-compose.prod.yml` at project root: `/Users/admin/Documents/Work/MyCareersFuture/docker-compose.prod.yml`\n\n## Key Source Files Referenced\n- `docker-compose.yml` — base config being extended\n- `src/api/app.py` — MCF_RATE_LIMIT_RPM env var (line 120)\n- `src/mcf/embeddings/search_engine.py` — memory usage patterns for resource limit sizing","status":"closed","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-06T15:53:35.979557+08:00","created_by":"David Ten","updated_at":"2026-02-06T16:18:45.818843+08:00","closed_at":"2026-02-06T16:18:45.818843+08:00","close_reason":"Created prod override with resource limits/restart/log rotation, and bootstrap script for data seeding","dependencies":[{"issue_id":"MyCareersFuture-54s.6","depends_on_id":"MyCareersFuture-54s","type":"parent-child","created_at":"2026-02-06T15:53:35.981025+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-54s.6","depends_on_id":"MyCareersFuture-54s.5","type":"blocks","created_at":"2026-02-06T15:53:47.71432+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-54s.7","title":"Create data bootstrapping workflow and helper script","description":"# Create Data Bootstrapping Workflow and Helper Script\n\n## What\nDocument and create a helper script for seeding initial data into Docker volumes on first run. The Docker images intentionally do NOT contain the SQLite database or FAISS indexes — these live in volumes and must be populated.\n\n## The First-Run Problem\n\nOn first `docker compose up`, the mcf-data volume is empty. The backend:\n1. Starts uvicorn and enters the lifespan handler\n2. Creates SemanticSearchEngine with db_path=/app/data/mcf_jobs.db\n3. SQLite auto-creates the database file with empty tables (schema initialization in MCFDatabase.__init__)\n4. Attempts to load FAISS indexes from /app/data/embeddings/ — finds nothing\n5. Enters degraded mode: _degraded=True, keyword-only search\n6. Health endpoint returns {\"status\":\"degraded\",\"index_loaded\":false,\"degraded\":true}\n\nThe service is functional (health check passes as degraded, not unhealthy), but useless for semantic search. Data must be bootstrapped.\n\n## Solution A: docker cp (Named Volumes)\n\nFor local dev with named volumes:\n\n```bash\n# 1. Start backend to create the volume\ndocker compose up -d backend\n\n# 2. Copy existing data into the running container's volume\ndocker cp data/mcf_jobs.db mcf-backend:/app/data/\ndocker cp data/embeddings/ mcf-backend:/app/data/\n\n# 3. Restart to reload FAISS indexes (they're loaded into RAM at startup, not re-read)\ndocker compose restart backend\n\n# 4. Verify\ncurl http://localhost:8000/health\n# Expected: {\"status\":\"healthy\",\"index_loaded\":true,\"degraded\":false}\n```\n\n## Solution B: Direct File Placement (Bind Mounts)\n\nFor homelab with bind mounts (prod override):\n\n```bash\n# 1. Create host directories\nmkdir -p /opt/mcf/data/embeddings /opt/mcf/hf-cache\n\n# 2. Copy data to host path\ncp data/mcf_jobs.db /opt/mcf/data/\ncp -r data/embeddings/* /opt/mcf/data/embeddings/\n\n# 3. Start services (backend reads data directly from host path)\ndocker compose -f docker-compose.yml -f docker-compose.prod.yml up -d\n```\n\n## Solution C: Generate Inside Container (Fresh Start)\n\nIf starting without existing data:\n\n```bash\n# 1. Start backend\ndocker compose up -d backend\n\n# 2. Scrape some jobs\ndocker exec mcf-backend python -m src.cli scrape \"data scientist\" --max-jobs 100\n\n# 3. Generate embeddings and build FAISS indexes\ndocker exec mcf-backend python -m src.cli embed-generate\n\n# 4. Check status\ndocker exec mcf-backend python -m src.cli embed-status\n\n# 5. Restart to load the newly-built indexes\ndocker compose restart backend\n```\n\n## Helper Script: docker/bootstrap-data.sh\n\nCreate a convenience script that detects the environment and bootstraps accordingly:\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Detect if data exists locally\nif [ ! -f data/mcf_jobs.db ]; then\n    echo \"ERROR: No local data/mcf_jobs.db found. Run the scraper first.\"\n    exit 1\nfi\n\n# Copy data into running container\necho \"Copying database...\"\ndocker cp data/mcf_jobs.db mcf-backend:/app/data/\n\nif [ -d data/embeddings ]; then\n    echo \"Copying embeddings...\"\n    docker cp data/embeddings/ mcf-backend:/app/data/\nfi\n\necho \"Restarting backend to reload indexes...\"\ndocker compose restart backend\n\necho \"Waiting for health check...\"\nsleep 10\ncurl -sf http://localhost:8000/health | python -m json.tool\n```\n\n## Ongoing Data Updates\n\nWhen new jobs are scraped and embeddings regenerated:\n\n1. **Scrape new jobs** (can run inside or outside container):\n   ```bash\n   docker exec mcf-backend python -m src.cli scrape \"machine learning\" --max-jobs 500\n   ```\n\n2. **Regenerate embeddings** (run inside container):\n   ```bash\n   docker exec mcf-backend python -m src.cli embed-generate\n   ```\n\n3. **Restart to pick up new FAISS indexes**:\n   ```bash\n   docker compose restart backend\n   ```\n\nThe restart is mandatory because FAISS indexes are loaded into RAM during the lifespan startup handler and never re-read from disk during the process lifetime. This is by design — mmap-style hot-reload would add complexity for a rare operation.\n\n## Backup Workflow\n\nFor the weekly maintenance backup:\n\n```bash\n# Named volumes:\ndocker cp mcf-backend:/app/data/mcf_jobs.db ./backups/mcf_jobs_$(date +%Y%m%d).db\n\n# Bind mounts:\ncp /opt/mcf/data/mcf_jobs.db /backups/mcf_jobs_$(date +%Y%m%d).db\n```\n\nSQLite is safe to copy while the app is running because:\n- The database uses rollback journal mode (not WAL)\n- Write transactions are short (analytics INSERT per search, ~1ms)\n- The file copy gets a consistent snapshot at the OS level\n- If a copy happens mid-transaction, the journal file provides recovery\n\n## File Location\n`docker/bootstrap-data.sh` at: `/Users/admin/Documents/Work/MyCareersFuture/docker/bootstrap-data.sh`\n\n## Key Source Files Referenced\n- `src/mcf/database.py` — schema auto-creation in __init__, _ensure_schema()\n- `src/api/app.py` — lifespan handler (lines 55-79), degraded mode behavior\n- `src/mcf/embeddings/search_engine.py` — load() method, _degraded flag\n- `src/cli.py` — scrape, embed-generate, embed-status commands","status":"closed","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-06T15:53:36.589817+08:00","created_by":"David Ten","updated_at":"2026-02-06T16:18:45.864738+08:00","closed_at":"2026-02-06T16:18:45.864738+08:00","close_reason":"Created prod override with resource limits/restart/log rotation, and bootstrap script for data seeding","dependencies":[{"issue_id":"MyCareersFuture-54s.7","depends_on_id":"MyCareersFuture-54s","type":"parent-child","created_at":"2026-02-06T15:53:36.592308+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-54s.7","depends_on_id":"MyCareersFuture-54s.5","type":"blocks","created_at":"2026-02-06T15:53:48.459467+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-54s.8","title":"End-to-end verification and smoke test","description":"# End-to-End Verification and Smoke Test\n\n## What\nVerify the complete Docker Compose deployment works correctly for both local development and homelab production configurations. This is the final validation gate before considering the deployment epic complete.\n\n## Test Checklist\n\n### 1. Build Verification\n```bash\ndocker compose build\n```\n- Both images should build without errors\n- Backend image: expected ~2.5-3GB (check with docker images)\n- Frontend image: expected ~50MB\n- No secrets or data/ contents in image layers (verify: docker history \u003cimage\u003e)\n\n### 2. Startup Verification\n```bash\ndocker compose up\n```\nBackend logs should show (in order):\n1. \"Uvicorn running on http://0.0.0.0:8000\"\n2. \"Loading FAISS indexes...\" (in lifespan handler)\n3. Either \"FAISS index loaded successfully\" or \"Running in degraded mode: ...\" if no indexes\n\nFrontend logs should show:\n1. nginx startup message\n2. No errors about upstream connection (thanks to depends_on: service_healthy)\n\n### 3. Health Check (Through nginx proxy)\n```bash\ncurl http://localhost:3000/health\n```\nExpected (with data loaded): `{\"status\":\"healthy\",\"index_loaded\":true,\"degraded\":false}`\nExpected (without data): `{\"status\":\"degraded\",\"index_loaded\":false,\"degraded\":true}`\n\nAlso verify direct backend access:\n```bash\ncurl http://localhost:8000/health\n```\nShould return identical response.\n\n### 4. Search End-to-End\n```bash\n# Via nginx proxy (the production request path)\ncurl -X POST http://localhost:3000/api/search \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"data scientist\", \"limit\": 5}'\n```\nShould return JSON with results array, search_time_ms, total_candidates.\n\nOpen browser at `http://localhost:3000`:\n- SearchBar should render\n- Type a query → results should appear\n- FilterPanel should work (salary, employment type)\n- \"Find Similar\" on a job card should open modal\n- Health check in React Query should show connected (no DegradedBanner unless degraded)\n\n### 5. API Documentation Access\n```bash\ncurl http://localhost:3000/docs\n```\nShould serve FastAPI's Swagger UI through the nginx proxy. This validates the /docs and /openapi.json proxy routes in nginx.conf.\n\n### 6. Volume Persistence Test\n```bash\n# Perform a search (creates analytics entry in SQLite)\ncurl -X POST http://localhost:3000/api/search \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\":\"persistence test\"}'\n\n# Destroy and recreate containers (volumes persist)\ndocker compose down\ndocker compose up -d\n\n# Verify analytics survived\ncurl http://localhost:3000/api/analytics/popular\n# Should include \"persistence test\" in results\n```\n\n### 7. Resilience Test (Production Config)\n```bash\n# Start with production overrides\ndocker compose -f docker-compose.yml -f docker-compose.prod.yml up -d\n\n# Force kill the backend (simulates crash/OOM)\ndocker kill mcf-backend\n\n# Wait 10-30 seconds for auto-restart\nsleep 15\ndocker ps\n# mcf-backend should be running again (STATUS: Up X seconds)\n\n# Verify it's healthy after restart\ncurl http://localhost:3000/health\n```\n\n### 8. Resource Limits Verification\n```bash\n# Start with production config\ndocker compose -f docker-compose.yml -f docker-compose.prod.yml up -d\n\n# Check resource limits are applied\ndocker stats --no-stream\n# Backend MEM LIMIT should show 3GB\n# Frontend MEM LIMIT should show 128MB\n\n# Check container inspect for detailed limits\ndocker inspect mcf-backend | grep -A5 Memory\ndocker inspect mcf-frontend | grep -A5 Memory\n```\n\n### 9. Log Rotation Verification\n```bash\n# Check logging driver config\ndocker inspect mcf-backend --format='{{.HostConfig.LogConfig}}'\n# Should show: {json-file map[max-file:5 max-size:10m]}\n```\n\n### 10. Data Bootstrapping Verification\n```bash\n# Test the bootstrap script\ndocker compose up -d backend\nbash docker/bootstrap-data.sh\n# Should copy data and restart, ending with healthy status\n```\n\n## Expected Final State\n\nAfter all tests pass:\n- `docker compose up` → both services start, frontend at :3000, backend at :8000\n- Health endpoint returns healthy (or degraded if no data, which is a valid state)\n- Search works end-to-end through nginx\n- Production config adds restart policy, resource limits, log rotation\n- Data survives container destruction (only volume removal destroys data)\n- Containers auto-restart after crashes\n\n## What to Do If Tests Fail\n\n- **Build fails**: Check pyproject.toml dependency compatibility with CPU-only torch. Check node version matches .nvmrc.\n- **502 Bad Gateway**: nginx started before backend was healthy. Check depends_on condition.\n- **Health returns degraded**: Data not loaded. Run bootstrap script (task 54s.7).\n- **CORS errors in browser**: Check MCF_CORS_ORIGINS env var and nginx proxy headers.\n- **Container doesn't restart**: Check restart policy in prod override. Ensure using both -f flags.\n- **OOM kill**: Backend exceeding 3GB limit. Check FAISS index size, consider increasing limit.\n\n## File Location\nNo new files — this task is verification of all previous tasks' output.\n\n## Depends On\nAll other tasks in this epic (54s.1 through 54s.7) must be complete before verification can run.","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-06T15:53:37.233365+08:00","created_by":"David Ten","updated_at":"2026-02-06T17:18:44.854168+08:00","closed_at":"2026-02-06T17:18:44.854168+08:00","close_reason":"All smoke tests passed: build, startup, health checks, search E2E through nginx, SPA fallback, /docs proxy, volume persistence, prod config (resource limits, log rotation, restart policy). Auto-restart not testable on Docker Desktop macOS but policy correctly configured for Linux.","dependencies":[{"issue_id":"MyCareersFuture-54s.8","depends_on_id":"MyCareersFuture-54s","type":"parent-child","created_at":"2026-02-06T15:53:37.234767+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-54s.8","depends_on_id":"MyCareersFuture-54s.6","type":"blocks","created_at":"2026-02-06T15:53:48.995749+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-54s.8","depends_on_id":"MyCareersFuture-54s.7","type":"blocks","created_at":"2026-02-06T15:53:49.566707+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-7io","title":"Freshness scoring is N+1 DB queries","description":"_compute_freshness_scores calls db.get_job per UUID in a loop. This is a performance issue (N+1 queries), not a correctness bug — results are correct, just slow. On large candidate lists (hundreds to thousands after SQL filtering), this can dominate latency with individual DB round-trips.\n\nFix with a single bulk query: SELECT uuid, posted_date FROM jobs WHERE uuid IN (...) using chunked IN clauses. Ref: src/mcf/embeddings/search_engine.py:878-909","status":"open","priority":2,"issue_type":"bug","owner":"davidten7@gmail.com","created_at":"2026-02-06T17:58:04.706219+08:00","created_by":"David Ten","updated_at":"2026-02-06T19:45:29.965603+08:00"}
{"id":"MyCareersFuture-8nv","title":"Rate limiter IP dict grows unbounded (memory leak)","description":"The in-memory _hits dict (defaultdict(list)) has no eviction of stale IP entries. Once an IP accesses the API, its key persists forever even after all its timestamps expire. Over months with thousands of unique IPs, this dict grows unbounded.\n\nFix: periodically sweep and remove IP keys whose timestamp lists are empty after pruning, or use a TTL-based dict (e.g., cachetools.TTLCache keyed by IP).\n\nNote: the related issue of X-Forwarded-For being trusted without a proxy allowlist is tracked separately.\n\nRef: src/api/middleware.py:14-61","status":"open","priority":3,"issue_type":"bug","owner":"davidten7@gmail.com","created_at":"2026-02-06T17:58:09.066099+08:00","created_by":"David Ten","updated_at":"2026-02-06T19:45:38.130773+08:00"}
{"id":"MyCareersFuture-ak2","title":"BM25 scored globally then filtered, missing candidates","description":"BM25 is computed over the full FTS index and filtered to the candidate set afterward. With large candidate sets, many candidates get a BM25 of 0 just because they aren't in the global top-N. This directly degrades search quality — the core value proposition. A job that's a perfect BM25 match for a query but ranks outside the global top-N never gets scored, even if it's in the SQL-filtered candidate set. Users get worse results than they should.\n\nConsider restricting BM25 to the filtered candidate set or using an adaptive window. Ref: src/mcf/embeddings/search_engine.py:771-797","status":"closed","priority":1,"issue_type":"bug","owner":"davidten7@gmail.com","created_at":"2026-02-06T17:58:03.252235+08:00","created_by":"David Ten","updated_at":"2026-02-06T19:58:49.532627+08:00","closed_at":"2026-02-06T19:58:49.532627+08:00","close_reason":"Added bm25_search_filtered() that restricts FTS5 scoring to the candidate set via temp table JOIN. Updated _get_bm25_scores to use it. All 315 tests pass."}
{"id":"MyCareersFuture-b5i","title":"Skill search filters are ignored","description":"SkillSearchRequest exposes salary/employment filters, but to_internal drops them and search_by_skill never applies SQL filtering. API callers can't actually filter skill searches. Either remove these fields or plumb them through to SQL filtering. Ref: src/api/models.py:138-164","status":"closed","priority":1,"issue_type":"bug","owner":"davidten7@gmail.com","created_at":"2026-02-06T17:57:57.451398+08:00","created_by":"David Ten","updated_at":"2026-02-06T19:45:44.75683+08:00","closed_at":"2026-02-06T19:45:44.75683+08:00","close_reason":"Merged into MyCareersFuture-cew — same class of issue (API accepts filters it doesn't apply). Both region and skill search filters are now tracked together."}
{"id":"MyCareersFuture-c6p","title":"Hard cap on candidates can drop relevant results","description":"SQL pre-filtering is capped at 100,000 rows. Large datasets can exceed this, so relevant jobs outside the cap never get ranked. In practice this is a theoretical edge case at current scale — most searches include filters that reduce the set well below 100K, and SQL default ordering returns recent jobs first. Only completely unfiltered searches on the full 6.2M dataset would hit this.\n\nConsider paging candidates, using index-filtered vector search, or lowering the cap only after a relevance cutoff. Ref: src/mcf/embeddings/search_engine.py:654-670","status":"open","priority":3,"issue_type":"bug","owner":"davidten7@gmail.com","created_at":"2026-02-06T17:58:02.09993+08:00","created_by":"David Ten","updated_at":"2026-02-06T19:45:22.415085+08:00"}
{"id":"MyCareersFuture-cew","title":"API filter parameters silently ignored (region, skill search filters)","description":"Two related issues where the API accepts filter parameters but never applies them:\n\n1. **Region filter (SearchRequest)**: SearchRequest includes region and passes it to the internal model, but _apply_sql_filters never passes it to the DB query. Users can set region but results are unfiltered. Ref: src/mcf/embeddings/search_engine.py:654-670\n\n2. **Skill search filters (SkillSearchRequest)**: SkillSearchRequest exposes salary_min, salary_max, and employment_type fields, but to_internal() drops them entirely. The internal SkillSearchRequest dataclass doesn't even have these fields. search_by_skill never applies SQL filtering. Ref: src/api/models.py:138-164\n\nFix: plumb all accepted filter fields through to_internal() and apply them in SQL queries. For skill search, add an _apply_sql_filters call to search_by_skill or intersect vector results with SQL-filtered candidates.","status":"open","priority":2,"issue_type":"bug","owner":"davidten7@gmail.com","created_at":"2026-02-06T17:57:58.915532+08:00","created_by":"David Ten","updated_at":"2026-02-06T19:45:14.958083+08:00"}
{"id":"MyCareersFuture-cho","title":"Update README with search \u0026 recommendation docs","status":"closed","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-06T17:26:22.764619+08:00","created_by":"David Ten","updated_at":"2026-02-06T17:29:46.374945+08:00","closed_at":"2026-02-06T17:29:46.374945+08:00","close_reason":"README updated with semantic search, REST API, embeddings, and Docker deployment docs"}
{"id":"MyCareersFuture-dx5","title":"Historical scraper loads all UUIDs into memory","description":"Loading all UUIDs for deduplication scales poorly as the DB grows into millions of rows. At 6.2M jobs, this set consumes ~600MB of memory on startup. This is a performance/scalability issue — the code is correct, just wasteful. It blocks startup, prevents running multiple instances, and loads UUIDs for all years even when scraping only one.\n\nConsider a bloom filter, a DB existence check per fetch (single indexed query is fast), or sequence-based dedup using fetch_attempts. Ref: src/mcf/historical_scraper.py:127-133","status":"open","priority":2,"issue_type":"bug","owner":"davidten7@gmail.com","created_at":"2026-02-06T17:58:07.172982+08:00","created_by":"David Ten","updated_at":"2026-02-06T19:45:26.289863+08:00"}
{"id":"MyCareersFuture-g05","title":"Complete company multi-centroid integration","status":"closed","priority":2,"issue_type":"feature","owner":"davidten7@gmail.com","created_at":"2026-02-06T12:05:35.79593+08:00","created_by":"David Ten","updated_at":"2026-02-06T12:09:34.129655+08:00","closed_at":"2026-02-06T12:09:34.129655+08:00","close_reason":"Implemented full company multi-centroid pipeline: DB get_all_companies, generator centroids from DB, index_manager get_company_centroids + has_company_index, search_engine multi-centroid find_similar_companies with fallback, CLI company index building, 22 tests all passing"}
{"id":"MyCareersFuture-lkp","title":"MCFScraper falsely advertises concurrent fetching","description":"MCFScraper's docstring claims 'Concurrent fetching with configurable parallelism' and __init__ accepts a concurrency parameter, but the scrape loop is purely sequential. This is misleading but not broken — the scraper works fine sequentially.\n\nRecommended fix: remove the concurrency parameter and update the docstring to accurately describe sequential behavior. If actual concurrent fetching is desired, that should be a separate feature request (and depends on proper 429 handling from MyCareersFuture-1xj being fixed first).\n\nRef: src/mcf/scraper.py:4-71","status":"open","priority":3,"issue_type":"bug","owner":"davidten7@gmail.com","created_at":"2026-02-06T17:58:07.638105+08:00","created_by":"David Ten","updated_at":"2026-02-06T19:45:33.609896+08:00"}
{"id":"MyCareersFuture-mk9","title":"Add performance benchmarking suite","description":"# Task: Add Performance Benchmarking Suite\n\n## What\nCreate a benchmarking suite to verify the system meets performance targets:\n- Search latency p95 \u003c 100ms\n- Cached search latency p95 \u003c 20ms  \n- Query embedding time \u003c 50ms\n- Index load time \u003c 5s\n\n## Benchmarking Script\n\n```python\n# scripts/benchmark.py\n\"\"\"\nPerformance benchmarking for the semantic search system.\n\nUsage:\n    python scripts/benchmark.py --queries 100 --warmup 10\n\"\"\"\n\nimport asyncio\nimport time\nimport statistics\nimport argparse\nfrom pathlib import Path\n\nimport numpy as np\nfrom rich.console import Console\nfrom rich.table import Table\n\nfrom src.mcf.embeddings import SemanticSearchEngine, EmbeddingGenerator\nfrom src.api.models import SearchRequest\n\nconsole = Console()\n\n# Sample queries for benchmarking\nBENCHMARK_QUERIES = [\n    \"python developer\",\n    \"machine learning engineer\",\n    \"data scientist singapore\",\n    \"full stack javascript react\",\n    \"devops kubernetes aws\",\n    \"backend java spring boot\",\n    \"frontend react typescript\",\n    \"data analyst sql tableau\",\n    \"product manager agile\",\n    \"cloud architect azure\",\n]\n\nasync def benchmark_search(engine: SemanticSearchEngine, n_queries: int, warmup: int) -\u003e dict:\n    \"\"\"Benchmark search latency.\"\"\"\n    queries = [BENCHMARK_QUERIES[i % len(BENCHMARK_QUERIES)] for i in range(n_queries + warmup)]\n    \n    # Warmup\n    console.print(f\"[dim]Warming up with {warmup} queries...[/dim]\")\n    for q in queries[:warmup]:\n        await engine.search(SearchRequest(query=q, limit=10))\n    \n    # Benchmark\n    console.print(f\"[dim]Running {n_queries} benchmark queries...[/dim]\")\n    latencies = []\n    cache_hits = 0\n    \n    for q in queries[warmup:]:\n        start = time.perf_counter()\n        response = await engine.search(SearchRequest(query=q, limit=10))\n        elapsed = (time.perf_counter() - start) * 1000\n        latencies.append(elapsed)\n        if response.cache_hit:\n            cache_hits += 1\n    \n    latencies.sort()\n    n = len(latencies)\n    \n    return {\n        'min': min(latencies),\n        'max': max(latencies),\n        'mean': statistics.mean(latencies),\n        'median': statistics.median(latencies),\n        'p90': latencies[int(n * 0.9)],\n        'p95': latencies[int(n * 0.95)],\n        'p99': latencies[int(n * 0.99)] if n \u003e= 100 else latencies[-1],\n        'cache_hit_rate': cache_hits / n * 100,\n    }\n\ndef benchmark_embedding(generator: EmbeddingGenerator, n_texts: int) -\u003e dict:\n    \"\"\"Benchmark embedding generation time.\"\"\"\n    texts = [\n        f\"We are looking for a {BENCHMARK_QUERIES[i % len(BENCHMARK_QUERIES)]} \"\n        f\"to join our team. Requirements include Python, SQL, and machine learning.\"\n        for i in range(n_texts)\n    ]\n    \n    # Single embedding\n    start = time.perf_counter()\n    for text in texts[:10]:\n        generator.model.encode(text)\n    single_time = (time.perf_counter() - start) / 10 * 1000\n    \n    # Batch embedding\n    start = time.perf_counter()\n    generator.model.encode(texts, batch_size=32)\n    batch_time = (time.perf_counter() - start) / n_texts * 1000\n    \n    return {\n        'single_ms': single_time,\n        'batch_ms': batch_time,\n    }\n\ndef benchmark_index_load(index_dir: Path) -\u003e dict:\n    \"\"\"Benchmark FAISS index load time.\"\"\"\n    from src.mcf.embeddings import FAISSIndexManager\n    \n    start = time.perf_counter()\n    manager = FAISSIndexManager(index_dir)\n    manager.load()\n    load_time = time.perf_counter() - start\n    \n    stats = manager.get_stats()\n    \n    return {\n        'load_time_s': load_time,\n        'index_size_mb': stats.get('jobs_index_size_mb', 0),\n        'n_vectors': stats.get('n_job_vectors', 0),\n    }\n\ndef print_results(search_results: dict, embedding_results: dict, index_results: dict):\n    \"\"\"Print benchmark results in a nice table.\"\"\"\n    \n    # Search latency table\n    table = Table(title=\"Search Latency (ms)\")\n    table.add_column(\"Metric\", style=\"cyan\")\n    table.add_column(\"Value\", justify=\"right\")\n    table.add_column(\"Target\", justify=\"right\")\n    table.add_column(\"Status\", justify=\"center\")\n    \n    def status(value, target, lower_is_better=True):\n        if lower_is_better:\n            return \"✅\" if value \u003c= target else \"❌\"\n        return \"✅\" if value \u003e= target else \"❌\"\n    \n    table.add_row(\"Min\", f\"{search_results['min']:.1f}\", \"-\", \"\")\n    table.add_row(\"Mean\", f\"{search_results['mean']:.1f}\", \"-\", \"\")\n    table.add_row(\"Median\", f\"{search_results['median']:.1f}\", \"-\", \"\")\n    table.add_row(\"P90\", f\"{search_results['p90']:.1f}\", \"-\", \"\")\n    table.add_row(\"P95\", f\"{search_results['p95']:.1f}\", \"\u003c100\", status(search_results['p95'], 100))\n    table.add_row(\"P99\", f\"{search_results['p99']:.1f}\", \"-\", \"\")\n    table.add_row(\"Max\", f\"{search_results['max']:.1f}\", \"-\", \"\")\n    table.add_row(\"Cache Hit Rate\", f\"{search_results['cache_hit_rate']:.1f}%\", \"\u003e30%\", status(search_results['cache_hit_rate'], 30, False))\n    \n    console.print(table)\n    console.print()\n    \n    # Embedding table\n    table = Table(title=\"Embedding Generation (ms per text)\")\n    table.add_column(\"Mode\", style=\"cyan\")\n    table.add_column(\"Value\", justify=\"right\")\n    table.add_column(\"Target\", justify=\"right\")\n    table.add_column(\"Status\", justify=\"center\")\n    \n    table.add_row(\"Single\", f\"{embedding_results['single_ms']:.1f}\", \"\u003c50\", status(embedding_results['single_ms'], 50))\n    table.add_row(\"Batch (per item)\", f\"{embedding_results['batch_ms']:.1f}\", \"-\", \"\")\n    \n    console.print(table)\n    console.print()\n    \n    # Index table\n    table = Table(title=\"Index Loading\")\n    table.add_column(\"Metric\", style=\"cyan\")\n    table.add_column(\"Value\", justify=\"right\")\n    table.add_column(\"Target\", justify=\"right\")\n    table.add_column(\"Status\", justify=\"center\")\n    \n    table.add_row(\"Load Time\", f\"{index_results['load_time_s']:.2f}s\", \"\u003c5s\", status(index_results['load_time_s'], 5))\n    table.add_row(\"Index Size\", f\"{index_results['index_size_mb']:.1f} MB\", \"-\", \"\")\n    table.add_row(\"Vectors\", f\"{index_results['n_vectors']:,}\", \"-\", \"\")\n    \n    console.print(table)\n\nasync def main():\n    parser = argparse.ArgumentParser(description=\"Benchmark semantic search performance\")\n    parser.add_argument(\"--queries\", type=int, default=100, help=\"Number of benchmark queries\")\n    parser.add_argument(\"--warmup\", type=int, default=10, help=\"Number of warmup queries\")\n    parser.add_argument(\"--db\", type=str, default=\"data/mcf_jobs.db\", help=\"Database path\")\n    parser.add_argument(\"--index-dir\", type=str, default=\"data/embeddings\", help=\"Index directory\")\n    args = parser.parse_args()\n    \n    console.print(\"[bold]MCF Semantic Search Benchmark[/bold]\")\n    console.print()\n    \n    # Initialize\n    console.print(\"[dim]Loading search engine...[/dim]\")\n    engine = SemanticSearchEngine(args.db, Path(args.index_dir))\n    await engine.load()\n    \n    generator = EmbeddingGenerator()\n    \n    # Run benchmarks\n    search_results = await benchmark_search(engine, args.queries, args.warmup)\n    embedding_results = benchmark_embedding(generator, 100)\n    index_results = benchmark_index_load(Path(args.index_dir))\n    \n    # Print results\n    print_results(search_results, embedding_results, index_results)\n    \n    # Summary\n    all_passed = (\n        search_results['p95'] \u003c= 100 and\n        embedding_results['single_ms'] \u003c= 50 and\n        index_results['load_time_s'] \u003c= 5\n    )\n    \n    if all_passed:\n        console.print(\"\\n[bold green]✅ All performance targets met![/bold green]\")\n    else:\n        console.print(\"\\n[bold red]❌ Some performance targets not met[/bold red]\")\n        return 1\n    \n    return 0\n\nif __name__ == \"__main__\":\n    exit(asyncio.run(main()))\n```\n\n## CLI Integration\n\n```python\n# Add to src/cli.py\n\n@app.command(name=\"benchmark\")\ndef run_benchmark(\n    queries: int = typer.Option(100, \"--queries\", \"-n\", help=\"Number of benchmark queries\"),\n    warmup: int = typer.Option(10, \"--warmup\", help=\"Number of warmup queries\"),\n    db_path: str = typer.Option(\"data/mcf_jobs.db\", \"--db\"),\n    index_dir: str = typer.Option(\"data/embeddings\", \"--index-dir\"),\n) -\u003e None:\n    \"\"\"\n    Run performance benchmarks.\n    \n    Measures search latency, embedding generation time, and index loading.\n    \"\"\"\n    import subprocess\n    result = subprocess.run([\n        \"python\", \"scripts/benchmark.py\",\n        \"--queries\", str(queries),\n        \"--warmup\", str(warmup),\n        \"--db\", db_path,\n        \"--index-dir\", index_dir\n    ])\n    raise typer.Exit(result.returncode)\n```\n\n## Expected Results\n\nWith 100K jobs:\n```\nSearch Latency (ms)\n┏━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━━┓\n┃ Metric         ┃ Value ┃ Target ┃ Status ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━━┩\n│ P95            │  45.2 │ \u003c100   │   ✅   │\n│ Cache Hit Rate │  35.0%│ \u003e30%   │   ✅   │\n└────────────────┴───────┴────────┴────────┘\n\nEmbedding Generation (ms per text)\n┏━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━━┓\n┃ Mode           ┃ Value ┃ Target ┃ Status ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━━┩\n│ Single         │  32.1 │ \u003c50    │   ✅   │\n└────────────────┴───────┴────────┴────────┘\n\nIndex Loading\n┏━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━━┓\n┃ Metric         ┃ Value ┃ Target ┃ Status ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━━┩\n│ Load Time      │ 2.3s  │ \u003c5s    │   ✅   │\n│ Index Size     │ 582 MB│ -      │        │\n│ Vectors        │100,234│ -      │        │\n└────────────────┴───────┴────────┴────────┘\n\n✅ All performance targets met!\n```\n\n## Dependencies\n- Requires working search engine (Phase 4)\n- Should be run before production deployment","status":"closed","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T20:27:53.516066+08:00","created_by":"David Ten","updated_at":"2026-02-06T14:56:35.910653+08:00","closed_at":"2026-02-06T14:56:35.910653+08:00","close_reason":"Created scripts/benchmark.py and CLI 'benchmark' command. Measures search latency (fresh/cached), embedding generation, and index loading against performance targets.","dependencies":[{"issue_id":"MyCareersFuture-mk9","depends_on_id":"MyCareersFuture-qwl.4","type":"parent-child","created_at":"2026-02-05T20:28:58.128403+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-mk9","depends_on_id":"MyCareersFuture-qwl.4.1","type":"blocks","created_at":"2026-02-05T20:28:58.342557+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-mxy","title":"X-Forwarded-For trusted without proxy allowlist","description":"The rate limiter middleware trusts X-Forwarded-For headers unconditionally via get_client_ip(). Any client can spoof their IP by setting this header, bypassing rate limits entirely or causing rate limits to apply to the wrong IP.\n\nFix: only trust X-Forwarded-For when the direct connection comes from a known proxy IP (e.g., a configured allowlist of load balancer IPs). When not behind a known proxy, use request.client.host directly.\n\nSplit from MyCareersFuture-8nv which now tracks only the memory leak issue.\n\nRef: src/api/middleware.py:14-24","status":"open","priority":3,"issue_type":"bug","owner":"davidten7@gmail.com","created_at":"2026-02-06T19:45:49.199144+08:00","created_by":"David Ten","updated_at":"2026-02-06T19:45:49.199144+08:00"}
{"id":"MyCareersFuture-qwl","title":"Semantic Search System for MCF Job Data","description":"# Epic: Semantic Search System for MCF Job Data\n\n## Project Vision\nBuild a production-ready semantic search system that enables users to find jobs not just by exact keyword matches, but by meaning and intent. The system will transform how users discover relevant job opportunities by understanding that \"ML engineer\" and \"Deep Learning Specialist\" are semantically related, even when the exact words don't match.\n\n## Business Context\nThe MCF (MyCareersFuture) job scraper has accumulated 100K-1M job listings in a SQLite database. Currently, search is limited to basic SQL LIKE queries which:\n- Miss semantically similar results (searching \"Python developer\" won't find \"Backend Engineer with Python experience\")\n- Require exact keyword matches\n- Don't support finding similar jobs or companies\n- Have no relevance ranking beyond recency\n\n## Strategic Goals\n1. **Improve job discovery**: Users find relevant jobs they would have missed with keyword search\n2. **Enable recommendations**: \"Show me jobs similar to this one\" and \"Find companies like this one\"\n3. **Support skill-based exploration**: Search by skill clusters, not just individual keywords\n4. **Maintain fast response times**: Sub-100ms search latency for production use\n5. **No runtime LLM dependency**: All NLP computation happens at indexing time, not search time\n\n## Technical Approach\nThe system uses a **hybrid search architecture** combining:\n1. **Vector embeddings** (Sentence Transformers): Capture semantic meaning\n2. **BM25/FTS5**: Handle exact keyword matches\n3. **Hybrid scoring**: `final_score = α * semantic + (1-α) * keyword` where α is tunable\n\nThis approach ensures that searching \"Python developer\" ranks exact Python matches highest (via BM25) while also surfacing semantically related roles (via embeddings).\n\n## Key Architectural Decisions\n- **FAISS with IVFFlat**: Chosen for 100K-1M scale, \u003c100ms search, no external services\n- **SQLite FTS5**: Built-in full-text search with BM25 ranking, zero dependencies\n- **Multi-centroid company embeddings**: Captures diverse job families within companies\n- **Query expansion**: Precomputed skill clusters expand queries automatically\n- **Graceful degradation**: Falls back to keyword search if vector index fails\n\n## Success Criteria\n- Search latency p95 \u003c 100ms\n- Cached search latency p95 \u003c 20ms\n- Hybrid search improves relevance over keyword-only (manual evaluation)\n- All four search modes working: semantic, similar jobs, skill-based, company similarity\n- Interactive web UI with combined filters + semantic search\n\n## Dependencies\n- Existing MCF scraper codebase (SQLite database, Pydantic models, Typer CLI)\n- Python 3.10+, Poetry for dependency management\n- New: sentence-transformers, faiss-cpu, FastAPI, React\n\n## Estimated Scope\n- 6 major phases\n- ~35-40 individual tasks\n- Backend: ~2000-3000 lines of new Python code\n- Frontend: React application (~1500 lines TypeScript)\n\n## Reference\n- Plan file: /Users/admin/.claude/plans/polished-imagining-sundae.md\n- Existing database: src/mcf/database.py\n- Existing models: src/mcf/models.py\n- Existing CLI: src/cli.py","status":"closed","priority":1,"issue_type":"epic","owner":"davidten7@gmail.com","created_at":"2026-02-05T17:55:34.696975+08:00","created_by":"David Ten","updated_at":"2026-02-06T15:34:39.052984+08:00","closed_at":"2026-02-06T15:34:39.052984+08:00","close_reason":"All 6 phases complete: core embeddings, FAISS indexing, search engine, FastAPI backend, skill/company features, and React frontend"}
{"id":"MyCareersFuture-qwl.1","title":"Phase 1: Core Embedding Infrastructure","description":"# Phase 1: Core Embedding Infrastructure\n\n## Purpose\nEstablish the foundational components for the semantic search system: embedding generation, database schema extensions, and CLI tooling. This phase creates the data pipeline that transforms raw job text into searchable vector representations.\n\n## Why This Phase Comes First\nEverything else depends on having embeddings. The FAISS index needs embeddings to index. The search engine needs embeddings to query. The API needs embeddings to return results. Without this foundation, no other component can function.\n\n## Key Components\n\n### 1.1 Dependencies Setup (pyproject.toml)\nAdd required packages:\n- `sentence-transformers ^2.2.0`: Embedding model (all-MiniLM-L6-v2, 384 dimensions)\n- `faiss-cpu ^1.7.4`: Vector similarity search\n- `fastapi ^0.104.0`: REST API framework\n- `uvicorn ^0.24.0`: ASGI server\n- `numpy ^1.24.0`: Array operations\n- `cachetools ^5.3.0`: LRU caching\n- `scikit-learn ^1.3.0`: K-means clustering for company embeddings\n\n**Rationale**: These are all well-maintained, production-grade libraries. Sentence Transformers provides easy access to SOTA embedding models. FAISS is Facebook's battle-tested vector search library.\n\n### 1.2 Database Schema Extensions (src/mcf/database.py)\nNew tables and columns:\n- `embeddings` table: Store embedding vectors as BLOBs with model version tracking\n- `jobs_fts` FTS5 virtual table: Full-text search with BM25 ranking\n- `search_analytics` table: Query logging for monitoring\n- `salary_annual` computed column: Normalize monthly/yearly/hourly to annual\n\n**Rationale**: \n- BLOBs are efficient for embeddings (384 floats × 4 bytes = 1.5KB per job)\n- FTS5 is built into SQLite, provides 10-100x faster keyword search than LIKE\n- Analytics enables monitoring search quality and popular queries\n- Salary normalization ensures consistent filtering regardless of salary type\n\n### 1.3 Embedding Generator (src/mcf/embeddings/generator.py)\nCore class that:\n- Loads Sentence Transformer model (lazy loading, cached)\n- Generates job embeddings from composite text: `title title description skills categories`\n- Batches jobs for efficient GPU/CPU utilization\n- Tracks progress for long-running batch operations\n\n**Technical Decision - Composite Text**:\nWe concatenate multiple fields rather than embedding them separately because:\n1. Single embedding per job is simpler to index and search\n2. Title is duplicated to give it 2x weight (most important signal)\n3. Description is truncated to 500 chars to fit model context\n4. Skills and categories provide structured signals\n\n### 1.4 CLI Commands (src/cli.py additions)\n- `embed-generate`: Batch generate all embeddings\n- `embed-sync`: Incremental update (new jobs only)\n- `embed-status`: Coverage statistics\n- `embed-upgrade --model NAME`: Re-embed with new model\n\n**Why Four Commands**:\n- `generate`: Initial setup, runs once on existing data\n- `sync`: Called after each scrape, fast incremental updates\n- `status`: Monitoring and debugging\n- `upgrade`: Future-proofing for model improvements\n\n## Technical Considerations\n\n### Memory Management\n- Batch processing (32 jobs at a time) prevents OOM on large datasets\n- Embeddings stored in SQLite BLOB, not in-memory\n- Model loaded once, reused across batches\n\n### Error Handling\n- Failed embeddings logged but don't crash batch\n- Checkpoint support for resume after interruption\n- Model download failure retried with exponential backoff\n\n### Testing Strategy\n- Unit tests for EmbeddingGenerator with mock model\n- Integration tests with small job dataset\n- Performance benchmark: target \u003e100 jobs/second on CPU\n\n## Files to Create/Modify\n- `pyproject.toml` (modify)\n- `src/mcf/database.py` (modify - add schema, methods)\n- `src/mcf/embeddings/__init__.py` (create)\n- `src/mcf/embeddings/generator.py` (create)\n- `src/mcf/embeddings/models.py` (create - Pydantic models)\n- `src/cli.py` (modify - add commands)\n\n## Exit Criteria\n- [ ] All dependencies install cleanly with `poetry install`\n- [ ] Database schema migrations run without error\n- [ ] `embed-generate` completes for 1000 test jobs\n- [ ] `embed-status` shows correct coverage percentage\n- [ ] FTS5 table syncs with jobs table via triggers","status":"closed","priority":1,"issue_type":"epic","owner":"davidten7@gmail.com","created_at":"2026-02-05T17:56:21.948897+08:00","created_by":"David Ten","updated_at":"2026-02-06T00:53:46.173118+08:00","closed_at":"2026-02-06T00:53:46.173118+08:00","close_reason":"All 5 subtasks completed. Infrastructure verified: embeddings module, FTS5, CLI commands all in place.","dependencies":[{"issue_id":"MyCareersFuture-qwl.1","depends_on_id":"MyCareersFuture-qwl","type":"parent-child","created_at":"2026-02-05T17:56:21.950255+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-qwl.1.1","title":"Add semantic search dependencies to pyproject.toml","description":"# Task: Add Semantic Search Dependencies\n\n## What\nAdd the required Python packages to pyproject.toml for the semantic search system.\n\n## Dependencies to Add\n\n```toml\n[tool.poetry.dependencies]\n# Existing dependencies remain unchanged\nsentence-transformers = \"^2.2.0\"  # Embedding model wrapper\nfaiss-cpu = \"^1.7.4\"              # Vector similarity search\nfastapi = \"^0.104.0\"              # REST API framework\nuvicorn = {extras = [\"standard\"], version = \"^0.24.0\"}  # ASGI server\nnumpy = \"^1.24.0\"                 # Array operations (may already be transitive)\ncachetools = \"^5.3.0\"             # LRU caching for query embeddings\nscikit-learn = \"^1.3.0\"           # K-means for company clustering\n```\n\n## Why These Specific Versions\n- `sentence-transformers ^2.2.0`: Stable release with all-MiniLM-L6-v2 model\n- `faiss-cpu ^1.7.4`: Latest stable, IVFFlat index support\n- `fastapi ^0.104.0`: Modern async support, auto-generated OpenAPI docs\n- `uvicorn standard`: Includes uvloop for better async performance\n\n## Considerations\n\n### Why faiss-cpu vs faiss-gpu?\n- Most dev machines don't have CUDA GPUs\n- CPU is fast enough for our scale (100K-1M vectors)\n- Users with GPUs can manually install faiss-gpu later\n\n### Why sentence-transformers vs direct Hugging Face?\n- Simpler API for embedding generation\n- Pre-trained models work out of the box\n- Handles batching and normalization automatically\n\n### Potential Conflicts\n- numpy version may conflict with pandas - check compatibility\n- torch (sentence-transformers dependency) is large (~2GB)\n- First `poetry install` will take several minutes\n\n## Verification\n```bash\npoetry install\npython -c \"from sentence_transformers import SentenceTransformer; print('OK')\"\npython -c \"import faiss; print(f'FAISS version: {faiss.__version__}')\"\npython -c \"import fastapi; print(f'FastAPI version: {fastapi.__version__}')\"\n```\n\n## Notes for Future Self\n- If installation fails on ARM Mac, may need: `pip install faiss-cpu --no-cache-dir`\n- Model download happens on first use, not at import time\n- Consider pinning exact versions before production deployment","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T17:56:45.978462+08:00","created_by":"David Ten","updated_at":"2026-02-05T23:09:16.090513+08:00","closed_at":"2026-02-05T23:09:16.090513+08:00","close_reason":"Added semantic search dependencies with torch pinned to \u003c2.3.0 for macOS x86_64 wheel availability. All imports verified.","dependencies":[{"issue_id":"MyCareersFuture-qwl.1.1","depends_on_id":"MyCareersFuture-qwl.1","type":"parent-child","created_at":"2026-02-05T17:56:45.979185+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-qwl.1.2","title":"Extend database schema for embeddings and FTS5","description":"# Task: Extend Database Schema for Embeddings and FTS5\n\n## What\nModify src/mcf/database.py to add new tables, virtual tables, and methods for the semantic search system.\n\n## Schema Changes\n\n### 1. Embeddings Table\n```sql\nCREATE TABLE IF NOT EXISTS embeddings (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    entity_id TEXT NOT NULL,           -- UUID for jobs, name for skills/companies\n    entity_type TEXT NOT NULL,         -- 'job', 'skill', 'company'\n    embedding_blob BLOB NOT NULL,      -- Serialized numpy array (384 × 4 = 1536 bytes)\n    model_version TEXT DEFAULT 'all-MiniLM-L6-v2',\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    UNIQUE(entity_id, entity_type)\n);\n\nCREATE INDEX IF NOT EXISTS idx_embeddings_entity ON embeddings(entity_id, entity_type);\nCREATE INDEX IF NOT EXISTS idx_embeddings_type ON embeddings(entity_type);\nCREATE INDEX IF NOT EXISTS idx_embeddings_model ON embeddings(model_version);\n```\n\n### 2. FTS5 Virtual Table for Full-Text Search\n```sql\nCREATE VIRTUAL TABLE IF NOT EXISTS jobs_fts USING fts5(\n    uuid,\n    title,\n    description,\n    skills,\n    company_name,\n    content='jobs',\n    content_rowid='id'\n);\n\n-- Triggers to keep FTS in sync\nCREATE TRIGGER IF NOT EXISTS jobs_ai AFTER INSERT ON jobs BEGIN\n    INSERT INTO jobs_fts(rowid, uuid, title, description, skills, company_name)\n    VALUES (new.id, new.uuid, new.title, new.description, new.skills, new.company_name);\nEND;\n\nCREATE TRIGGER IF NOT EXISTS jobs_ad AFTER DELETE ON jobs BEGIN\n    INSERT INTO jobs_fts(jobs_fts, rowid, uuid, title, description, skills, company_name)\n    VALUES ('delete', old.id, old.uuid, old.title, old.description, old.skills, old.company_name);\nEND;\n\nCREATE TRIGGER IF NOT EXISTS jobs_au AFTER UPDATE ON jobs BEGIN\n    INSERT INTO jobs_fts(jobs_fts, rowid, uuid, title, description, skills, company_name)\n    VALUES ('delete', old.id, old.uuid, old.title, old.description, old.skills, old.company_name);\n    INSERT INTO jobs_fts(rowid, uuid, title, description, skills, company_name)\n    VALUES (new.id, new.uuid, new.title, new.description, new.skills, new.company_name);\nEND;\n```\n\n### 3. Search Analytics Table\n```sql\nCREATE TABLE IF NOT EXISTS search_analytics (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    query TEXT NOT NULL,\n    query_type TEXT DEFAULT 'semantic',\n    result_count INTEGER,\n    latency_ms REAL,\n    cache_hit BOOLEAN DEFAULT FALSE,\n    degraded BOOLEAN DEFAULT FALSE,\n    filters_used TEXT,\n    searched_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE INDEX IF NOT EXISTS idx_analytics_time ON search_analytics(searched_at);\nCREATE INDEX IF NOT EXISTS idx_analytics_query ON search_analytics(query);\n```\n\n### 4. Salary Normalization Columns (with Migration)\n```python\ndef _migrate_salary_annual(self):\n    \"\"\"\n    Add salary_annual columns if they don't exist.\n    \n    This handles the ALTER TABLE gracefully for existing databases.\n    \"\"\"\n    with self._connection() as conn:\n        # Check if columns exist\n        cursor = conn.execute(\"PRAGMA table_info(jobs)\")\n        columns = {row[1] for row in cursor.fetchall()}\n        \n        if 'salary_annual_min' not in columns:\n            conn.execute(\"ALTER TABLE jobs ADD COLUMN salary_annual_min INTEGER\")\n            conn.execute(\"ALTER TABLE jobs ADD COLUMN salary_annual_max INTEGER\")\n            \n            # Populate for existing rows\n            conn.execute(\"\"\"\n                UPDATE jobs SET\n                    salary_annual_min = CASE salary_type\n                        WHEN 'Monthly' THEN salary_min * 12\n                        WHEN 'Yearly' THEN salary_min\n                        WHEN 'Hourly' THEN salary_min * 2080\n                        WHEN 'Daily' THEN salary_min * 260\n                        ELSE salary_min * 12  -- Assume monthly\n                    END,\n                    salary_annual_max = CASE salary_type\n                        WHEN 'Monthly' THEN salary_max * 12\n                        WHEN 'Yearly' THEN salary_max\n                        WHEN 'Hourly' THEN salary_max * 2080\n                        WHEN 'Daily' THEN salary_max * 260\n                        ELSE salary_max * 12\n                    END\n                WHERE salary_min IS NOT NULL OR salary_max IS NOT NULL\n            \"\"\")\n            conn.commit()\n```\n\n## New Methods\n\n### Embedding Methods\n```python\ndef upsert_embedding(self, entity_id: str, entity_type: str, \n                    embedding: np.ndarray, model_version: str = None) -\u003e None:\n    \"\"\"Insert or update an embedding.\"\"\"\n    blob = embedding.astype(np.float32).tobytes()\n    model = model_version or \"all-MiniLM-L6-v2\"\n    \n    with self._connection() as conn:\n        conn.execute(\"\"\"\n            INSERT INTO embeddings (entity_id, entity_type, embedding_blob, model_version, updated_at)\n            VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP)\n            ON CONFLICT(entity_id, entity_type) DO UPDATE SET\n                embedding_blob = excluded.embedding_blob,\n                model_version = excluded.model_version,\n                updated_at = CURRENT_TIMESTAMP\n        \"\"\", (entity_id, entity_type, blob, model))\n        conn.commit()\n\ndef get_embedding(self, entity_id: str, entity_type: str) -\u003e Optional[np.ndarray]:\n    \"\"\"Retrieve embedding as numpy array.\"\"\"\n    with self._connection() as conn:\n        row = conn.execute(\n            \"SELECT embedding_blob FROM embeddings WHERE entity_id = ? AND entity_type = ?\",\n            (entity_id, entity_type)\n        ).fetchone()\n    \n    if row:\n        return np.frombuffer(row[0], dtype=np.float32)\n    return None\n\ndef get_all_embeddings(self, entity_type: str) -\u003e tuple[list[str], np.ndarray]:\n    \"\"\"Get all embeddings of a type (IDs and stacked array).\"\"\"\n    with self._connection() as conn:\n        rows = conn.execute(\n            \"SELECT entity_id, embedding_blob FROM embeddings WHERE entity_type = ? ORDER BY id\",\n            (entity_type,)\n        ).fetchall()\n    \n    if not rows:\n        return [], np.array([])\n    \n    ids = [row[0] for row in rows]\n    embeddings = np.array([np.frombuffer(row[1], dtype=np.float32) for row in rows])\n    return ids, embeddings\n\ndef get_embeddings_for_uuids(self, uuids: list[str]) -\u003e dict[str, np.ndarray]:\n    \"\"\"Get embeddings for specific job UUIDs.\"\"\"\n    if not uuids:\n        return {}\n    \n    placeholders = \",\".join(\"?\" * len(uuids))\n    with self._connection() as conn:\n        rows = conn.execute(\n            f\"SELECT entity_id, embedding_blob FROM embeddings WHERE entity_type = 'job' AND entity_id IN ({placeholders})\",\n            uuids\n        ).fetchall()\n    \n    return {row[0]: np.frombuffer(row[1], dtype=np.float32) for row in rows}\n\ndef get_embedding_stats(self) -\u003e dict:\n    \"\"\"Count embeddings by type, coverage percentage.\"\"\"\n    with self._connection() as conn:\n        # Count by type\n        type_counts = {}\n        for row in conn.execute(\n            \"SELECT entity_type, COUNT(*) FROM embeddings GROUP BY entity_type\"\n        ):\n            type_counts[row[0]] = row[1]\n        \n        # Job coverage\n        total_jobs = conn.execute(\"SELECT COUNT(*) FROM jobs\").fetchone()[0]\n        jobs_with_embeddings = type_counts.get('job', 0)\n        \n        # Model version\n        model = conn.execute(\n            \"SELECT model_version FROM embeddings LIMIT 1\"\n        ).fetchone()\n    \n    return {\n        'job_embeddings': type_counts.get('job', 0),\n        'skill_embeddings': type_counts.get('skill', 0),\n        'company_embeddings': type_counts.get('company', 0),\n        'total_jobs': total_jobs,\n        'coverage_pct': (jobs_with_embeddings / total_jobs * 100) if total_jobs \u003e 0 else 0,\n        'model_version': model[0] if model else None\n    }\n\ndef delete_embeddings_for_model(self, model_version: str) -\u003e int:\n    \"\"\"Delete embeddings for a specific model version (for upgrades).\"\"\"\n    with self._connection() as conn:\n        cursor = conn.execute(\n            \"DELETE FROM embeddings WHERE model_version = ?\",\n            (model_version,)\n        )\n        conn.commit()\n        return cursor.rowcount\n```\n\n### FTS5 Methods\n```python\ndef bm25_search(self, query: str, limit: int = 100) -\u003e list[tuple[str, float]]:\n    \"\"\"Full-text search returning (uuid, bm25_score) tuples.\"\"\"\n    with self._connection() as conn:\n        rows = conn.execute(\"\"\"\n            SELECT uuid, bm25(jobs_fts) as score\n            FROM jobs_fts\n            WHERE jobs_fts MATCH ?\n            ORDER BY score\n            LIMIT ?\n        \"\"\", (query, limit)).fetchall()\n    \n    return [(row[0], row[1]) for row in rows]\n\ndef rebuild_fts_index(self) -\u003e None:\n    \"\"\"Rebuild FTS index from jobs table (for recovery).\"\"\"\n    with self._connection() as conn:\n        conn.execute(\"INSERT INTO jobs_fts(jobs_fts) VALUES('rebuild')\")\n        conn.commit()\n```\n\n### Analytics Methods\n```python\ndef log_search(self, query: str, query_type: str, result_count: int,\n              latency_ms: float, cache_hit: bool, degraded: bool,\n              filters_used: dict = None) -\u003e None:\n    \"\"\"Log a search query for analytics.\"\"\"\n    import json\n    filters_json = json.dumps(filters_used) if filters_used else None\n    \n    with self._connection() as conn:\n        conn.execute(\"\"\"\n            INSERT INTO search_analytics \n            (query, query_type, result_count, latency_ms, cache_hit, degraded, filters_used)\n            VALUES (?, ?, ?, ?, ?, ?, ?)\n        \"\"\", (query, query_type, result_count, latency_ms, cache_hit, degraded, filters_json))\n        conn.commit()\n\ndef get_popular_queries(self, days: int = 7, limit: int = 20) -\u003e list[dict]:\n    \"\"\"Get most popular queries in the last N days.\"\"\"\n    with self._connection() as conn:\n        rows = conn.execute(\"\"\"\n            SELECT query, COUNT(*) as count, AVG(latency_ms) as avg_latency\n            FROM search_analytics\n            WHERE searched_at \u003e datetime('now', ?)\n            GROUP BY query\n            ORDER BY count DESC\n            LIMIT ?\n        \"\"\", (f'-{days} days', limit)).fetchall()\n    \n    return [{'query': r[0], 'count': r[1], 'avg_latency_ms': r[2]} for r in rows]\n\ndef get_search_latency_percentiles(self, days: int = 7) -\u003e dict:\n    \"\"\"Get p50, p90, p95, p99 latency statistics.\"\"\"\n    with self._connection() as conn:\n        rows = conn.execute(\"\"\"\n            SELECT latency_ms FROM search_analytics\n            WHERE searched_at \u003e datetime('now', ?)\n            ORDER BY latency_ms\n        \"\"\", (f'-{days} days',)).fetchall()\n    \n    if not rows:\n        return {'p50': 0, 'p90': 0, 'p95': 0, 'p99': 0, 'count': 0}\n    \n    latencies = [r[0] for r in rows]\n    n = len(latencies)\n    \n    return {\n        'p50': latencies[int(n * 0.5)],\n        'p90': latencies[int(n * 0.9)],\n        'p95': latencies[int(n * 0.95)],\n        'p99': latencies[min(int(n * 0.99), n - 1)],\n        'count': n\n    }\n```\n\n### Company Stats Method (NEW - used by similar companies)\n```python\ndef get_company_stats(self, company_name: str) -\u003e dict:\n    \"\"\"\n    Get statistics for a company.\n    \n    Used by similar companies endpoint.\n    \"\"\"\n    with self._connection() as conn:\n        row = conn.execute(\"\"\"\n            SELECT \n                COUNT(*) as job_count,\n                AVG(salary_annual_min) as avg_salary_min,\n                AVG(salary_annual_max) as avg_salary_max\n            FROM jobs\n            WHERE company_name = ?\n        \"\"\", (company_name,)).fetchone()\n        \n        # Get top skills\n        skills_row = conn.execute(\"\"\"\n            SELECT skills FROM jobs WHERE company_name = ? AND skills IS NOT NULL\n        \"\"\", (company_name,)).fetchall()\n    \n    # Parse and count skills\n    from collections import Counter\n    skill_counts = Counter()\n    for r in skills_row:\n        skills = [s.strip() for s in r[0].split(',') if s.strip()]\n        skill_counts.update(skills)\n    \n    top_skills = [s for s, _ in skill_counts.most_common(10)]\n    \n    avg_salary = None\n    if row[1] and row[2]:\n        avg_salary = int((row[1] + row[2]) / 2)\n    \n    return {\n        'job_count': row[0],\n        'avg_salary': avg_salary,\n        'top_skills': top_skills\n    }\n\ndef get_all_unique_skills(self) -\u003e list[str]:\n    \"\"\"Extract all unique skills from job postings.\"\"\"\n    with self._connection() as conn:\n        rows = conn.execute(\n            \"SELECT DISTINCT skills FROM jobs WHERE skills IS NOT NULL AND skills != ''\"\n        ).fetchall()\n    \n    skills_set = set()\n    for row in rows:\n        skills = [s.strip() for s in row[0].split(',')]\n        skills_set.update(s for s in skills if s)\n    \n    return sorted(list(skills_set))\n\ndef get_skill_frequencies(self, min_jobs: int = 1, limit: int = 100) -\u003e list[tuple[str, int]]:\n    \"\"\"Get skill frequencies for visualization.\"\"\"\n    with self._connection() as conn:\n        rows = conn.execute(\n            \"SELECT skills FROM jobs WHERE skills IS NOT NULL AND skills != ''\"\n        ).fetchall()\n    \n    from collections import Counter\n    skill_counts = Counter()\n    for row in rows:\n        skills = [s.strip() for s in row[0].split(',')]\n        skill_counts.update(s for s in skills if s)\n    \n    filtered = [(skill, count) for skill, count in skill_counts.items()\n                if count \u003e= min_jobs]\n    filtered.sort(key=lambda x: x[1], reverse=True)\n    \n    return filtered[:limit]\n```\n\n## Migration Strategy\n\nThe `_ensure_tables` method should:\n1. Create new tables (safe - IF NOT EXISTS)\n2. Call `_migrate_salary_annual()` for ALTER TABLE\n3. Populate FTS5 if empty: `INSERT INTO jobs_fts(jobs_fts) VALUES('rebuild')`\n\n```python\ndef _ensure_tables(self):\n    \"\"\"Create tables and run migrations.\"\"\"\n    with self._connection() as conn:\n        # Create new tables (existing create statements)\n        conn.executescript(EMBEDDINGS_SCHEMA)\n        conn.executescript(FTS5_SCHEMA)\n        conn.executescript(ANALYTICS_SCHEMA)\n        conn.commit()\n    \n    # Run migrations\n    self._migrate_salary_annual()\n    \n    # Populate FTS5 if empty\n    with self._connection() as conn:\n        count = conn.execute(\"SELECT COUNT(*) FROM jobs_fts\").fetchone()[0]\n        if count == 0:\n            jobs_count = conn.execute(\"SELECT COUNT(*) FROM jobs\").fetchone()[0]\n            if jobs_count \u003e 0:\n                logger.info(\"Rebuilding FTS5 index...\")\n                conn.execute(\"INSERT INTO jobs_fts(jobs_fts) VALUES('rebuild')\")\n                conn.commit()\n```\n\n## Testing\n- Insert test job, verify FTS trigger fires\n- BM25 search for known term, verify ranking\n- Store/retrieve embedding, verify round-trip fidelity\n- Salary normalization for Monthly, Yearly, Hourly\n- Company stats for known company","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T17:57:18.82245+08:00","created_by":"David Ten","updated_at":"2026-02-05T23:41:13.828724+08:00","closed_at":"2026-02-05T23:41:13.828724+08:00","close_reason":"Implemented all schema changes:\n- Embeddings table with upsert/get/batch methods\n- FTS5 virtual table with sync triggers  \n- Search analytics table with logging and percentile stats\n- Salary annual columns with migration for existing data\n- Company stats and skill extraction methods\nAll verified working on production database (94,695 jobs migrated)","dependencies":[{"issue_id":"MyCareersFuture-qwl.1.2","depends_on_id":"MyCareersFuture-qwl.1","type":"parent-child","created_at":"2026-02-05T17:57:18.823303+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-qwl.1.2","depends_on_id":"MyCareersFuture-qwl.1.1","type":"blocks","created_at":"2026-02-05T18:00:56.577142+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-qwl.1.3","title":"Create EmbeddingGenerator class","description":"# Task: Create EmbeddingGenerator Class\n\n## What\nCreate src/mcf/embeddings/generator.py with the core EmbeddingGenerator class that transforms job text into vector embeddings. **This also includes skill extraction and clustering** to support query expansion.\n\n## Class Design\n\n```python\nclass EmbeddingGenerator:\n    \"\"\"\n    Generates semantic embeddings for jobs, skills, and companies.\n    \n    Uses Sentence Transformers with all-MiniLM-L6-v2 model (384 dimensions).\n    Designed for batch processing with progress tracking.\n    \n    Also handles:\n    - Skill extraction from all jobs\n    - Skill clustering for query expansion\n    - Company multi-centroid embedding preparation\n    \n    Example:\n        generator = EmbeddingGenerator()\n        embedding = generator.generate_job_embedding(job)\n        \n        # Batch processing with skill clustering\n        async for stats in generator.generate_all(db):\n            print(f\"Progress: {stats.jobs_processed}\")\n    \"\"\"\n    \n    MODEL_NAME = \"all-MiniLM-L6-v2\"\n    DIMENSION = 384\n    \n    def __init__(self, model_name: str = None, device: str = None):\n        \"\"\"\n        Initialize generator with lazy model loading.\n        \n        Args:\n            model_name: Override default model\n            device: 'cpu', 'cuda', or None for auto-detect\n        \"\"\"\n        self._model = None  # Lazy loaded\n        self.model_name = model_name or self.MODEL_NAME\n        self.device = device\n    \n    @property\n    def model(self) -\u003e SentenceTransformer:\n        \"\"\"Lazy load model on first use.\"\"\"\n        if self._model is None:\n            self._model = SentenceTransformer(self.model_name, device=self.device)\n        return self._model\n    \n    def generate_job_embedding(self, job: Job) -\u003e np.ndarray:\n        \"\"\"Generate embedding for a single job.\"\"\"\n        \n    def generate_job_embeddings_batch(self, jobs: list[Job], \n                                      batch_size: int = 32) -\u003e np.ndarray:\n        \"\"\"Generate embeddings for multiple jobs efficiently.\"\"\"\n        \n    def generate_skill_embedding(self, skill: str) -\u003e np.ndarray:\n        \"\"\"Generate embedding for a single skill.\"\"\"\n        \n    def generate_skill_embeddings_batch(self, skills: list[str]) -\u003e dict[str, np.ndarray]:\n        \"\"\"Generate embeddings for multiple skills.\"\"\"\n        \n    def cluster_skills(self, skills: list[str], \n                      n_clusters: int = None) -\u003e dict:\n        \"\"\"\n        Cluster skills by embedding similarity for query expansion.\n        \n        Returns:\n            {\n                'clusters': {cluster_id: [skill_names]},\n                'skill_to_cluster': {skill_name: cluster_id},\n                'cluster_centroids': {cluster_id: centroid_embedding}\n            }\n        \"\"\"\n        \n    def generate_company_embeddings(self, \n                                   company_jobs: dict[str, list[Job]],\n                                   k_centroids: int = 3) -\u003e dict[str, list[np.ndarray]]:\n        \"\"\"\n        Generate multi-centroid embeddings for companies.\n        \n        For companies with \u003e= 10 jobs: k-means clustering\n        For companies with \u003c 10 jobs: single weighted centroid\n        \"\"\"\n        \n    async def generate_all(self, \n                          db: MCFDatabase,\n                          batch_size: int = 32,\n                          skip_existing: bool = True,\n                          progress_callback = None) -\u003e EmbeddingStats:\n        \"\"\"\n        Generate all embeddings: jobs, skills (with clustering), companies.\n        \n        This is the main entry point for batch embedding generation.\n        Includes skill clustering for query expansion support.\n        \"\"\"\n```\n\n## Composite Text Strategy\n\n**Text template:**\n```python\ndef _compose_job_text(self, job: Job) -\u003e str:\n    \"\"\"Create composite text for embedding.\"\"\"\n    parts = [\n        job.title,           # Include title\n        job.title,           # Repeat for 2x weight\n        job.description_text[:500],  # Truncated description\n        job.skills_list,     # Comma-separated skills\n        job.categories_list, # Comma-separated categories\n    ]\n    return \" \".join(filter(None, parts))\n```\n\n**Why title 2x?**\n- Title is the most discriminative signal\n- \"Data Scientist\" vs \"Software Engineer\" is obvious from title\n- Description often has boilerplate that dilutes signal\n\n## Skill Clustering (NEW - for Query Expansion)\n\n```python\ndef cluster_skills(self, skills: list[str], n_clusters: int = None) -\u003e dict:\n    \"\"\"\n    Cluster skills by embedding similarity.\n    \n    This enables query expansion: \"ML\" -\u003e [\"ML\", \"Machine Learning\", \"Deep Learning\"]\n    \n    Uses agglomerative clustering with cosine distance for better cluster quality.\n    \"\"\"\n    from sklearn.cluster import AgglomerativeClustering\n    \n    if len(skills) \u003c 3:\n        return {\n            'clusters': {0: skills},\n            'skill_to_cluster': {s: 0 for s in skills},\n            'cluster_centroids': {}\n        }\n    \n    # Generate embeddings\n    embeddings = np.array([self.generate_skill_embedding(s) for s in skills])\n    \n    # Normalize for cosine distance\n    normalized = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n    \n    # Auto-compute n_clusters (~5 skills per cluster)\n    if n_clusters is None:\n        n_clusters = max(1, min(len(skills) // 5, 100))\n    \n    # Cluster\n    clustering = AgglomerativeClustering(\n        n_clusters=n_clusters,\n        metric='cosine',\n        linkage='average'\n    )\n    labels = clustering.fit_predict(normalized)\n    \n    # Build result structures\n    clusters = defaultdict(list)\n    skill_to_cluster = {}\n    cluster_centroids = {}\n    \n    for skill, label, embedding in zip(skills, labels, embeddings):\n        clusters[int(label)].append(skill)\n        skill_to_cluster[skill] = int(label)\n    \n    # Compute cluster centroids\n    for cluster_id, cluster_skills in clusters.items():\n        indices = [skills.index(s) for s in cluster_skills]\n        centroid = embeddings[indices].mean(axis=0)\n        cluster_centroids[cluster_id] = centroid\n    \n    return {\n        'clusters': dict(clusters),\n        'skill_to_cluster': skill_to_cluster,\n        'cluster_centroids': cluster_centroids\n    }\n```\n\n## Extended generate_all with Skill Clustering\n\n```python\nasync def generate_all(self, db: MCFDatabase, ...) -\u003e EmbeddingStats:\n    \"\"\"\n    Generate all embeddings including skill clustering.\n    \n    Steps:\n    1. Generate job embeddings (batched)\n    2. Extract unique skills from all jobs\n    3. Generate skill embeddings\n    4. Cluster skills for query expansion\n    5. Save cluster data to disk\n    \"\"\"\n    stats = EmbeddingStats(...)\n    \n    # Step 1: Job embeddings\n    jobs = db.get_all_jobs()\n    for batch in batched(jobs, batch_size):\n        embeddings = self.generate_job_embeddings_batch(batch)\n        for job, emb in zip(batch, embeddings):\n            db.upsert_embedding(job.uuid, \"job\", emb)\n        stats.jobs_processed += len(batch)\n        if progress_callback:\n            progress_callback(stats)\n    \n    # Step 2-4: Skill extraction and clustering\n    logger.info(\"Extracting and clustering skills...\")\n    skills = db.get_all_unique_skills()\n    stats.unique_skills = len(skills)\n    \n    if skills:\n        cluster_result = self.cluster_skills(skills)\n        stats.skill_clusters = len(cluster_result['clusters'])\n        \n        # Store skill embeddings\n        for skill in skills:\n            embedding = self.generate_skill_embedding(skill)\n            db.upsert_embedding(skill, \"skill\", embedding)\n        \n        # Save cluster data for QueryExpander\n        self._save_skill_clusters(cluster_result)\n    \n    return stats\n\ndef _save_skill_clusters(self, cluster_result: dict, \n                        path: Path = Path(\"data/embeddings\")):\n    \"\"\"Save skill cluster data to disk for QueryExpander.\"\"\"\n    path.mkdir(parents=True, exist_ok=True)\n    \n    import pickle\n    \n    with open(path / \"skill_clusters.pkl\", \"wb\") as f:\n        pickle.dump(cluster_result['clusters'], f)\n    \n    with open(path / \"skill_to_cluster.pkl\", \"wb\") as f:\n        pickle.dump(cluster_result['skill_to_cluster'], f)\n    \n    # Save cluster centroids\n    with open(path / \"skill_cluster_centroids.pkl\", \"wb\") as f:\n        pickle.dump(cluster_result['cluster_centroids'], f)\n```\n\n## Company Multi-Centroid Algorithm\n\n```python\ndef generate_company_embeddings(self, company_jobs: dict[str, list[Job]],\n                               k_centroids: int = 3) -\u003e dict[str, list[np.ndarray]]:\n    \"\"\"\n    Generate multi-centroid embeddings for companies.\n    \n    For companies with \u003e= 10 jobs: k-means clustering into job families\n    For companies with \u003c 10 jobs: single weighted centroid\n    \"\"\"\n    from sklearn.cluster import KMeans\n    \n    company_centroids = {}\n    \n    for company, jobs in company_jobs.items():\n        # Get job embeddings\n        job_embeddings = np.array([\n            self.generate_job_embedding(job) for job in jobs\n        ])\n        \n        if len(jobs) \u003c 10:\n            # Single weighted centroid (recent jobs weighted higher)\n            weights = self._compute_recency_weights(jobs)\n            centroid = np.average(job_embeddings, weights=weights, axis=0)\n            company_centroids[company] = [centroid]\n        else:\n            # K-means clustering\n            actual_k = min(k_centroids, len(jobs) // 3)\n            kmeans = KMeans(n_clusters=actual_k, random_state=42, n_init=10)\n            kmeans.fit(job_embeddings)\n            company_centroids[company] = list(kmeans.cluster_centers_)\n    \n    return company_centroids\n```\n\n## Progress Tracking\n\n```python\n@dataclass\nclass EmbeddingStats:\n    \"\"\"Statistics from embedding generation.\"\"\"\n    jobs_total: int = 0\n    jobs_processed: int = 0\n    jobs_skipped: int = 0\n    jobs_failed: int = 0\n    unique_skills: int = 0\n    skill_clusters: int = 0\n    companies_processed: int = 0\n    elapsed_seconds: float = 0.0\n    \n    @property\n    def jobs_per_second(self) -\u003e float:\n        if self.elapsed_seconds \u003e 0:\n            return self.jobs_processed / self.elapsed_seconds\n        return 0.0\n```\n\n## Testing\n\n```python\ndef test_embedding_generator():\n    generator = EmbeddingGenerator()\n    \n    # Test single job\n    job = create_test_job(title=\"Data Scientist\", skills=\"Python, ML\")\n    embedding = generator.generate_job_embedding(job)\n    assert embedding.shape == (384,)\n    assert np.isclose(np.linalg.norm(embedding), 1.0)  # Normalized\n    \n    # Test similarity\n    job1 = create_test_job(title=\"ML Engineer\")\n    job2 = create_test_job(title=\"Machine Learning Specialist\")\n    job3 = create_test_job(title=\"Accountant\")\n    \n    e1 = generator.generate_job_embedding(job1)\n    e2 = generator.generate_job_embedding(job2)\n    e3 = generator.generate_job_embedding(job3)\n    \n    # ML jobs should be more similar to each other than to Accountant\n    sim_12 = np.dot(e1, e2)\n    sim_13 = np.dot(e1, e3)\n    assert sim_12 \u003e sim_13\n\ndef test_skill_clustering():\n    generator = EmbeddingGenerator()\n    \n    skills = [\"Python\", \"Java\", \"JavaScript\", \"Machine Learning\", \"Deep Learning\", \"SQL\"]\n    result = generator.cluster_skills(skills, n_clusters=2)\n    \n    assert len(result['clusters']) == 2\n    assert sum(len(v) for v in result['clusters'].values()) == 6\n    assert all(s in result['skill_to_cluster'] for s in skills)\n    \n    # ML and DL should be in same cluster\n    ml_cluster = result['skill_to_cluster']['Machine Learning']\n    dl_cluster = result['skill_to_cluster']['Deep Learning']\n    assert ml_cluster == dl_cluster\n```\n\n## Files to Create\n- `src/mcf/embeddings/__init__.py`: Package exports\n- `src/mcf/embeddings/generator.py`: Main generator class with skill clustering\n- `src/mcf/embeddings/models.py`: EmbeddingStats and related Pydantic models\n\n## Output Files\nAfter running embed-generate:\n```\ndata/embeddings/\n├── skill_clusters.pkl          # Cluster ID -\u003e skill names\n├── skill_to_cluster.pkl        # Skill name -\u003e cluster ID\n└── skill_cluster_centroids.pkl # Cluster ID -\u003e centroid embedding\n```","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T17:59:25.103292+08:00","created_by":"David Ten","updated_at":"2026-02-06T00:05:34.848333+08:00","closed_at":"2026-02-06T00:05:34.848333+08:00","close_reason":"Created EmbeddingGenerator class with job/skill/company embedding support, skill clustering, batch processing, and database integration. All tests pass.","dependencies":[{"issue_id":"MyCareersFuture-qwl.1.3","depends_on_id":"MyCareersFuture-qwl.1","type":"parent-child","created_at":"2026-02-05T17:59:25.104141+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-qwl.1.3","depends_on_id":"MyCareersFuture-qwl.1.2","type":"blocks","created_at":"2026-02-05T18:00:56.364738+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-qwl.1.3","depends_on_id":"MyCareersFuture-qwl.1.1","type":"blocks","created_at":"2026-02-05T18:00:56.763589+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-qwl.1.4","title":"Add embed-* CLI commands","description":"# Task: Add embed-* CLI Commands\n\n## What\nAdd four new CLI commands to src/cli.py for managing embeddings.\n\n## Commands to Implement\n\n### 1. embed-generate\n```python\n@app.command(name=\"embed-generate\")\ndef generate_embeddings(\n    batch_size: int = typer.Option(32, \"--batch-size\", \"-b\",\n        help=\"Jobs to process in each batch\"),\n    skip_existing: bool = typer.Option(True, \"--skip-existing/--no-skip-existing\",\n        help=\"Skip jobs that already have embeddings\"),\n    db_path: str = typer.Option(\"data/mcf_jobs.db\", \"--db\",\n        help=\"Path to SQLite database\"),\n    verbose: bool = typer.Option(False, \"--verbose\", \"-v\",\n        help=\"Enable debug logging\"),\n) -\u003e None:\n    \"\"\"\n    Generate embeddings for all jobs in the database.\n    \n    This preprocesses all jobs to create semantic embeddings.\n    Run this once after initial data load, then use embed-sync for updates.\n    \n    Examples:\n        mcf embed-generate\n        mcf embed-generate --batch-size 64\n        mcf embed-generate --no-skip-existing  # Regenerate all\n    \"\"\"\n```\n\n**Output Format (Rich):**\n```\nGenerating Embeddings\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nModel: all-MiniLM-L6-v2 (384 dimensions)\nDatabase: data/mcf_jobs.db\n\nJobs:     ████████████████████████████████ 100% 150,234/150,234\nSkills:   ████████████████████████████████ 100% 3,842/3,842\nCompanies: ████████████████████████████████ 100% 12,456/12,456\n\n✓ Embedding generation complete!\n\nSummary:\n  Jobs processed:      150,234\n  Jobs skipped:        0\n  Skills extracted:    3,842\n  Companies clustered: 12,456\n  Total time:          12m 34s\n  Speed:               199.2 jobs/sec\n```\n\n### 2. embed-sync\n```python\n@app.command(name=\"embed-sync\")\ndef sync_embeddings(\n    batch_size: int = typer.Option(32, \"--batch-size\", \"-b\"),\n    db_path: str = typer.Option(\"data/mcf_jobs.db\", \"--db\"),\n    verbose: bool = typer.Option(False, \"--verbose\", \"-v\"),\n) -\u003e None:\n    \"\"\"\n    Generate embeddings for new jobs (incremental update).\n    \n    Only processes jobs that don't have embeddings yet.\n    Run this after each scrape to keep embeddings up-to-date.\n    \n    Examples:\n        mcf embed-sync\n        mcf scrape \"data scientist\" \u0026\u0026 mcf embed-sync  # Chain commands\n    \"\"\"\n```\n\n**Purpose:** Efficient incremental updates after scraping. Much faster than regenerating everything.\n\n### 3. embed-status\n```python\n@app.command(name=\"embed-status\")\ndef embedding_status(\n    db_path: str = typer.Option(\"data/mcf_jobs.db\", \"--db\"),\n) -\u003e None:\n    \"\"\"\n    Show embedding generation status and coverage.\n    \n    Displays statistics about embeddings in the database.\n    \n    Example:\n        mcf embed-status\n    \"\"\"\n```\n\n**Output Format:**\n```\nEmbedding Status\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nJobs:\n  Total in database:     150,234\n  With embeddings:       150,234\n  Coverage:              100.0%\n  Model version:         all-MiniLM-L6-v2\n\nSkills:\n  Unique skills:         3,842\n  With embeddings:       3,842\n\nCompanies:\n  Unique companies:      12,456\n  With embeddings:       12,456\n  Avg centroids/company: 2.3\n\nFAISS Indexes:\n  jobs.index:            ✓ exists (582 MB)\n  skills.index:          ✓ exists (1.4 MB)\n  companies.index:       ✓ exists (18 MB)\n\nLast updated: 2024-01-15 14:32:00\n```\n\n### 4. embed-upgrade\n```python\n@app.command(name=\"embed-upgrade\")\ndef upgrade_embeddings(\n    model: str = typer.Argument(..., help=\"New model name\"),\n    batch_size: int = typer.Option(32, \"--batch-size\", \"-b\"),\n    db_path: str = typer.Option(\"data/mcf_jobs.db\", \"--db\"),\n    verbose: bool = typer.Option(False, \"--verbose\", \"-v\"),\n    confirm: bool = typer.Option(False, \"--yes\", \"-y\",\n        help=\"Skip confirmation prompt\"),\n) -\u003e None:\n    \"\"\"\n    Re-generate all embeddings with a new model version.\n    \n    This will:\n    1. Delete all existing embeddings\n    2. Generate new embeddings with the specified model\n    3. Rebuild FAISS indexes\n    \n    Use this when upgrading to a better embedding model.\n    \n    Examples:\n        mcf embed-upgrade all-mpnet-base-v2\n        mcf embed-upgrade all-MiniLM-L12-v2 --yes\n    \"\"\"\n```\n\n**Confirmation Prompt:**\n```\n⚠️  This will regenerate ALL embeddings!\n\nCurrent model: all-MiniLM-L6-v2\nNew model:     all-mpnet-base-v2\nJobs to re-embed: 150,234\n\nThis operation will take approximately 25 minutes.\nContinue? [y/N]:\n```\n\n## Implementation Details\n\n### Progress Bar Setup\n```python\nfrom rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeElapsedColumn\n\ndef _create_progress():\n    return Progress(\n        SpinnerColumn(),\n        TextColumn(\"[progress.description]{task.description}\"),\n        BarColumn(),\n        TextColumn(\"[progress.percentage]{task.percentage:\u003e3.0f}%\"),\n        TextColumn(\"{task.completed:,}/{task.total:,}\"),\n        TimeElapsedColumn(),\n    )\n```\n\n### Async Wrapper\n```python\n@app.command(name=\"embed-generate\")\ndef generate_embeddings(...):\n    \"\"\"...\"\"\"\n    setup_logging(verbose)\n    \n    async def run():\n        generator = EmbeddingGenerator()\n        db = MCFDatabase(db_path)\n        \n        with _create_progress() as progress:\n            task_id = progress.add_task(\"Jobs\", total=0)\n            \n            async for stats in generator.generate_all(db, batch_size, skip_existing):\n                progress.update(task_id, \n                    total=stats.jobs_total,\n                    completed=stats.jobs_processed)\n        \n        # Print summary table\n        _print_summary(stats)\n    \n    asyncio.run(run())\n```\n\n### Error Handling\n- Catch keyboard interrupt, save progress\n- Log failed jobs but continue processing\n- Show warning if \u003c90% coverage achieved\n\n## Integration with Existing CLI\n\nThe existing CLI structure in src/cli.py uses:\n- Typer for command definition\n- Rich for output formatting\n- asyncio.run() for async operations\n\nFollow the same patterns as existing commands (scrape, list, search, etc.)\n\n## Testing\n\n```bash\n# Test basic generation\nmcf embed-generate --db test.db\n\n# Test incremental sync\nmcf embed-sync --db test.db\n\n# Test status display\nmcf embed-status --db test.db\n\n# Test upgrade (with test model)\nmcf embed-upgrade paraphrase-MiniLM-L6-v2 --db test.db --yes\n```\n\n## Dependencies\n- Requires EmbeddingGenerator class (MyCareersFuture-qwl.1.3)\n- Requires database schema extensions (MyCareersFuture-qwl.1.2)","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T18:00:35.296395+08:00","created_by":"David Ten","updated_at":"2026-02-06T00:36:51.035593+08:00","closed_at":"2026-02-06T00:36:51.035593+08:00","close_reason":"Implemented four CLI commands: embed-generate, embed-sync, embed-status, embed-upgrade","dependencies":[{"issue_id":"MyCareersFuture-qwl.1.4","depends_on_id":"MyCareersFuture-qwl.1","type":"parent-child","created_at":"2026-02-05T18:00:35.297253+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-qwl.1.4","depends_on_id":"MyCareersFuture-qwl.1.2","type":"blocks","created_at":"2026-02-05T18:00:56.960647+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-qwl.1.4","depends_on_id":"MyCareersFuture-qwl.1.3","type":"blocks","created_at":"2026-02-05T18:00:57.150542+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-qwl.2","title":"Phase 2: FAISS Index Management","description":"# Phase 2: FAISS Index Management\n\n## Purpose\nBuild the vector index layer that enables fast similarity search. FAISS (Facebook AI Similarity Search) provides efficient nearest-neighbor search for high-dimensional vectors.\n\n## Why This Phase Follows Phase 1\nWe need embeddings stored in the database before we can build indexes over them. Phase 1 provides the embedding generation; Phase 2 builds the searchable index structure.\n\n## Key Concepts\n\n### What is FAISS?\nFAISS is a library for efficient similarity search of dense vectors. It provides:\n- Multiple index types with different speed/accuracy tradeoffs\n- GPU acceleration (optional)\n- Index serialization for persistence\n- Batch search operations\n\n### Index Type Selection: IVFFlat\n\nWe chose **IVFFlat (Inverted File with Flat quantization)** because:\n\n| Factor | IVFFlat | Flat | HNSW |\n|--------|---------|------|------|\n| Search speed | O(n/k) | O(n) | O(log n) |\n| Memory | Low | Low | High |\n| Build time | Medium | Fast | Slow |\n| Recall | ~95%+ | 100% | ~99% |\n| Best for | 100K-10M | \u003c50K | \u003e10M |\n\nFor 100K-1M jobs, IVFFlat provides the best balance:\n- Fast enough (\u003c100ms for 1M vectors)\n- Good recall (adjustable via nprobe parameter)\n- Reasonable memory footprint\n- Simple to understand and tune\n\n### How IVFFlat Works\n1. **Training**: K-means clusters vectors into `nlist` partitions\n2. **Adding**: Each vector assigned to nearest cluster centroid\n3. **Searching**: Only search `nprobe` nearest clusters instead of all vectors\n\n**Tuning Parameters:**\n- `nlist`: Number of clusters (recommend sqrt(n), so 1000 for 1M jobs)\n- `nprobe`: Clusters to search (higher = slower but more accurate, recommend 10-50)\n\n## Components to Build\n\n### 2.1 FAISSIndexManager Class\nThe main class managing all FAISS indexes:\n- Job embeddings index (IVFFlat)\n- Skill embeddings index (Flat - small enough)\n- Company embeddings index (Flat - multi-centroid support)\n\n### 2.2 UUID Mapping\nFAISS uses sequential integer IDs. We need:\n- UUID → FAISS ID mapping for search\n- FAISS ID → UUID mapping for result translation\n- Persistence of mappings alongside indexes\n\n### 2.3 Index Operations\n- `build()`: Create index from embeddings\n- `add_with_ids()`: Incremental updates (for embed-sync)\n- `search()`: k-NN search\n- `save()`/`load()`: Persistence\n- `remove()`: Remove vectors (rare, but needed)\n\n### 2.4 Integration with embed-generate\nAfter embedding generation, automatically build/update indexes.\n\n## Technical Deep-Dive\n\n### Index Building\n```python\ndef build_job_index(self, embeddings: np.ndarray, uuids: list[str]) -\u003e None:\n    \"\"\"\n    Build IVFFlat index for job embeddings.\n    \n    Args:\n        embeddings: (N, 384) array of normalized embeddings\n        uuids: List of N UUIDs in same order\n    \"\"\"\n    n_vectors = len(embeddings)\n    dimension = embeddings.shape[1]\n    \n    # Choose nlist based on dataset size\n    nlist = min(4096, int(np.sqrt(n_vectors)))\n    nlist = max(nlist, 1)\n    \n    # Create quantizer and index\n    quantizer = faiss.IndexFlatL2(dimension)\n    index = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n    \n    # Train on sample (or all if small)\n    train_size = min(n_vectors, 100000)\n    train_data = embeddings[:train_size]\n    index.train(train_data)\n    \n    # Add all vectors\n    index.add(embeddings)\n    \n    # Store UUID mapping\n    self.uuid_to_idx = {uuid: i for i, uuid in enumerate(uuids)}\n    self.idx_to_uuid = uuids.copy()\n    \n    self.indexes['jobs'] = index\n```\n\n### Incremental Updates\n```python\ndef add_jobs(self, embeddings: np.ndarray, uuids: list[str]) -\u003e None:\n    \"\"\"Add new jobs to existing index.\"\"\"\n    if 'jobs' not in self.indexes:\n        raise ValueError(\"Index not built. Run build_job_index first.\")\n    \n    # Assign new sequential IDs\n    start_idx = len(self.idx_to_uuid)\n    \n    # Add to FAISS index\n    self.indexes['jobs'].add(embeddings)\n    \n    # Update mappings\n    for i, uuid in enumerate(uuids):\n        idx = start_idx + i\n        self.uuid_to_idx[uuid] = idx\n        self.idx_to_uuid.append(uuid)\n```\n\n### Search\n```python\ndef search_jobs(self, query_vector: np.ndarray, k: int = 10,\n               nprobe: int = 10) -\u003e list[tuple[str, float]]:\n    \"\"\"\n    Search for similar jobs.\n    \n    Args:\n        query_vector: (384,) normalized embedding\n        k: Number of results\n        nprobe: Clusters to search (accuracy/speed tradeoff)\n        \n    Returns:\n        List of (uuid, similarity_score) tuples, highest first\n    \"\"\"\n    index = self.indexes['jobs']\n    index.nprobe = nprobe\n    \n    # FAISS search\n    distances, indices = index.search(query_vector.reshape(1, -1), k)\n    \n    # Convert to (uuid, score) tuples\n    results = []\n    for dist, idx in zip(distances[0], indices[0]):\n        if idx \u003e= 0:  # -1 indicates no result\n            uuid = self.idx_to_uuid[idx]\n            # Convert L2 distance to similarity score [0, 1]\n            # For normalized vectors: L2 distance = 2(1 - cosine_similarity)\n            similarity = 1 - (dist / 2)\n            results.append((uuid, float(similarity)))\n    \n    return results\n```\n\n### Persistence\n```python\ndef save(self, index_dir: Path) -\u003e None:\n    \"\"\"Save all indexes and mappings to disk.\"\"\"\n    index_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Save FAISS indexes\n    for name, index in self.indexes.items():\n        if index is not None:\n            faiss.write_index(index, str(index_dir / f\"{name}.index\"))\n    \n    # Save UUID mappings\n    np.save(index_dir / \"jobs_uuids.npy\", np.array(self.idx_to_uuid))\n    \n    # Save model version for compatibility checks\n    with open(index_dir / \"model_version.txt\", \"w\") as f:\n        f.write(self.model_version)\n\ndef load(self, index_dir: Path) -\u003e None:\n    \"\"\"Load indexes and mappings from disk.\"\"\"\n    # Load FAISS indexes\n    jobs_path = index_dir / \"jobs.index\"\n    if jobs_path.exists():\n        self.indexes['jobs'] = faiss.read_index(str(jobs_path))\n    \n    # Load UUID mappings\n    uuids_path = index_dir / \"jobs_uuids.npy\"\n    if uuids_path.exists():\n        self.idx_to_uuid = np.load(uuids_path).tolist()\n        self.uuid_to_idx = {uuid: i for i, uuid in enumerate(self.idx_to_uuid)}\n```\n\n## Memory Considerations\n\nFor 1M jobs with 384-dim embeddings:\n- Embeddings: 1M × 384 × 4 bytes = 1.5 GB\n- IVFFlat index: ~1.6 GB (slight overhead for cluster data)\n- UUID mapping: ~50 MB (assuming 36-char UUIDs)\n- **Total: ~3.2 GB RAM when loaded**\n\nFor production, consider:\n- Memory-mapped indexes (`faiss.read_index(..., faiss.IO_FLAG_MMAP)`)\n- On-disk index with selective loading\n- Sharding by year/category if scale grows further\n\n## Files to Create\n- `src/mcf/embeddings/index_manager.py`: FAISSIndexManager class\n- Update `src/mcf/embeddings/__init__.py`: Export new class\n\n## Exit Criteria\n- [ ] Build job index for 10K test embeddings\n- [ ] Search returns correct similar jobs\n- [ ] Save/load round-trip works correctly\n- [ ] Incremental add works without full rebuild\n- [ ] UUID mapping is consistent after operations","status":"closed","priority":1,"issue_type":"epic","owner":"davidten7@gmail.com","created_at":"2026-02-05T18:02:20.647052+08:00","created_by":"David Ten","updated_at":"2026-02-06T01:09:08.250253+08:00","closed_at":"2026-02-06T01:09:08.250253+08:00","close_reason":"All exit criteria verified: 29 tests pass, FAISSIndexManager implemented with IVFFlat/Flat indexes, save/load persistence, incremental add_jobs, and CLI integration complete","dependencies":[{"issue_id":"MyCareersFuture-qwl.2","depends_on_id":"MyCareersFuture-qwl","type":"parent-child","created_at":"2026-02-05T18:02:20.647875+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-qwl.2","depends_on_id":"MyCareersFuture-qwl.1","type":"blocks","created_at":"2026-02-05T18:25:27.745043+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-qwl.2.1","title":"Implement FAISSIndexManager class","description":"# Task: Implement FAISSIndexManager Class\n\n## What\nCreate src/mcf/embeddings/index_manager.py with the FAISSIndexManager class that handles all FAISS index operations.\n\n## Class Interface\n\n```python\nclass FAISSIndexManager:\n    \"\"\"\n    Manages FAISS indexes for semantic search.\n    \n    Handles three index types:\n    - jobs: IVFFlat index for job embeddings\n    - skills: Flat index for skill embeddings  \n    - companies: Flat index for company centroids\n    \n    Example:\n        manager = FAISSIndexManager(index_dir=Path(\"data/embeddings\"))\n        manager.build_job_index(embeddings, uuids)\n        results = manager.search_jobs(query_vector, k=10)\n        manager.save()\n    \"\"\"\n    \n    def __init__(self, index_dir: Path = Path(\"data/embeddings\"),\n                 model_version: str = \"all-MiniLM-L6-v2\"):\n        \"\"\"\n        Initialize index manager.\n        \n        Args:\n            index_dir: Directory for index files\n            model_version: Embedding model version for compatibility\n        \"\"\"\n        \n    # ===== Build Methods =====\n    \n    def build_job_index(self, embeddings: np.ndarray, uuids: list[str],\n                       nlist: int = None) -\u003e None:\n        \"\"\"Build IVFFlat index for job embeddings.\"\"\"\n        \n    def build_skill_index(self, embeddings: np.ndarray, \n                         skill_names: list[str]) -\u003e None:\n        \"\"\"Build Flat index for skill embeddings.\"\"\"\n        \n    def build_company_index(self, company_centroids: dict[str, list[np.ndarray]]) -\u003e None:\n        \"\"\"Build index for company multi-centroids.\"\"\"\n        \n    # ===== Search Methods =====\n    \n    def search_jobs(self, query_vector: np.ndarray, k: int = 10,\n                   nprobe: int = 10) -\u003e list[tuple[str, float]]:\n        \"\"\"Search for similar jobs. Returns [(uuid, score), ...]\"\"\"\n        \n    def search_jobs_filtered(self, query_vector: np.ndarray, \n                            allowed_uuids: set[str], k: int = 10) -\u003e list[tuple[str, float]]:\n        \"\"\"Search only among specific UUIDs (for filter-then-search).\"\"\"\n        \n    def search_skills(self, query_vector: np.ndarray, \n                     k: int = 10) -\u003e list[tuple[str, float]]:\n        \"\"\"Search for similar skills.\"\"\"\n        \n    def search_companies(self, query_vector: np.ndarray,\n                        k: int = 10) -\u003e list[tuple[str, float]]:\n        \"\"\"Search for similar companies (max across centroids).\"\"\"\n        \n    # ===== Update Methods =====\n    \n    def add_jobs(self, embeddings: np.ndarray, uuids: list[str]) -\u003e None:\n        \"\"\"Add new jobs to existing index (incremental).\"\"\"\n        \n    def remove_jobs(self, uuids: list[str]) -\u003e None:\n        \"\"\"Remove jobs from index (requires rebuild for IVFFlat).\"\"\"\n        \n    # ===== Persistence =====\n    \n    def save(self) -\u003e None:\n        \"\"\"Save all indexes and mappings to disk.\"\"\"\n        \n    def load(self) -\u003e bool:\n        \"\"\"Load indexes from disk. Returns False if not found.\"\"\"\n        \n    def exists(self) -\u003e bool:\n        \"\"\"Check if saved indexes exist.\"\"\"\n        \n    # ===== Utilities =====\n    \n    def get_stats(self) -\u003e dict:\n        \"\"\"Get index statistics (size, memory usage, etc.).\"\"\"\n        \n    def is_compatible(self, model_version: str) -\u003e bool:\n        \"\"\"Check if index is compatible with given model version.\"\"\"\n```\n\n## Implementation Notes\n\n### IVFFlat Tuning\n```python\ndef _calculate_nlist(self, n_vectors: int) -\u003e int:\n    \"\"\"\n    Calculate optimal nlist for IVFFlat.\n    \n    Rule of thumb: sqrt(n) for balanced performance.\n    But clamp to reasonable range.\n    \"\"\"\n    nlist = int(np.sqrt(n_vectors))\n    nlist = max(nlist, 16)    # Minimum for small datasets\n    nlist = min(nlist, 4096)  # Maximum practical value\n    return nlist\n\ndef _calculate_nprobe(self, nlist: int) -\u003e int:\n    \"\"\"\n    Calculate default nprobe for searches.\n    \n    Higher nprobe = better recall, slower search.\n    Rule of thumb: nlist / 10 to nlist / 4\n    \"\"\"\n    return max(1, nlist // 10)\n```\n\n### Filtered Search Strategy\nFor filter-then-search (when SQL filters reduce candidates):\n\n```python\ndef search_jobs_filtered(self, query_vector: np.ndarray,\n                        allowed_uuids: set[str], k: int = 10) -\u003e list[tuple[str, float]]:\n    \"\"\"\n    Search only among allowed UUIDs.\n    \n    Strategy depends on filter selectivity:\n    - If \u003c 10K allowed: Build temporary Flat index\n    - If \u003e= 10K allowed: Search full index, filter results\n    \"\"\"\n    if len(allowed_uuids) \u003c 10000:\n        return self._search_with_temp_index(query_vector, allowed_uuids, k)\n    else:\n        return self._search_and_filter(query_vector, allowed_uuids, k)\n\ndef _search_with_temp_index(self, query_vector, allowed_uuids, k):\n    \"\"\"Build temporary Flat index for small candidate sets.\"\"\"\n    # Get embeddings for allowed UUIDs\n    embeddings = []\n    uuids = []\n    for uuid in allowed_uuids:\n        if uuid in self.uuid_to_idx:\n            idx = self.uuid_to_idx[uuid]\n            # Reconstruct embedding from index\n            embedding = self.indexes['jobs'].reconstruct(idx)\n            embeddings.append(embedding)\n            uuids.append(uuid)\n    \n    if not embeddings:\n        return []\n    \n    # Build temporary Flat index\n    embeddings = np.array(embeddings)\n    temp_index = faiss.IndexFlatL2(embeddings.shape[1])\n    temp_index.add(embeddings)\n    \n    # Search\n    distances, indices = temp_index.search(query_vector.reshape(1, -1), k)\n    \n    results = []\n    for dist, idx in zip(distances[0], indices[0]):\n        if idx \u003e= 0:\n            similarity = 1 - (dist / 2)\n            results.append((uuids[idx], float(similarity)))\n    \n    return results\n```\n\n### Company Multi-Centroid Search\n```python\ndef search_companies(self, query_vector: np.ndarray, k: int = 10) -\u003e list[tuple[str, float]]:\n    \"\"\"\n    Search for similar companies.\n    \n    For each company, we have multiple centroids (job families).\n    Similarity = max(query · centroid) across all centroids.\n    \"\"\"\n    # Get all centroid similarities\n    all_centroids = self._all_company_centroids  # Shape: (M, 384)\n    similarities = np.dot(all_centroids, query_vector)  # Shape: (M,)\n    \n    # Group by company, take max\n    company_max_scores = {}\n    for i, (company, centroid_indices) in enumerate(self._company_centroid_map.items()):\n        max_sim = max(similarities[idx] for idx in centroid_indices)\n        company_max_scores[company] = float(max_sim)\n    \n    # Sort and return top k\n    sorted_companies = sorted(company_max_scores.items(), \n                             key=lambda x: x[1], reverse=True)\n    return sorted_companies[:k]\n```\n\n## File Structure After Index Save\n\n```\ndata/embeddings/\n├── jobs.index              # FAISS IVFFlat index\n├── jobs_uuids.npy          # UUID list in index order\n├── skills.index            # FAISS Flat index\n├── skills_names.pkl        # Skill name list\n├── companies.index         # FAISS Flat index (all centroids flattened)\n├── companies_centroids.pkl # Dict[company_name -\u003e list of centroid indices]\n├── companies_names.pkl     # Company name list\n└── model_version.txt       # \"all-MiniLM-L6-v2\"\n```\n\n## Error Handling\n\n```python\nclass IndexNotBuiltError(Exception):\n    \"\"\"Raised when searching an unbuilt index.\"\"\"\n\nclass IndexCompatibilityError(Exception):\n    \"\"\"Raised when model version doesn't match index.\"\"\"\n\ndef search_jobs(self, ...):\n    if 'jobs' not in self.indexes or self.indexes['jobs'] is None:\n        raise IndexNotBuiltError(\"Job index not built. Run embed-generate first.\")\n    ...\n```\n\n## Testing\n\n```python\ndef test_faiss_index_manager():\n    manager = FAISSIndexManager(index_dir=Path(\"/tmp/test_index\"))\n    \n    # Create test data\n    np.random.seed(42)\n    embeddings = np.random.randn(1000, 384).astype(np.float32)\n    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n    uuids = [f\"uuid-{i}\" for i in range(1000)]\n    \n    # Build and search\n    manager.build_job_index(embeddings, uuids)\n    results = manager.search_jobs(embeddings[0], k=5)\n    \n    # First result should be the query itself\n    assert results[0][0] == \"uuid-0\"\n    assert results[0][1] \u003e 0.99  # Nearly identical\n    \n    # Test save/load\n    manager.save()\n    \n    manager2 = FAISSIndexManager(index_dir=Path(\"/tmp/test_index\"))\n    manager2.load()\n    results2 = manager2.search_jobs(embeddings[0], k=5)\n    \n    assert results == results2\n```\n\n## Dependencies\n- Requires Phase 1 completion (embeddings in database)","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T18:08:27.939635+08:00","created_by":"David Ten","updated_at":"2026-02-06T01:00:29.377658+08:00","closed_at":"2026-02-06T01:00:29.377658+08:00","close_reason":"Implemented FAISSIndexManager with IVFFlat for jobs, Flat for skills/companies, filtered search, persistence, and comprehensive test coverage (29 tests)","dependencies":[{"issue_id":"MyCareersFuture-qwl.2.1","depends_on_id":"MyCareersFuture-qwl.2","type":"parent-child","created_at":"2026-02-05T18:08:27.940989+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-qwl.2.2","title":"Integrate index building with embed-generate command","description":"# Task: Integrate Index Building with embed-generate Command\n\n## What\nUpdate the embed-generate CLI command to automatically build FAISS indexes after generating embeddings.\n\n## Current Flow (After Phase 1)\n```\nembed-generate → Generate embeddings → Store in SQLite\n```\n\n## New Flow\n```\nembed-generate → Generate embeddings → Store in SQLite → Build FAISS indexes → Save to disk\n```\n\n## Implementation\n\n### Update embed-generate command\n\n```python\n@app.command(name=\"embed-generate\")\ndef generate_embeddings(\n    batch_size: int = typer.Option(32, \"--batch-size\", \"-b\"),\n    skip_existing: bool = typer.Option(True, \"--skip-existing/--no-skip-existing\"),\n    build_index: bool = typer.Option(True, \"--build-index/--no-build-index\",\n        help=\"Build FAISS indexes after embedding generation\"),\n    index_dir: str = typer.Option(\"data/embeddings\", \"--index-dir\",\n        help=\"Directory for FAISS index files\"),\n    db_path: str = typer.Option(\"data/mcf_jobs.db\", \"--db\"),\n    verbose: bool = typer.Option(False, \"--verbose\", \"-v\"),\n) -\u003e None:\n    \"\"\"\n    Generate embeddings for all jobs and build search indexes.\n    ...\n    \"\"\"\n    async def run():\n        generator = EmbeddingGenerator()\n        db = MCFDatabase(db_path)\n        \n        # Phase 1: Generate embeddings\n        console.print(\"\\n[bold blue]Phase 1: Generating Embeddings[/bold blue]\\n\")\n        with _create_progress() as progress:\n            stats = await generator.generate_all(db, batch_size, skip_existing, progress)\n        \n        _print_embedding_summary(stats)\n        \n        if not build_index:\n            console.print(\"\\n[yellow]Skipping index building (--no-build-index)[/yellow]\")\n            return\n        \n        # Phase 2: Build FAISS indexes\n        console.print(\"\\n[bold blue]Phase 2: Building FAISS Indexes[/bold blue]\\n\")\n        \n        index_manager = FAISSIndexManager(Path(index_dir))\n        \n        with _create_progress() as progress:\n            task = progress.add_task(\"Loading embeddings from database\", total=None)\n            \n            # Load all embeddings\n            job_uuids, job_embeddings = db.get_all_embeddings(\"job\")\n            progress.update(task, description=f\"Building job index ({len(job_uuids):,} vectors)\")\n            \n            # Build job index\n            index_manager.build_job_index(job_embeddings, job_uuids)\n            progress.update(task, description=\"Building skill index\")\n            \n            # Build skill index\n            skill_names, skill_embeddings = db.get_all_embeddings(\"skill\")\n            if len(skill_names) \u003e 0:\n                index_manager.build_skill_index(skill_embeddings, skill_names)\n            progress.update(task, description=\"Building company index\")\n            \n            # Build company index\n            company_centroids = _load_company_centroids(db)\n            if company_centroids:\n                index_manager.build_company_index(company_centroids)\n            progress.update(task, description=\"Saving indexes to disk\")\n            \n            # Save\n            index_manager.save()\n            progress.update(task, description=\"Complete\", completed=True)\n        \n        # Print index summary\n        _print_index_summary(index_manager)\n    \n    asyncio.run(run())\n```\n\n### Update embed-sync for incremental index updates\n\n```python\n@app.command(name=\"embed-sync\")\ndef sync_embeddings(\n    batch_size: int = typer.Option(32, \"--batch-size\", \"-b\"),\n    update_index: bool = typer.Option(True, \"--update-index/--no-update-index\",\n        help=\"Update FAISS indexes with new embeddings\"),\n    index_dir: str = typer.Option(\"data/embeddings\", \"--index-dir\"),\n    db_path: str = typer.Option(\"data/mcf_jobs.db\", \"--db\"),\n    verbose: bool = typer.Option(False, \"--verbose\", \"-v\"),\n) -\u003e None:\n    \"\"\"\n    Generate embeddings for new jobs and update indexes.\n    ...\n    \"\"\"\n    async def run():\n        generator = EmbeddingGenerator()\n        db = MCFDatabase(db_path)\n        \n        # Find jobs without embeddings\n        new_uuids = db.get_jobs_without_embeddings()\n        \n        if not new_uuids:\n            console.print(\"[green]All jobs already have embeddings![/green]\")\n            return\n        \n        console.print(f\"Found {len(new_uuids):,} new jobs to embed\\n\")\n        \n        # Generate embeddings for new jobs\n        new_embeddings = await generator.generate_for_uuids(db, new_uuids, batch_size)\n        \n        if update_index:\n            # Load existing index\n            index_manager = FAISSIndexManager(Path(index_dir))\n            \n            if not index_manager.load():\n                console.print(\"[yellow]No existing index found. Run embed-generate first.[/yellow]\")\n                return\n            \n            # Add new embeddings to index\n            console.print(\"Updating FAISS index...\")\n            index_manager.add_jobs(new_embeddings, new_uuids)\n            index_manager.save()\n            \n            console.print(f\"[green]Added {len(new_uuids):,} jobs to index[/green]\")\n    \n    asyncio.run(run())\n```\n\n### Update embed-status to show index info\n\n```python\n@app.command(name=\"embed-status\")\ndef embedding_status(\n    index_dir: str = typer.Option(\"data/embeddings\", \"--index-dir\"),\n    db_path: str = typer.Option(\"data/mcf_jobs.db\", \"--db\"),\n) -\u003e None:\n    \"\"\"Show embedding and index status.\"\"\"\n    db = MCFDatabase(db_path)\n    index_manager = FAISSIndexManager(Path(index_dir))\n    \n    # Database stats\n    db_stats = db.get_embedding_stats()\n    \n    # Index stats\n    index_loaded = index_manager.load()\n    index_stats = index_manager.get_stats() if index_loaded else None\n    \n    # Display\n    console.print(\"\\n[bold blue]Embedding Status[/bold blue]\\n\")\n    \n    # ... existing embedding stats ...\n    \n    console.print(\"\\n[bold]FAISS Indexes[/bold]\")\n    if index_stats:\n        table = Table(show_header=True)\n        table.add_column(\"Index\")\n        table.add_column(\"Vectors\", justify=\"right\")\n        table.add_column(\"Size\", justify=\"right\")\n        table.add_column(\"Status\")\n        \n        table.add_row(\n            \"jobs.index\",\n            f\"{index_stats['jobs_count']:,}\",\n            _format_size(index_stats['jobs_size']),\n            \"[green]✓ ready[/green]\"\n        )\n        # ... skills and companies ...\n        \n        console.print(table)\n    else:\n        console.print(\"[yellow]No indexes found. Run embed-generate to build.[/yellow]\")\n```\n\n## Output Example\n\n```\nPhase 1: Generating Embeddings\n\nJobs:     ████████████████████████████████ 100% 150,234/150,234\nSkills:   ████████████████████████████████ 100% 3,842/3,842\nCompanies: ████████████████████████████████ 100% 12,456/12,456\n\nSummary:\n  Jobs processed:      150,234\n  Skills extracted:    3,842\n  Companies clustered: 12,456\n  Time:               12m 34s\n\nPhase 2: Building FAISS Indexes\n\nBuilding job index (150,234 vectors)...\nBuilding skill index...\nBuilding company index...\nSaving indexes to disk...\n\nIndex Summary:\n  jobs.index:      582 MB (150,234 vectors, IVFFlat nlist=388)\n  skills.index:    1.4 MB (3,842 vectors, Flat)\n  companies.index: 18 MB (28,234 centroids, Flat)\n\n✓ Embedding generation and indexing complete!\n```\n\n## Testing\n\n```bash\n# Full generation + indexing\nmcf embed-generate --db test.db\n\n# Verify index files created\nls -la data/embeddings/\n\n# Test incremental update\nmcf embed-sync --db test.db\n\n# Check status\nmcf embed-status --db test.db\n```\n\n## Dependencies\n- Requires FAISSIndexManager (MyCareersFuture-qwl.2.1)\n- Modifies CLI commands from Phase 1 (MyCareersFuture-qwl.1.4)","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T18:19:54.436207+08:00","created_by":"David Ten","updated_at":"2026-02-06T01:06:11.429675+08:00","closed_at":"2026-02-06T01:06:11.429675+08:00","close_reason":"Implemented FAISS index integration: embed-generate builds indexes, embed-sync adds incrementally, embed-status shows index stats","dependencies":[{"issue_id":"MyCareersFuture-qwl.2.2","depends_on_id":"MyCareersFuture-qwl.2","type":"parent-child","created_at":"2026-02-05T18:19:54.436996+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-qwl.2.2","depends_on_id":"MyCareersFuture-qwl.2.1","type":"blocks","created_at":"2026-02-05T18:25:27.939007+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-qwl.3","title":"Phase 3: Search Engine Core","description":"# Phase 3: Search Engine Core\n\n## Purpose\nBuild the SemanticSearchEngine class that orchestrates all search operations. This is the \"brain\" of the system - it combines SQL filtering, vector search, BM25 ranking, query expansion, and caching into a unified search experience.\n\n## Why This Phase Follows Phase 2\nThe search engine needs FAISS indexes to query. Phase 2 provides the index infrastructure; Phase 3 builds the search orchestration on top.\n\n## Key Innovation: Hybrid Search\n\nThe core innovation of this system is **hybrid search** that combines multiple ranking signals:\n\n```\nfinal_score = α × semantic_score + (1-α) × bm25_score + β × freshness_boost\n```\n\n### Why Hybrid?\n\n**Problem with pure semantic search:**\n- Query: \"Python developer\"\n- Semantic might rank \"Backend Engineer (Python)\" lower than \"Software Developer\"\n- Because \"Software Developer\" description might have more similar vocabulary\n\n**Problem with pure keyword search:**\n- Query: \"ML engineer\"\n- Keyword search misses \"Machine Learning Specialist\" and \"Deep Learning Researcher\"\n- No semantic understanding\n\n**Hybrid solves both:**\n- BM25 ensures exact matches rank highly\n- Semantic captures related meanings\n- Freshness ensures recent jobs surface\n\n## Components to Build\n\n### 3.1 QueryExpander\nExpands user queries with related terms:\n- \"ML\" → \"ML, Machine Learning, Deep Learning\"\n- \"DS\" → \"DS, Data Science, Data Scientist\"\n\nUses precomputed skill clusters from embedding similarity.\n\n### 3.2 SemanticSearchEngine\nMain search orchestration:\n- Combined SQL filter + vector search\n- Hybrid BM25 + semantic scoring\n- Caching for repeated queries\n- Graceful degradation to keyword-only\n\n### 3.3 Search Strategies\nDifferent strategies based on filter selectivity:\n- Broad filters (\u003e10K results): Search full index, filter results\n- Narrow filters (\u003c10K results): Build temp index from filtered set\n\n## Technical Deep-Dive\n\n### Hybrid Scoring Algorithm\n\n```python\ndef _compute_hybrid_scores(\n    self,\n    query: str,\n    query_vector: np.ndarray,\n    candidate_uuids: list[str],\n    alpha: float = 0.7,\n    freshness_weight: float = 0.1\n) -\u003e list[tuple[str, float]]:\n    \"\"\"\n    Compute hybrid scores combining semantic + BM25 + freshness.\n    \n    Args:\n        query: Original text query\n        query_vector: Query embedding\n        candidate_uuids: UUIDs that passed SQL filters\n        alpha: Weight for semantic vs BM25 (0.7 = 70% semantic)\n        freshness_weight: How much to boost recent jobs\n        \n    Returns:\n        List of (uuid, score) tuples, sorted by score descending\n    \"\"\"\n    # Get semantic scores from FAISS\n    semantic_results = self.index_manager.search_jobs_filtered(\n        query_vector, set(candidate_uuids), k=len(candidate_uuids)\n    )\n    semantic_scores = {uuid: score for uuid, score in semantic_results}\n    \n    # Get BM25 scores from FTS5\n    bm25_results = self.db.bm25_search(query, limit=len(candidate_uuids))\n    bm25_scores = {uuid: score for uuid, score in bm25_results}\n    \n    # Normalize BM25 scores to [0, 1]\n    if bm25_scores:\n        max_bm25 = max(bm25_scores.values())\n        if max_bm25 \u003e 0:\n            bm25_scores = {k: v/max_bm25 for k, v in bm25_scores.items()}\n    \n    # Get freshness scores\n    freshness_scores = self._compute_freshness_scores(candidate_uuids)\n    \n    # Combine scores\n    final_scores = []\n    for uuid in candidate_uuids:\n        semantic = semantic_scores.get(uuid, 0.0)\n        bm25 = bm25_scores.get(uuid, 0.0)\n        freshness = freshness_scores.get(uuid, 0.5)  # Default for missing dates\n        \n        # Hybrid formula\n        score = (\n            alpha * semantic +\n            (1 - alpha) * bm25 +\n            freshness_weight * freshness\n        )\n        final_scores.append((uuid, score))\n    \n    # Sort by score\n    final_scores.sort(key=lambda x: x[1], reverse=True)\n    return final_scores\n\ndef _compute_freshness_scores(self, uuids: list[str]) -\u003e dict[str, float]:\n    \"\"\"\n    Compute freshness boost for jobs.\n    \n    Formula: exp(-days_old / 30)\n    - Today: 1.0\n    - 30 days old: 0.37\n    - 60 days old: 0.14\n    - 90 days old: 0.05\n    \"\"\"\n    jobs = self.db.get_jobs_by_uuids(uuids)\n    scores = {}\n    today = datetime.now().date()\n    \n    for job in jobs:\n        if job.get('posted_date'):\n            posted = datetime.fromisoformat(job['posted_date']).date()\n            days_old = (today - posted).days\n            scores[job['uuid']] = np.exp(-days_old / 30)\n        else:\n            scores[job['uuid']] = 0.5  # Unknown date\n    \n    return scores\n```\n\n### Caching Strategy\n\n```python\nfrom cachetools import TTLCache\n\nclass SemanticSearchEngine:\n    def __init__(self, ...):\n        # Cache query embeddings (most expensive operation)\n        self._query_cache = TTLCache(maxsize=1000, ttl=3600)  # 1 hour TTL\n        \n        # Cache full search results for popular queries\n        self._result_cache = TTLCache(maxsize=200, ttl=300)  # 5 min TTL\n    \n    def _get_query_embedding(self, query: str) -\u003e np.ndarray:\n        \"\"\"Get query embedding with caching.\"\"\"\n        cache_key = query.lower().strip()\n        \n        if cache_key in self._query_cache:\n            return self._query_cache[cache_key]\n        \n        embedding = self.generator.encode(query)\n        self._query_cache[cache_key] = embedding\n        return embedding\n    \n    def _get_cached_results(self, cache_key: str) -\u003e Optional[SearchResponse]:\n        \"\"\"Check if we have cached results.\"\"\"\n        return self._result_cache.get(cache_key)\n    \n    def _cache_results(self, cache_key: str, response: SearchResponse) -\u003e None:\n        \"\"\"Cache search results.\"\"\"\n        self._result_cache[cache_key] = response\n```\n\n### Graceful Degradation\n\n```python\nasync def search(self, request: SearchRequest) -\u003e SearchResponse:\n    \"\"\"\n    Main search method with graceful degradation.\n    \"\"\"\n    start_time = time.time()\n    degraded = False\n    \n    try:\n        # Try hybrid search\n        results = await self._hybrid_search(request)\n    except (IndexNotBuiltError, IndexCompatibilityError) as e:\n        # Fall back to BM25 only\n        logger.warning(f\"Vector search unavailable: {e}. Using keyword search.\")\n        results = self._keyword_only_search(request)\n        degraded = True\n    \n    # Log analytics\n    latency_ms = (time.time() - start_time) * 1000\n    self.db.log_search(\n        query=request.query,\n        query_type='semantic',\n        result_count=len(results),\n        latency_ms=latency_ms,\n        cache_hit=False,  # Set correctly based on actual cache status\n        degraded=degraded,\n        filters_used=request.dict(exclude={'query', 'limit'})\n    )\n    \n    return SearchResponse(\n        results=results,\n        degraded=degraded,\n        search_time_ms=latency_ms\n    )\n\ndef _keyword_only_search(self, request: SearchRequest) -\u003e list[JobResult]:\n    \"\"\"Fallback to pure BM25 search when vector search unavailable.\"\"\"\n    # Apply SQL filters\n    candidates = self.db.search_jobs(\n        salary_min=request.salary_min,\n        salary_max=request.salary_max,\n        employment_type=request.employment_type,\n        limit=10000\n    )\n    candidate_uuids = [j['uuid'] for j in candidates]\n    \n    # BM25 ranking\n    bm25_results = self.db.bm25_search(request.query, limit=request.limit)\n    \n    # Filter to candidates only\n    filtered_results = [\n        (uuid, score) for uuid, score in bm25_results\n        if uuid in set(candidate_uuids)\n    ]\n    \n    return self._enrich_results(filtered_results[:request.limit])\n```\n\n## Search API Contract\n\n```python\nclass SearchRequest(BaseModel):\n    query: str\n    limit: int = 10\n    \n    # SQL filters\n    salary_min: Optional[int] = None\n    salary_max: Optional[int] = None\n    employment_type: Optional[str] = None\n    region: Optional[str] = None\n    company: Optional[str] = None\n    \n    # Hybrid tuning\n    alpha: float = 0.7           # Semantic vs BM25 weight\n    freshness_weight: float = 0.1\n    expand_query: bool = True    # Enable query expansion\n    \n    # Advanced\n    min_similarity: float = 0.3  # Minimum score threshold\n\nclass SearchResponse(BaseModel):\n    results: list[JobResult]\n    total_candidates: int        # After SQL filter, before ranking\n    search_time_ms: float\n    query_expansion: Optional[list[str]]  # Expanded terms if enabled\n    degraded: bool = False       # True if fell back to keyword search\n    cache_hit: bool = False\n\nclass JobResult(BaseModel):\n    uuid: str\n    title: str\n    company_name: str\n    description: str\n    salary_min: Optional[int]\n    salary_max: Optional[int]\n    employment_type: str\n    skills: str\n    location: str\n    posted_date: Optional[str]\n    job_url: str\n    similarity_score: float\n    bm25_score: Optional[float]\n    freshness_score: Optional[float]\n```\n\n## Files to Create\n- `src/mcf/embeddings/query_expander.py`: QueryExpander class\n- `src/mcf/embeddings/search_engine.py`: SemanticSearchEngine class\n\n## Exit Criteria\n- [ ] Hybrid search returns relevant results for test queries\n- [ ] BM25 fallback works when index unavailable\n- [ ] Caching reduces latency for repeated queries\n- [ ] Query expansion improves recall for abbreviations\n- [ ] Search latency \u003c 100ms for typical queries","status":"closed","priority":1,"issue_type":"epic","owner":"davidten7@gmail.com","created_at":"2026-02-05T18:32:09.798792+08:00","created_by":"David Ten","updated_at":"2026-02-06T07:40:00.295965+08:00","closed_at":"2026-02-06T07:40:00.295965+08:00","close_reason":"All 3 tasks complete: QueryExpander, SemanticSearchEngine, and search-semantic CLI command","dependencies":[{"issue_id":"MyCareersFuture-qwl.3","depends_on_id":"MyCareersFuture-qwl","type":"parent-child","created_at":"2026-02-05T18:32:09.800716+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-qwl.3","depends_on_id":"MyCareersFuture-qwl.2","type":"blocks","created_at":"2026-02-05T18:34:40.117773+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-qwl.3.1","title":"Implement QueryExpander for synonym expansion","description":"# Task: Implement QueryExpander for Synonym Expansion\n\n## What\nCreate src/mcf/embeddings/query_expander.py with the QueryExpander class that expands user queries with related terms.\n\n**Data Source:** Skill clusters are generated during `embed-generate` (Phase 1.3) and saved to `data/embeddings/skill_clusters.pkl`. This task just loads and uses that data.\n\n## Problem Being Solved\nUsers search with different vocabulary than job postings:\n- User: \"ML engineer\" → Job: \"Machine Learning Specialist\"\n- User: \"DS\" → Job: \"Data Scientist\"  \n- User: \"devops\" → Job: \"Site Reliability Engineer\"\n\nWithout expansion, these don't match well in keyword search.\n\n## Class Design\n\n```python\nclass QueryExpander:\n    \"\"\"\n    Expands search queries with related terms.\n    \n    Uses precomputed skill clusters (from embed-generate) to find synonyms.\n    \n    Example:\n        expander = QueryExpander.load(Path(\"data/embeddings\"))\n        expanded = expander.expand(\"ML engineer\")\n        # Returns: [\"ML\", \"Machine Learning\", \"Deep Learning\", \"engineer\"]\n    \"\"\"\n    \n    def __init__(self, skill_clusters: dict[int, list[str]],\n                 skill_to_cluster: dict[str, int],\n                 generator: EmbeddingGenerator = None):\n        \"\"\"\n        Args:\n            skill_clusters: Mapping of cluster_id -\u003e list of skill names\n            skill_to_cluster: Mapping of skill_name -\u003e cluster_id\n            generator: Optional, for embedding new query terms\n        \"\"\"\n        self.skill_clusters = skill_clusters\n        self.skill_to_cluster = skill_to_cluster\n        self.generator = generator\n        \n        # Build reverse lookup for fast matching\n        self._skill_lower_map = {s.lower(): s for s in skill_to_cluster.keys()}\n        \n    @classmethod\n    def load(cls, index_dir: Path) -\u003e \"QueryExpander\":\n        \"\"\"\n        Load precomputed clusters from disk.\n        \n        Files created by embed-generate (Phase 1.3):\n        - skill_clusters.pkl\n        - skill_to_cluster.pkl\n        \"\"\"\n        import pickle\n        \n        clusters_path = index_dir / \"skill_clusters.pkl\"\n        mapping_path = index_dir / \"skill_to_cluster.pkl\"\n        \n        if not clusters_path.exists():\n            raise FileNotFoundError(\n                f\"Skill clusters not found at {clusters_path}. \"\n                f\"Run 'mcf embed-generate' first.\"\n            )\n        \n        with open(clusters_path, \"rb\") as f:\n            skill_clusters = pickle.load(f)\n        \n        with open(mapping_path, \"rb\") as f:\n            skill_to_cluster = pickle.load(f)\n        \n        return cls(skill_clusters, skill_to_cluster)\n        \n    def expand(self, query: str, max_expansions: int = 3) -\u003e list[str]:\n        \"\"\"\n        Expand query with related terms.\n        \n        Args:\n            query: Original search query\n            max_expansions: Max additional terms per query word\n            \n        Returns:\n            List of expanded terms (original terms + expansions)\n        \"\"\"\n        words = query.lower().split()\n        expanded = list(words)  # Start with original words\n        \n        for word in words:\n            # Find matching skill\n            matching_skill = self._find_matching_skill(word)\n            \n            if matching_skill:\n                # Get related skills from same cluster\n                related = self.get_related_skills(matching_skill, k=max_expansions)\n                expanded.extend(related)\n        \n        # Deduplicate while preserving order\n        seen = set()\n        result = []\n        for term in expanded:\n            if term.lower() not in seen:\n                seen.add(term.lower())\n                result.append(term)\n        \n        return result\n        \n    def get_related_skills(self, skill: str, k: int = 5) -\u003e list[str]:\n        \"\"\"\n        Get skills related to a given skill.\n        \n        Returns other skills from the same cluster.\n        \"\"\"\n        cluster_id = self.skill_to_cluster.get(skill)\n        if cluster_id is None:\n            return []\n        \n        cluster_skills = self.skill_clusters.get(cluster_id, [])\n        \n        # Return other skills from same cluster (not the input skill)\n        related = [s for s in cluster_skills if s != skill]\n        return related[:k]\n        \n    def _find_matching_skill(self, word: str) -\u003e Optional[str]:\n        \"\"\"\n        Find skill that matches the word.\n        \n        Matching strategies:\n        1. Exact match (case-insensitive)\n        2. Prefix match (\"python\" matches \"Python\")\n        3. Acronym match (\"ML\" matches \"Machine Learning\")\n        \"\"\"\n        word_lower = word.lower()\n        \n        # Exact match\n        if word_lower in self._skill_lower_map:\n            return self._skill_lower_map[word_lower]\n        \n        # Prefix match\n        for skill_lower, skill in self._skill_lower_map.items():\n            if skill_lower.startswith(word_lower) and len(word_lower) \u003e= 3:\n                return skill\n        \n        # Acronym match (for 2-3 letter uppercase queries)\n        if len(word) \u003c= 3 and word.isupper():\n            for skill in self.skill_to_cluster.keys():\n                skill_acronym = ''.join(w[0] for w in skill.split() if w).upper()\n                if skill_acronym == word:\n                    return skill\n        \n        return None\n```\n\n## Expected Behavior\n\n```python\n\u003e\u003e\u003e expander = QueryExpander.load(Path(\"data/embeddings\"))\n\n\u003e\u003e\u003e expander.expand(\"ML engineer\")\n[\"ML\", \"Machine Learning\", \"Deep Learning\", \"Neural Networks\", \"engineer\"]\n\n\u003e\u003e\u003e expander.expand(\"python developer\")\n[\"python\", \"Pandas\", \"NumPy\", \"developer\"]\n\n\u003e\u003e\u003e expander.expand(\"devops aws\")\n[\"devops\", \"Kubernetes\", \"Docker\", \"CI/CD\", \"aws\", \"Azure\", \"GCP\"]\n\n\u003e\u003e\u003e expander.expand(\"accounting\")  # No match in skill clusters\n[\"accounting\"]\n```\n\n## File Storage\n\nInput files (created by Phase 1.3):\n```\ndata/embeddings/\n├── skill_clusters.pkl      # Dict[int, list[str]] - cluster_id -\u003e skills\n├── skill_to_cluster.pkl    # Dict[str, int] - skill -\u003e cluster_id\n└── skill_cluster_centroids.pkl  # Optional, for advanced similarity\n```\n\n## Integration with Search\n\n```python\n# In SemanticSearchEngine.search():\n\nif request.expand_query and self.query_expander:\n    expanded_terms = self.query_expander.expand(request.query)\n    \n    # Use expanded terms for BM25 search\n    expanded_query = \" OR \".join(expanded_terms)\n    bm25_results = self.db.bm25_search(expanded_query)\n    \n    # Include expansion in response for transparency\n    response.query_expansion = expanded_terms\n```\n\n## Testing\n\n```python\ndef test_query_expander():\n    expander = QueryExpander.load(Path(\"data/embeddings\"))\n    \n    # Test acronym expansion\n    expanded = expander.expand(\"ML engineer\")\n    assert \"Machine Learning\" in expanded or \"ML\" in expanded\n    \n    # Test skill expansion\n    expanded = expander.expand(\"Python developer\")\n    assert len(expanded) \u003e 2  # Should have some expansions\n    \n    # Test unknown term (no expansion)\n    expanded = expander.expand(\"xyznonexistent\")\n    assert expanded == [\"xyznonexistent\"]\n\ndef test_related_skills():\n    expander = QueryExpander.load(Path(\"data/embeddings\"))\n    \n    related = expander.get_related_skills(\"Python\", k=5)\n    assert len(related) \u003c= 5\n    assert \"Python\" not in related  # Should not include input skill\n```\n\n## Dependencies\n- **Requires:** Skill cluster data from embed-generate (Phase 1.3)\n- **No dependency on Phase 5** - Phase 5 adds advanced features, not basic clustering","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T18:32:53.231705+08:00","created_by":"David Ten","updated_at":"2026-02-06T01:12:34.131128+08:00","closed_at":"2026-02-06T01:12:34.131128+08:00","close_reason":"Implemented QueryExpander with 4 matching strategies (exact, acronym, prefix, word-boundary). 26 tests passing.","dependencies":[{"issue_id":"MyCareersFuture-qwl.3.1","depends_on_id":"MyCareersFuture-qwl.3","type":"parent-child","created_at":"2026-02-05T18:32:53.232963+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-qwl.3.1","depends_on_id":"MyCareersFuture-qwl.1.4","type":"blocks","created_at":"2026-02-05T20:33:11.569594+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-qwl.3.2","title":"Implement SemanticSearchEngine class","description":"# Task: Implement SemanticSearchEngine Class\n\n## What\nCreate src/mcf/embeddings/search_engine.py with the main SemanticSearchEngine class that orchestrates all search operations.\n\n## Class Design\n\n```python\nclass SemanticSearchEngine:\n    \"\"\"\n    Orchestrates hybrid semantic + keyword search.\n    \n    Combines:\n    - SQL filtering (salary, location, type)\n    - FAISS vector search (semantic similarity)\n    - FTS5 BM25 search (keyword relevance)\n    - Query expansion (synonym matching)\n    - Result caching (performance)\n    - Graceful degradation (reliability)\n    \n    Example:\n        engine = SemanticSearchEngine(db_path, index_dir)\n        await engine.load()\n        \n        response = await engine.search(SearchRequest(\n            query=\"machine learning engineer\",\n            salary_min=10000,\n            limit=20\n        ))\n    \"\"\"\n    \n    def __init__(self, db_path: str = \"data/mcf_jobs.db\",\n                 index_dir: Path = Path(\"data/embeddings\")):\n        self.db = MCFDatabase(db_path)\n        self.index_manager = FAISSIndexManager(index_dir)\n        self.generator = EmbeddingGenerator()\n        self.query_expander = None  # Loaded lazily\n        \n        # Caches\n        self._query_cache = TTLCache(maxsize=1000, ttl=3600)\n        self._result_cache = TTLCache(maxsize=200, ttl=300)\n        \n        # State\n        self._loaded = False\n        self._degraded = False\n        \n    async def load(self) -\u003e None:\n        \"\"\"Load indexes and prepare for searching.\"\"\"\n        \n    async def search(self, request: SearchRequest) -\u003e SearchResponse:\n        \"\"\"Main semantic search with all features.\"\"\"\n        \n    async def find_similar(self, request: SimilarJobsRequest) -\u003e SearchResponse:\n        \"\"\"Find jobs similar to a given job.\"\"\"\n        \n    async def search_by_skill(self, request: SkillSearchRequest) -\u003e SearchResponse:\n        \"\"\"Search jobs by skill similarity.\"\"\"\n        \n    async def find_similar_companies(self, request: CompanySimilarityRequest) -\u003e list[CompanySimilarity]:\n        \"\"\"Find companies with similar job profiles.\"\"\"\n        \n    def get_stats(self) -\u003e dict:\n        \"\"\"Get search engine statistics.\"\"\"\n```\n\n## Main Search Implementation\n\n```python\nasync def search(self, request: SearchRequest) -\u003e SearchResponse:\n    \"\"\"\n    Main search method implementing hybrid search.\n    \n    Flow:\n    1. Check cache for identical request\n    2. Apply SQL filters to get candidates\n    3. Expand query if enabled\n    4. Get query embedding (cached)\n    5. Compute hybrid scores\n    6. Return top k results\n    \"\"\"\n    start_time = time.time()\n    cache_hit = False\n    \n    # Check result cache\n    cache_key = self._make_cache_key(request)\n    cached = self._result_cache.get(cache_key)\n    if cached:\n        cached.cache_hit = True\n        return cached\n    \n    try:\n        # SQL filtering\n        candidates = self._apply_sql_filters(request)\n        total_candidates = len(candidates)\n        \n        if not candidates:\n            return SearchResponse(\n                results=[],\n                total_candidates=0,\n                search_time_ms=(time.time() - start_time) * 1000,\n                degraded=self._degraded\n            )\n        \n        # Query expansion\n        query_expansion = None\n        search_query = request.query\n        if request.expand_query and self.query_expander:\n            expanded = self.query_expander.expand(request.query)\n            if len(expanded) \u003e 1:\n                query_expansion = expanded\n                # Use expanded for BM25\n                search_query = \" \".join(expanded)\n        \n        # Get query embedding\n        query_vector = self._get_query_embedding(request.query)\n        \n        # Hybrid scoring\n        scored_results = self._compute_hybrid_scores(\n            search_query, query_vector,\n            [c['uuid'] for c in candidates],\n            alpha=request.alpha,\n            freshness_weight=request.freshness_weight\n        )\n        \n        # Apply minimum similarity threshold\n        filtered_results = [\n            (uuid, score) for uuid, score in scored_results\n            if score \u003e= request.min_similarity\n        ][:request.limit]\n        \n        # Enrich with full job data\n        results = self._enrich_results(filtered_results)\n        \n        response = SearchResponse(\n            results=results,\n            total_candidates=total_candidates,\n            search_time_ms=(time.time() - start_time) * 1000,\n            query_expansion=query_expansion,\n            degraded=self._degraded,\n            cache_hit=False\n        )\n        \n        # Cache result\n        self._result_cache[cache_key] = response\n        \n        # Log analytics\n        self._log_search(request, response)\n        \n        return response\n        \n    except (IndexNotBuiltError, IndexCompatibilityError) as e:\n        logger.warning(f\"Vector search failed: {e}. Falling back to keyword search.\")\n        self._degraded = True\n        return self._keyword_only_search(request, start_time)\n```\n\n## Similar Jobs Implementation\n\n```python\nasync def find_similar(self, request: SimilarJobsRequest) -\u003e SearchResponse:\n    \"\"\"\n    Find jobs similar to a given job.\n    \n    Uses the job's own embedding as the query vector.\n    \"\"\"\n    start_time = time.time()\n    \n    # Get source job embedding\n    source_embedding = self.db.get_embedding(request.job_uuid, \"job\")\n    if source_embedding is None:\n        raise ValueError(f\"No embedding found for job {request.job_uuid}\")\n    \n    # Get source job for company exclusion\n    source_job = self.db.get_job(request.job_uuid)\n    \n    # Search for similar (request extra to allow for filtering)\n    search_k = request.limit + 10 if request.exclude_same_company else request.limit + 1\n    \n    results = self.index_manager.search_jobs(source_embedding, k=search_k)\n    \n    # Filter results\n    filtered = []\n    for uuid, score in results:\n        # Skip the source job itself\n        if uuid == request.job_uuid:\n            continue\n        \n        # Skip same company if requested\n        if request.exclude_same_company:\n            job = self.db.get_job(uuid)\n            if job and job['company_name'] == source_job['company_name']:\n                continue\n        \n        filtered.append((uuid, score))\n        \n        if len(filtered) \u003e= request.limit:\n            break\n    \n    # Apply freshness boost\n    if request.freshness_weight \u003e 0:\n        freshness = self._compute_freshness_scores([uuid for uuid, _ in filtered])\n        filtered = [\n            (uuid, score + request.freshness_weight * freshness.get(uuid, 0.5))\n            for uuid, score in filtered\n        ]\n        filtered.sort(key=lambda x: x[1], reverse=True)\n    \n    results = self._enrich_results(filtered)\n    \n    return SearchResponse(\n        results=results,\n        total_candidates=len(results),\n        search_time_ms=(time.time() - start_time) * 1000,\n        degraded=self._degraded\n    )\n```\n\n## Company Similarity Implementation\n\n```python\nasync def find_similar_companies(self, \n                                request: CompanySimilarityRequest) -\u003e list[CompanySimilarity]:\n    \"\"\"\n    Find companies with similar job profiles.\n    \n    Uses multi-centroid company embeddings.\n    \"\"\"\n    # Get company centroids\n    source_centroids = self._get_company_centroids(request.company_name)\n    if not source_centroids:\n        raise ValueError(f\"No embeddings found for company: {request.company_name}\")\n    \n    # Search for each centroid and aggregate\n    company_scores = {}\n    for centroid in source_centroids:\n        results = self.index_manager.search_companies(centroid, k=request.limit * 3)\n        for company, score in results:\n            if company == request.company_name:\n                continue\n            if company not in company_scores:\n                company_scores[company] = score\n            else:\n                # Take max score across centroids\n                company_scores[company] = max(company_scores[company], score)\n    \n    # Sort and limit\n    sorted_companies = sorted(\n        company_scores.items(), key=lambda x: x[1], reverse=True\n    )[:request.limit]\n    \n    # Enrich with company stats\n    results = []\n    for company_name, score in sorted_companies:\n        stats = self.db.get_company_stats(company_name)\n        results.append(CompanySimilarity(\n            company_name=company_name,\n            similarity_score=score,\n            job_count=stats.get('job_count', 0),\n            avg_salary=stats.get('avg_salary'),\n            top_skills=stats.get('top_skills', [])[:5]\n        ))\n    \n    return results\n```\n\n## Helper Methods\n\n```python\ndef _apply_sql_filters(self, request: SearchRequest) -\u003e list[dict]:\n    \"\"\"Apply SQL filters and return matching jobs.\"\"\"\n    return self.db.search_jobs(\n        salary_min=request.salary_min,\n        salary_max=request.salary_max,\n        employment_type=request.employment_type,\n        company_name=request.company,\n        limit=100000  # Get all matching for ranking\n    )\n\ndef _enrich_results(self, scored_results: list[tuple[str, float]]) -\u003e list[JobResult]:\n    \"\"\"Convert (uuid, score) tuples to full JobResult objects.\"\"\"\n    results = []\n    for uuid, score in scored_results:\n        job = self.db.get_job(uuid)\n        if job:\n            results.append(JobResult(\n                uuid=job['uuid'],\n                title=job['title'],\n                company_name=job['company_name'],\n                description=job['description'][:500] + \"...\" if len(job['description']) \u003e 500 else job['description'],\n                salary_min=job['salary_min'],\n                salary_max=job['salary_max'],\n                employment_type=job['employment_type'],\n                skills=job['skills'],\n                location=job['location'],\n                posted_date=job['posted_date'],\n                job_url=job['job_url'],\n                similarity_score=score\n            ))\n    return results\n\ndef _log_search(self, request: SearchRequest, response: SearchResponse) -\u003e None:\n    \"\"\"Log search to analytics table.\"\"\"\n    self.db.log_search(\n        query=request.query,\n        query_type='semantic',\n        result_count=len(response.results),\n        latency_ms=response.search_time_ms,\n        cache_hit=response.cache_hit,\n        degraded=response.degraded,\n        filters_used={\n            'salary_min': request.salary_min,\n            'salary_max': request.salary_max,\n            'employment_type': request.employment_type,\n            'company': request.company,\n            'region': request.region\n        }\n    )\n```\n\n## Testing\n\n```python\n@pytest.mark.asyncio\nasync def test_semantic_search():\n    engine = SemanticSearchEngine(\"test.db\", Path(\"test_embeddings\"))\n    await engine.load()\n    \n    response = await engine.search(SearchRequest(\n        query=\"python developer\",\n        limit=10\n    ))\n    \n    assert len(response.results) \u003c= 10\n    assert all(isinstance(r, JobResult) for r in response.results)\n    assert response.search_time_ms \u003c 500  # Should be fast\n\n@pytest.mark.asyncio\nasync def test_degradation():\n    # Test with missing index\n    engine = SemanticSearchEngine(\"test.db\", Path(\"nonexistent\"))\n    await engine.load()  # Should not crash\n    \n    response = await engine.search(SearchRequest(query=\"test\"))\n    assert response.degraded == True  # Should fall back to keyword\n\n@pytest.mark.asyncio  \nasync def test_caching():\n    engine = SemanticSearchEngine(\"test.db\", Path(\"test_embeddings\"))\n    await engine.load()\n    \n    request = SearchRequest(query=\"data scientist\")\n    \n    # First search\n    response1 = await engine.search(request)\n    assert response1.cache_hit == False\n    \n    # Second search (should be cached)\n    response2 = await engine.search(request)\n    assert response2.cache_hit == True\n    assert response2.search_time_ms \u003c response1.search_time_ms\n```\n\n## Dependencies\n- Requires FAISSIndexManager (Phase 2)\n- Requires QueryExpander (this phase)\n- Requires database methods (Phase 1)","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T18:33:50.184116+08:00","created_by":"David Ten","updated_at":"2026-02-06T01:21:27.618074+08:00","closed_at":"2026-02-06T01:21:27.618074+08:00","close_reason":"Implemented SemanticSearchEngine class with hybrid semantic+keyword search, caching, query expansion, and graceful degradation. All 32 tests pass.","dependencies":[{"issue_id":"MyCareersFuture-qwl.3.2","depends_on_id":"MyCareersFuture-qwl.3","type":"parent-child","created_at":"2026-02-05T18:33:50.185005+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-qwl.3.2","depends_on_id":"MyCareersFuture-qwl.3.1","type":"blocks","created_at":"2026-02-05T18:34:40.375052+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-qwl.3.3","title":"Add search-semantic CLI command","description":"# Task: Add search-semantic CLI Command\n\n## What\nAdd the `search-semantic` command to src/cli.py for command-line semantic search.\n\n## Command Design\n\n```python\n@app.command(name=\"search-semantic\")\ndef semantic_search_cli(\n    query: str = typer.Argument(..., help=\"Search query\"),\n    limit: int = typer.Option(10, \"--limit\", \"-n\", help=\"Number of results\"),\n    salary_min: Optional[int] = typer.Option(None, \"--salary-min\"),\n    salary_max: Optional[int] = typer.Option(None, \"--salary-max\"),\n    company: Optional[str] = typer.Option(None, \"--company\", \"-c\"),\n    employment_type: Optional[str] = typer.Option(None, \"--employment-type\", \"-e\"),\n    region: Optional[str] = typer.Option(None, \"--region\", \"-r\"),\n    alpha: float = typer.Option(0.7, \"--alpha\", help=\"Semantic vs BM25 weight (0-1)\"),\n    no_expand: bool = typer.Option(False, \"--no-expand\", help=\"Disable query expansion\"),\n    json_output: bool = typer.Option(False, \"--json\", help=\"Output as JSON\"),\n    db_path: str = typer.Option(\"data/mcf_jobs.db\", \"--db\"),\n    index_dir: str = typer.Option(\"data/embeddings\", \"--index-dir\"),\n    verbose: bool = typer.Option(False, \"--verbose\", \"-v\"),\n) -\u003e None:\n    \"\"\"\n    Semantic search for jobs.\n    \n    Combines keyword matching with semantic similarity for better results.\n    \n    Examples:\n        mcf search-semantic \"machine learning engineer\"\n        mcf search-semantic \"python developer\" --salary-min 8000\n        mcf search-semantic \"data scientist\" --company Google\n        mcf search-semantic \"ML\" --no-expand  # Disable expansion\n    \"\"\"\n```\n\n## Output Format (Rich Table)\n\n```\nSemantic Search Results\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nQuery: machine learning engineer\nExpanded: machine learning, ML, deep learning, engineer\nCandidates: 12,456 jobs (after filters)\nSearch time: 47ms\n\n┌───────┬────────────────────────────────────┬─────────────────────┬───────────────┐\n│ Score │ Title                              │ Company             │ Salary        │\n├───────┼────────────────────────────────────┼─────────────────────┼───────────────┤\n│ 0.923 │ Machine Learning Engineer          │ Google              │ $12,000-18,000│\n│ 0.891 │ Senior ML Engineer                 │ Meta                │ $15,000-22,000│\n│ 0.876 │ Deep Learning Specialist           │ NVIDIA              │ $10,000-16,000│\n│ 0.854 │ AI/ML Platform Engineer            │ Amazon              │ $11,000-17,000│\n│ 0.832 │ Machine Learning Research Engineer │ Microsoft           │ $13,000-19,000│\n└───────┴────────────────────────────────────┴─────────────────────┴───────────────┘\n\nTip: Use --json for programmatic access\n```\n\n## JSON Output Format\n\n```json\n{\n  \"query\": \"machine learning engineer\",\n  \"query_expansion\": [\"machine learning\", \"ML\", \"deep learning\", \"engineer\"],\n  \"total_candidates\": 12456,\n  \"search_time_ms\": 47.2,\n  \"degraded\": false,\n  \"results\": [\n    {\n      \"uuid\": \"abc-123\",\n      \"title\": \"Machine Learning Engineer\",\n      \"company_name\": \"Google\",\n      \"salary_min\": 12000,\n      \"salary_max\": 18000,\n      \"similarity_score\": 0.923,\n      \"job_url\": \"https://...\"\n    }\n  ]\n}\n```\n\n## Implementation\n\n```python\n@app.command(name=\"search-semantic\")\ndef semantic_search_cli(...):\n    setup_logging(verbose)\n    \n    async def run():\n        engine = SemanticSearchEngine(db_path, Path(index_dir))\n        \n        # Load indexes\n        console.print(\"[dim]Loading search indexes...[/dim]\")\n        try:\n            await engine.load()\n        except Exception as e:\n            console.print(f\"[red]Failed to load indexes: {e}[/red]\")\n            console.print(\"[yellow]Run 'mcf embed-generate' first.[/yellow]\")\n            raise typer.Exit(1)\n        \n        # Build request\n        request = SearchRequest(\n            query=query,\n            limit=limit,\n            salary_min=salary_min,\n            salary_max=salary_max,\n            company=company,\n            employment_type=employment_type,\n            region=region,\n            alpha=alpha,\n            expand_query=not no_expand\n        )\n        \n        # Search\n        response = await engine.search(request)\n        \n        if json_output:\n            # JSON output\n            print(response.model_dump_json(indent=2))\n        else:\n            # Rich table output\n            _display_search_results(response, query)\n    \n    asyncio.run(run())\n\ndef _display_search_results(response: SearchResponse, query: str):\n    \"\"\"Display search results as Rich table.\"\"\"\n    console.print(f\"\\n[bold blue]Semantic Search Results[/bold blue]\")\n    console.print(\"━\" * 70)\n    \n    # Query info\n    console.print(f\"\\nQuery: [green]{query}[/green]\")\n    if response.query_expansion:\n        console.print(f\"Expanded: [dim]{', '.join(response.query_expansion)}[/dim]\")\n    console.print(f\"Candidates: {response.total_candidates:,} jobs (after filters)\")\n    console.print(f\"Search time: [cyan]{response.search_time_ms:.0f}ms[/cyan]\")\n    \n    if response.degraded:\n        console.print(\"[yellow]⚠ Running in degraded mode (keyword search only)[/yellow]\")\n    \n    if not response.results:\n        console.print(\"\\n[yellow]No results found. Try broadening your search.[/yellow]\")\n        return\n    \n    # Results table\n    table = Table(show_header=True, header_style=\"bold\")\n    table.add_column(\"Score\", justify=\"right\", style=\"green\", width=7)\n    table.add_column(\"Title\", max_width=40)\n    table.add_column(\"Company\", max_width=20)\n    table.add_column(\"Salary\", justify=\"right\", width=15)\n    \n    for job in response.results:\n        salary = \"\"\n        if job.salary_min and job.salary_max:\n            salary = f\"${job.salary_min:,}-${job.salary_max:,}\"\n        elif job.salary_min:\n            salary = f\"${job.salary_min:,}+\"\n        \n        table.add_row(\n            f\"{job.similarity_score:.3f}\",\n            job.title[:40],\n            job.company_name[:20] if job.company_name else \"N/A\",\n            salary\n        )\n    \n    console.print()\n    console.print(table)\n    console.print(f\"\\n[dim]Tip: Use --json for programmatic access[/dim]\")\n```\n\n## Error Handling\n\n- If indexes don't exist: Suggest running embed-generate\n- If search fails: Show error and suggest checking logs\n- If no results: Suggest broadening search terms\n\n## Testing\n\n```bash\n# Basic search\nmcf search-semantic \"python developer\"\n\n# With filters\nmcf search-semantic \"data engineer\" --salary-min 10000 --company Google\n\n# JSON output for scripting\nmcf search-semantic \"ML\" --json | jq '.results[0].title'\n\n# Disable expansion\nmcf search-semantic \"DS\" --no-expand\n\n# Verbose mode\nmcf search-semantic \"AI\" --verbose\n```\n\n## Dependencies\n- Requires SemanticSearchEngine (MyCareersFuture-qwl.3.2)","status":"closed","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T18:34:31.221151+08:00","created_by":"David Ten","updated_at":"2026-02-06T07:36:12.728862+08:00","closed_at":"2026-02-06T07:36:12.728862+08:00","close_reason":"Implemented search-semantic CLI command with Rich table output, JSON output, all filter options (salary, company, employment type, region), alpha tuning, query expansion toggle, and graceful degradation handling","dependencies":[{"issue_id":"MyCareersFuture-qwl.3.3","depends_on_id":"MyCareersFuture-qwl.3","type":"parent-child","created_at":"2026-02-05T18:34:31.22199+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-qwl.3.3","depends_on_id":"MyCareersFuture-qwl.3.2","type":"blocks","created_at":"2026-02-05T18:34:40.603332+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-qwl.4","title":"Phase 4: FastAPI Backend","description":"# Phase 4: FastAPI Backend\n\n## Purpose\nCreate a REST API that exposes the semantic search functionality to web clients. This enables the React frontend (Phase 6) and any other integrations to use the search system.\n\n## Why FastAPI?\nWe chose FastAPI over Flask/Django because:\n1. **Native async support**: Search operations can run concurrently\n2. **Automatic OpenAPI docs**: Self-documenting API at /docs\n3. **Pydantic integration**: Our models work directly as request/response schemas\n4. **Type hints**: Better IDE support and runtime validation\n5. **Performance**: One of the fastest Python web frameworks\n\n## API Design Principles\n\n### RESTful Conventions\n- POST for searches (body contains complex query parameters)\n- GET for simple reads (stats, health)\n- Consistent error responses\n- Pagination support\n\n### Request/Response Patterns\nAll endpoints return:\n```json\n{\n  \"data\": [...],\n  \"meta\": {\n    \"total\": 1234,\n    \"latency_ms\": 47,\n    \"degraded\": false\n  }\n}\n```\n\n## Endpoints to Implement\n\n### Core Search Endpoints\n| Method | Path | Description |\n|--------|------|-------------|\n| POST | /api/search | Main semantic search |\n| POST | /api/similar | Similar jobs to given UUID |\n| POST | /api/similar/batch | Batch similar jobs (max 50) |\n| POST | /api/search/skills | Skill-based search |\n| POST | /api/companies/similar | Company similarity |\n\n### Utility Endpoints  \n| Method | Path | Description |\n|--------|------|-------------|\n| GET | /api/stats | System statistics |\n| GET | /api/analytics/popular | Popular queries |\n| GET | /api/analytics/performance | Latency percentiles |\n| GET | /health | Health check |\n\n## Key Components\n\n### 4.1 FastAPI Application Setup\n- Application factory\n- CORS middleware configuration\n- Startup/shutdown hooks for index loading\n- Exception handlers\n\n### 4.2 API Models (Pydantic)\n- Request schemas with validation\n- Response schemas with computed fields\n- Error response models\n\n### 4.3 Middleware\n- Rate limiting (100 req/min per IP)\n- Request logging\n- Response timing headers\n\n### 4.4 api-serve CLI Command\n- Start server with uvicorn\n- Configurable host/port\n- Development reload mode\n\n## Request/Response Examples\n\n### POST /api/search\nRequest:\n```json\n{\n  \"query\": \"machine learning engineer\",\n  \"limit\": 20,\n  \"salary_min\": 10000,\n  \"employment_type\": \"Full Time\",\n  \"alpha\": 0.7,\n  \"expand_query\": true\n}\n```\n\nResponse:\n```json\n{\n  \"results\": [\n    {\n      \"uuid\": \"abc-123\",\n      \"title\": \"Machine Learning Engineer\",\n      \"company_name\": \"Google\",\n      \"similarity_score\": 0.923,\n      ...\n    }\n  ],\n  \"total_candidates\": 12456,\n  \"search_time_ms\": 47.2,\n  \"query_expansion\": [\"machine learning\", \"ML\", \"deep learning\"],\n  \"degraded\": false,\n  \"cache_hit\": false\n}\n```\n\n### POST /api/similar/batch\nRequest:\n```json\n{\n  \"job_uuids\": [\"uuid-1\", \"uuid-2\", \"uuid-3\"],\n  \"limit_per_job\": 5,\n  \"exclude_same_company\": true\n}\n```\n\nResponse:\n```json\n{\n  \"results\": {\n    \"uuid-1\": [...],\n    \"uuid-2\": [...],\n    \"uuid-3\": [...]\n  },\n  \"search_time_ms\": 123.4\n}\n```\n\n## Error Handling\n\nStandard error response:\n```json\n{\n  \"error\": {\n    \"code\": \"VALIDATION_ERROR\",\n    \"message\": \"salary_min must be positive\",\n    \"details\": {...}\n  }\n}\n```\n\nError codes:\n- `VALIDATION_ERROR`: Invalid request parameters\n- `NOT_FOUND`: Resource not found (job, company)\n- `INDEX_NOT_READY`: Embeddings not generated yet\n- `RATE_LIMITED`: Too many requests\n- `INTERNAL_ERROR`: Unexpected server error\n\n## Security Considerations\n\n### Rate Limiting\n- 100 requests/minute per IP\n- Configurable via environment variable\n- Returns 429 with Retry-After header\n\n### Input Validation\n- Query length: max 500 chars\n- Limit: max 100\n- UUID format validation\n- SQL injection prevention (parameterized queries)\n\n### CORS Configuration\n- Allow localhost:3000 (React dev)\n- Production: Configure allowed origins\n\n## Performance Targets\n- p95 latency: \u003c100ms\n- Cold start: \u003c5s (index loading)\n- Concurrent requests: 100+\n\n## Files to Create\n- `src/api/__init__.py`: Package\n- `src/api/app.py`: FastAPI application\n- `src/api/models.py`: Request/response schemas\n- `src/api/middleware.py`: Rate limiting, logging\n\n## Exit Criteria\n- [ ] All endpoints return correct responses\n- [ ] OpenAPI docs accessible at /docs\n- [ ] Rate limiting works correctly\n- [ ] CORS allows frontend access\n- [ ] Error responses follow standard format\n- [ ] api-serve CLI command works","status":"closed","priority":1,"issue_type":"epic","owner":"davidten7@gmail.com","created_at":"2026-02-05T18:35:10.675337+08:00","created_by":"David Ten","updated_at":"2026-02-06T08:34:17.327689+08:00","closed_at":"2026-02-06T08:34:17.327689+08:00","close_reason":"All P1 tasks complete: FastAPI app with endpoints (qwl.4.1), Pydantic models (qwl.4.2), api-serve CLI (qwl.4.4). Remaining P2 tasks (rate limiting, benchmarking) are standalone enhancements.","dependencies":[{"issue_id":"MyCareersFuture-qwl.4","depends_on_id":"MyCareersFuture-qwl","type":"parent-child","created_at":"2026-02-05T18:35:10.676911+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-qwl.4","depends_on_id":"MyCareersFuture-qwl.3","type":"blocks","created_at":"2026-02-05T18:38:34.435437+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-qwl.4.1","title":"Create FastAPI application and core endpoints","description":"# Task: Create FastAPI Application and Core Endpoints\n\n## What\nCreate src/api/app.py with the FastAPI application, middleware setup, and all search endpoints.\n\n## Application Structure\n\n```python\n# src/api/app.py\n\nfrom fastapi import FastAPI, HTTPException, Depends, Request\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom contextlib import asynccontextmanager\nfrom pathlib import Path\nimport logging\n\nfrom .models import (\n    SearchRequest, SearchResponse,\n    SimilarJobsRequest, SimilarBatchRequest, SimilarBatchResponse,\n    SkillSearchRequest, CompanySimilarityRequest, CompanySimilarity,\n    StatsResponse, HealthResponse, ErrorResponse\n)\nfrom .middleware import RateLimitMiddleware\nfrom src.mcf.embeddings import SemanticSearchEngine\n\nlogger = logging.getLogger(__name__)\n\n# Global search engine instance\n_search_engine: Optional[SemanticSearchEngine] = None\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Application lifespan handler - load indexes on startup.\"\"\"\n    global _search_engine\n    \n    logger.info(\"Loading search indexes...\")\n    _search_engine = SemanticSearchEngine(\n        db_path=app.state.db_path,\n        index_dir=Path(app.state.index_dir)\n    )\n    \n    try:\n        await _search_engine.load()\n        logger.info(\"Search indexes loaded successfully\")\n    except Exception as e:\n        logger.warning(f\"Failed to load indexes: {e}. Running in degraded mode.\")\n    \n    yield\n    \n    # Cleanup on shutdown\n    logger.info(\"Shutting down search engine\")\n\ndef create_app(\n    db_path: str = \"data/mcf_jobs.db\",\n    index_dir: str = \"data/embeddings\",\n    cors_origins: list[str] = None\n) -\u003e FastAPI:\n    \"\"\"\n    Application factory for FastAPI.\n    \n    Args:\n        db_path: Path to SQLite database\n        index_dir: Path to FAISS indexes\n        cors_origins: Allowed CORS origins\n    \"\"\"\n    app = FastAPI(\n        title=\"MCF Semantic Search API\",\n        description=\"Semantic job search with hybrid BM25 + vector ranking\",\n        version=\"1.0.0\",\n        lifespan=lifespan\n    )\n    \n    # Store config in app state\n    app.state.db_path = db_path\n    app.state.index_dir = index_dir\n    \n    # CORS\n    if cors_origins is None:\n        cors_origins = [\"http://localhost:3000\", \"http://localhost:5173\"]\n    \n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=cors_origins,\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n    \n    # Rate limiting\n    app.add_middleware(RateLimitMiddleware, requests_per_minute=100)\n    \n    return app\n\napp = create_app()\n\ndef get_engine() -\u003e SemanticSearchEngine:\n    \"\"\"Dependency to get search engine.\"\"\"\n    if _search_engine is None:\n        raise HTTPException(503, \"Search engine not initialized\")\n    return _search_engine\n```\n\n## Core Search Endpoints\n\n```python\n@app.post(\"/api/search\", response_model=SearchResponse)\nasync def semantic_search(\n    request: SearchRequest,\n    engine: SemanticSearchEngine = Depends(get_engine)\n) -\u003e SearchResponse:\n    \"\"\"\n    Semantic job search with filters.\n    \n    Combines:\n    - SQL filtering (salary, location, type)\n    - Semantic vector search\n    - BM25 keyword ranking\n    - Query expansion\n    - Freshness boosting\n    \"\"\"\n    try:\n        return await engine.search(request)\n    except ValueError as e:\n        raise HTTPException(400, str(e))\n    except Exception as e:\n        logger.exception(\"Search failed\")\n        raise HTTPException(500, \"Internal search error\")\n\n@app.post(\"/api/similar\", response_model=SearchResponse)\nasync def similar_jobs(\n    request: SimilarJobsRequest,\n    engine: SemanticSearchEngine = Depends(get_engine)\n) -\u003e SearchResponse:\n    \"\"\"Find jobs similar to a given job.\"\"\"\n    try:\n        return await engine.find_similar(request)\n    except ValueError as e:\n        raise HTTPException(404, str(e))\n\n@app.post(\"/api/similar/batch\", response_model=SimilarBatchResponse)\nasync def similar_jobs_batch(\n    request: SimilarBatchRequest,\n    engine: SemanticSearchEngine = Depends(get_engine)\n) -\u003e SimilarBatchResponse:\n    \"\"\"\n    Find similar jobs for multiple UUIDs.\n    \n    Efficient batch operation for recommendation carousels.\n    Limited to 50 UUIDs per request.\n    \"\"\"\n    if len(request.job_uuids) \u003e 50:\n        raise HTTPException(400, \"Maximum 50 UUIDs per batch request\")\n    \n    import asyncio\n    \n    async def find_similar_one(uuid: str) -\u003e tuple[str, list]:\n        try:\n            response = await engine.find_similar(SimilarJobsRequest(\n                job_uuid=uuid,\n                limit=request.limit_per_job,\n                exclude_same_company=request.exclude_same_company\n            ))\n            return uuid, response.results\n        except ValueError:\n            return uuid, []\n    \n    tasks = [find_similar_one(uuid) for uuid in request.job_uuids]\n    results_list = await asyncio.gather(*tasks)\n    \n    return SimilarBatchResponse(\n        results=dict(results_list),\n        search_time_ms=sum(r[1].search_time_ms for r in results_list if r[1])\n    )\n\n@app.post(\"/api/search/skills\", response_model=SearchResponse)\nasync def skill_search(\n    request: SkillSearchRequest,\n    engine: SemanticSearchEngine = Depends(get_engine)\n) -\u003e SearchResponse:\n    \"\"\"Search jobs by skill similarity.\"\"\"\n    return await engine.search_by_skill(request)\n\n@app.post(\"/api/companies/similar\", response_model=list[CompanySimilarity])\nasync def similar_companies(\n    request: CompanySimilarityRequest,\n    engine: SemanticSearchEngine = Depends(get_engine)\n) -\u003e list[CompanySimilarity]:\n    \"\"\"Find companies with similar job profiles.\"\"\"\n    try:\n        return await engine.find_similar_companies(request)\n    except ValueError as e:\n        raise HTTPException(404, str(e))\n```\n\n## Utility Endpoints\n\n```python\n@app.get(\"/api/stats\", response_model=StatsResponse)\nasync def get_stats(\n    engine: SemanticSearchEngine = Depends(get_engine)\n) -\u003e StatsResponse:\n    \"\"\"Get system statistics.\"\"\"\n    return StatsResponse(**engine.get_stats())\n\n@app.get(\"/api/analytics/popular\")\nasync def popular_queries(\n    days: int = 7,\n    limit: int = 20,\n    engine: SemanticSearchEngine = Depends(get_engine)\n) -\u003e list[dict]:\n    \"\"\"Get most popular search queries.\"\"\"\n    return engine.db.get_popular_queries(days=days, limit=limit)\n\n@app.get(\"/api/analytics/performance\")\nasync def performance_stats(\n    days: int = 7,\n    engine: SemanticSearchEngine = Depends(get_engine)\n) -\u003e dict:\n    \"\"\"Get search latency percentiles.\"\"\"\n    return engine.db.get_search_latency_percentiles(days=days)\n\n@app.get(\"/health\", response_model=HealthResponse)\nasync def health_check() -\u003e HealthResponse:\n    \"\"\"Health check endpoint.\"\"\"\n    degraded = _search_engine._degraded if _search_engine else True\n    return HealthResponse(\n        status=\"healthy\" if not degraded else \"degraded\",\n        index_loaded=_search_engine is not None and _search_engine._loaded,\n        degraded=degraded\n    )\n```\n\n## Exception Handlers\n\n```python\n@app.exception_handler(HTTPException)\nasync def http_exception_handler(request: Request, exc: HTTPException):\n    return JSONResponse(\n        status_code=exc.status_code,\n        content={\n            \"error\": {\n                \"code\": _status_to_code(exc.status_code),\n                \"message\": exc.detail\n            }\n        }\n    )\n\n@app.exception_handler(Exception)\nasync def general_exception_handler(request: Request, exc: Exception):\n    logger.exception(\"Unhandled exception\")\n    return JSONResponse(\n        status_code=500,\n        content={\n            \"error\": {\n                \"code\": \"INTERNAL_ERROR\",\n                \"message\": \"An unexpected error occurred\"\n            }\n        }\n    )\n\ndef _status_to_code(status: int) -\u003e str:\n    codes = {\n        400: \"VALIDATION_ERROR\",\n        404: \"NOT_FOUND\",\n        429: \"RATE_LIMITED\",\n        500: \"INTERNAL_ERROR\",\n        503: \"SERVICE_UNAVAILABLE\"\n    }\n    return codes.get(status, \"UNKNOWN_ERROR\")\n```\n\n## Testing\n\n```python\nfrom fastapi.testclient import TestClient\n\ndef test_search_endpoint():\n    client = TestClient(app)\n    \n    response = client.post(\"/api/search\", json={\n        \"query\": \"python developer\",\n        \"limit\": 10\n    })\n    \n    assert response.status_code == 200\n    data = response.json()\n    assert \"results\" in data\n    assert \"search_time_ms\" in data\n\ndef test_health_endpoint():\n    client = TestClient(app)\n    \n    response = client.get(\"/health\")\n    assert response.status_code == 200\n    assert response.json()[\"status\"] in [\"healthy\", \"degraded\"]\n\ndef test_rate_limiting():\n    client = TestClient(app)\n    \n    # Make 101 requests rapidly\n    for _ in range(101):\n        response = client.get(\"/health\")\n    \n    # Last one should be rate limited\n    assert response.status_code == 429\n```\n\n## Dependencies\n- Requires SemanticSearchEngine (Phase 3)\n- Will be used by api-serve CLI command","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T18:35:44.595634+08:00","created_by":"David Ten","updated_at":"2026-02-06T08:12:38.553573+08:00","closed_at":"2026-02-06T08:12:38.553573+08:00","close_reason":"Created FastAPI app with all endpoints, dependency injection, error handling, CORS. 26 tests passing.","dependencies":[{"issue_id":"MyCareersFuture-qwl.4.1","depends_on_id":"MyCareersFuture-qwl.4","type":"parent-child","created_at":"2026-02-05T18:35:44.597387+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-qwl.4.1","depends_on_id":"MyCareersFuture-qwl.4.2","type":"blocks","created_at":"2026-02-05T18:38:34.654749+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-qwl.4.2","title":"Create API request/response models","description":"# Task: Create API Request/Response Models\n\n## What\nCreate src/api/models.py with all Pydantic models for API requests and responses.\n\n## Models to Create\n\n```python\n# src/api/models.py\n\nfrom typing import Optional\nfrom pydantic import BaseModel, Field, field_validator\nfrom datetime import date\n\n# ==================== Request Models ====================\n\nclass SearchRequest(BaseModel):\n    \"\"\"Request for semantic job search.\"\"\"\n    query: str = Field(..., min_length=1, max_length=500,\n        description=\"Search query text\")\n    limit: int = Field(10, ge=1, le=100,\n        description=\"Maximum results to return\")\n    \n    # SQL Filters\n    salary_min: Optional[int] = Field(None, ge=0,\n        description=\"Minimum salary filter\")\n    salary_max: Optional[int] = Field(None, ge=0,\n        description=\"Maximum salary filter\")\n    employment_type: Optional[str] = Field(None,\n        description=\"Employment type: 'Full Time', 'Part Time', 'Contract'\")\n    region: Optional[str] = Field(None,\n        description=\"Region filter\")\n    company: Optional[str] = Field(None,\n        description=\"Company name filter (partial match)\")\n    \n    # Hybrid Search Tuning\n    alpha: float = Field(0.7, ge=0.0, le=1.0,\n        description=\"Weight for semantic vs BM25 (0.7 = 70% semantic)\")\n    freshness_weight: float = Field(0.1, ge=0.0, le=1.0,\n        description=\"Weight for recency boost\")\n    expand_query: bool = Field(True,\n        description=\"Enable query expansion with synonyms\")\n    min_similarity: float = Field(0.3, ge=0.0, le=1.0,\n        description=\"Minimum similarity score threshold\")\n    \n    @field_validator('salary_max')\n    @classmethod\n    def salary_max_greater_than_min(cls, v, info):\n        if v is not None and info.data.get('salary_min') is not None:\n            if v \u003c info.data['salary_min']:\n                raise ValueError('salary_max must be \u003e= salary_min')\n        return v\n\nclass SimilarJobsRequest(BaseModel):\n    \"\"\"Request for finding similar jobs.\"\"\"\n    job_uuid: str = Field(..., description=\"Source job UUID\")\n    limit: int = Field(10, ge=1, le=100)\n    exclude_same_company: bool = Field(False,\n        description=\"Exclude jobs from the same company\")\n    freshness_weight: float = Field(0.1, ge=0.0, le=1.0)\n\nclass SimilarBatchRequest(BaseModel):\n    \"\"\"Request for batch similar jobs.\"\"\"\n    job_uuids: list[str] = Field(..., min_length=1, max_length=50,\n        description=\"List of job UUIDs (max 50)\")\n    limit_per_job: int = Field(5, ge=1, le=20)\n    exclude_same_company: bool = Field(False)\n\nclass SkillSearchRequest(BaseModel):\n    \"\"\"Request for skill-based job search.\"\"\"\n    skill: str = Field(..., min_length=1, max_length=100,\n        description=\"Skill to search for\")\n    limit: int = Field(20, ge=1, le=100)\n    \n    # Optional SQL filters\n    salary_min: Optional[int] = Field(None, ge=0)\n    salary_max: Optional[int] = Field(None, ge=0)\n    employment_type: Optional[str] = None\n\nclass CompanySimilarityRequest(BaseModel):\n    \"\"\"Request for finding similar companies.\"\"\"\n    company_name: str = Field(..., min_length=1, max_length=200,\n        description=\"Source company name\")\n    limit: int = Field(10, ge=1, le=50)\n\n# ==================== Response Models ====================\n\nclass JobResult(BaseModel):\n    \"\"\"A job in search results.\"\"\"\n    uuid: str\n    title: str\n    company_name: Optional[str]\n    description: str = Field(description=\"Truncated to 500 chars\")\n    salary_min: Optional[int]\n    salary_max: Optional[int]\n    employment_type: Optional[str]\n    skills: Optional[str]\n    location: Optional[str]\n    posted_date: Optional[str]\n    job_url: str\n    similarity_score: float = Field(description=\"Combined hybrid score [0, 1]\")\n    bm25_score: Optional[float] = Field(None, description=\"BM25 keyword score\")\n    freshness_score: Optional[float] = Field(None, description=\"Recency score\")\n\nclass SearchResponse(BaseModel):\n    \"\"\"Response from search endpoints.\"\"\"\n    results: list[JobResult]\n    total_candidates: int = Field(\n        description=\"Jobs matching filters before semantic ranking\")\n    search_time_ms: float\n    query_expansion: Optional[list[str]] = Field(None,\n        description=\"Expanded query terms if expansion was enabled\")\n    degraded: bool = Field(False,\n        description=\"True if fell back to keyword-only search\")\n    cache_hit: bool = Field(False,\n        description=\"True if result was served from cache\")\n\nclass SimilarBatchResponse(BaseModel):\n    \"\"\"Response from batch similar jobs endpoint.\"\"\"\n    results: dict[str, list[JobResult]] = Field(\n        description=\"Mapping of UUID -\u003e similar jobs\")\n    search_time_ms: float\n\nclass CompanySimilarity(BaseModel):\n    \"\"\"A similar company result.\"\"\"\n    company_name: str\n    similarity_score: float\n    job_count: int = Field(description=\"Total jobs from this company\")\n    avg_salary: Optional[int] = Field(None, description=\"Average salary offered\")\n    top_skills: list[str] = Field(default_factory=list,\n        description=\"Most common skills in job postings\")\n\nclass StatsResponse(BaseModel):\n    \"\"\"System statistics response.\"\"\"\n    total_jobs: int\n    jobs_with_embeddings: int\n    embedding_coverage_pct: float\n    unique_skills: int\n    unique_companies: int\n    index_size_mb: float\n    model_version: str\n    \nclass HealthResponse(BaseModel):\n    \"\"\"Health check response.\"\"\"\n    status: str = Field(description=\"'healthy' or 'degraded'\")\n    index_loaded: bool\n    degraded: bool = Field(description=\"True if running without vector search\")\n\n# ==================== Error Models ====================\n\nclass ErrorDetail(BaseModel):\n    \"\"\"Error detail in response.\"\"\"\n    code: str = Field(description=\"Error code (e.g., VALIDATION_ERROR)\")\n    message: str\n    details: Optional[dict] = None\n\nclass ErrorResponse(BaseModel):\n    \"\"\"Standard error response.\"\"\"\n    error: ErrorDetail\n\n# ==================== Config for OpenAPI ====================\n\n# Add examples for OpenAPI documentation\nSearchRequest.model_config = {\n    \"json_schema_extra\": {\n        \"examples\": [\n            {\n                \"query\": \"machine learning engineer\",\n                \"limit\": 20,\n                \"salary_min\": 10000,\n                \"alpha\": 0.7\n            }\n        ]\n    }\n}\n\nSearchResponse.model_config = {\n    \"json_schema_extra\": {\n        \"examples\": [\n            {\n                \"results\": [\n                    {\n                        \"uuid\": \"abc-123\",\n                        \"title\": \"ML Engineer\",\n                        \"company_name\": \"Google\",\n                        \"description\": \"We are looking for...\",\n                        \"salary_min\": 12000,\n                        \"salary_max\": 18000,\n                        \"employment_type\": \"Full Time\",\n                        \"skills\": \"Python, TensorFlow, PyTorch\",\n                        \"location\": \"Singapore\",\n                        \"posted_date\": \"2024-01-15\",\n                        \"job_url\": \"https://...\",\n                        \"similarity_score\": 0.923\n                    }\n                ],\n                \"total_candidates\": 1234,\n                \"search_time_ms\": 47.2,\n                \"degraded\": False,\n                \"cache_hit\": False\n            }\n        ]\n    }\n}\n```\n\n## Validation Rules\n\n### Query Constraints\n- `query`: 1-500 characters\n- `limit`: 1-100 for most endpoints\n- `salary_*`: Non-negative integers\n- `alpha`, `freshness_weight`: 0.0-1.0\n- `job_uuids` (batch): Max 50 items\n\n### Business Rules\n- `salary_max` must be \u003e= `salary_min` if both provided\n- `employment_type` should match known values (validated loosely)\n- UUID format not strictly validated (handled by database)\n\n## Testing\n\n```python\ndef test_search_request_validation():\n    # Valid request\n    req = SearchRequest(query=\"test\")\n    assert req.limit == 10  # Default\n    assert req.alpha == 0.7  # Default\n    \n    # Invalid: query too long\n    with pytest.raises(ValidationError):\n        SearchRequest(query=\"x\" * 501)\n    \n    # Invalid: salary_max \u003c salary_min\n    with pytest.raises(ValidationError):\n        SearchRequest(query=\"test\", salary_min=10000, salary_max=5000)\n    \n    # Invalid: alpha out of range\n    with pytest.raises(ValidationError):\n        SearchRequest(query=\"test\", alpha=1.5)\n\ndef test_batch_request_validation():\n    # Valid\n    req = SimilarBatchRequest(job_uuids=[\"a\", \"b\", \"c\"])\n    \n    # Invalid: too many UUIDs\n    with pytest.raises(ValidationError):\n        SimilarBatchRequest(job_uuids=[\"x\"] * 51)\n```\n\n## Dependencies\n- Used by FastAPI app (MyCareersFuture-qwl.4.1)\n- Consistent with SemanticSearchEngine interface","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T18:36:35.530211+08:00","created_by":"David Ten","updated_at":"2026-02-06T07:57:13.918649+08:00","closed_at":"2026-02-06T07:57:13.918649+08:00","close_reason":"Created src/api/models.py with all Pydantic request/response models, to_internal()/from_internal() converters, validation rules, OpenAPI examples, and 36 tests","dependencies":[{"issue_id":"MyCareersFuture-qwl.4.2","depends_on_id":"MyCareersFuture-qwl.4","type":"parent-child","created_at":"2026-02-05T18:36:35.531567+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-qwl.4.3","title":"Implement rate limiting and logging middleware","description":"# Task: Implement Rate Limiting and Logging Middleware\n\n## What\nCreate src/api/middleware.py with rate limiting and request logging middleware.\n\n**Note:** This is an enhancement task. The FastAPI app works without middleware - this adds production-hardening features.\n\n## Rate Limiting Middleware\n\n```python\n# src/api/middleware.py\n\nfrom fastapi import Request, HTTPException\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom collections import defaultdict\nimport time\nimport asyncio\n\nclass RateLimitMiddleware(BaseHTTPMiddleware):\n    \"\"\"\n    Simple in-memory rate limiter.\n    \n    For production, consider Redis-based rate limiting for multi-instance deployments.\n    \"\"\"\n    \n    def __init__(self, app, requests_per_minute: int = 100):\n        super().__init__(app)\n        self.requests_per_minute = requests_per_minute\n        self.requests = defaultdict(list)  # IP -\u003e list of timestamps\n        self._cleanup_task = None\n    \n    async def dispatch(self, request: Request, call_next):\n        client_ip = self._get_client_ip(request)\n        now = time.time()\n        \n        # Clean old requests for this IP\n        cutoff = now - 60\n        self.requests[client_ip] = [\n            ts for ts in self.requests[client_ip] if ts \u003e cutoff\n        ]\n        \n        # Check rate limit\n        if len(self.requests[client_ip]) \u003e= self.requests_per_minute:\n            raise HTTPException(\n                status_code=429,\n                detail=f\"Rate limit exceeded. Max {self.requests_per_minute} requests per minute.\"\n            )\n        \n        # Record this request\n        self.requests[client_ip].append(now)\n        \n        # Process request\n        response = await call_next(request)\n        return response\n    \n    def _get_client_ip(self, request: Request) -\u003e str:\n        \"\"\"Get client IP, respecting X-Forwarded-For header.\"\"\"\n        forwarded = request.headers.get(\"X-Forwarded-For\")\n        if forwarded:\n            return forwarded.split(\",\")[0].strip()\n        return request.client.host if request.client else \"unknown\"\n```\n\n## Request Logging Middleware\n\n```python\nclass RequestLoggingMiddleware(BaseHTTPMiddleware):\n    \"\"\"Log all requests for debugging and analytics.\"\"\"\n    \n    async def dispatch(self, request: Request, call_next):\n        start_time = time.time()\n        \n        # Process request\n        response = await call_next(request)\n        \n        # Log after response\n        duration_ms = (time.time() - start_time) * 1000\n        logger.info(\n            f\"{request.method} {request.url.path} \"\n            f\"status={response.status_code} \"\n            f\"duration={duration_ms:.1f}ms \"\n            f\"ip={self._get_client_ip(request)}\"\n        )\n        \n        return response\n```\n\n## Testing\n\n```python\ndef test_rate_limiting():\n    from fastapi.testclient import TestClient\n    \n    app = create_app()\n    app.add_middleware(RateLimitMiddleware, requests_per_minute=5)\n    client = TestClient(app)\n    \n    # First 5 requests should succeed\n    for _ in range(5):\n        response = client.get(\"/health\")\n        assert response.status_code == 200\n    \n    # 6th request should be rate limited\n    response = client.get(\"/health\")\n    assert response.status_code == 429\n```\n\n## Dependencies\n- Used by FastAPI app (optional enhancement)\n- Can be added after initial API is working","status":"closed","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T18:37:24.616203+08:00","created_by":"David Ten","updated_at":"2026-02-06T09:56:05.068238+08:00","closed_at":"2026-02-06T09:56:05.068238+08:00","close_reason":"Implemented rate limiting and request logging middleware with 19 passing tests","dependencies":[{"issue_id":"MyCareersFuture-qwl.4.3","depends_on_id":"MyCareersFuture-qwl.4","type":"parent-child","created_at":"2026-02-05T18:37:24.617845+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-qwl.4.4","title":"Add api-serve CLI command","description":"# Task: Add api-serve CLI Command\n\n## What\nAdd the `mcf api-serve` command to src/cli.py for starting the FastAPI server.\n\n**Priority: P1** - This is essential for testing the API during development.\n\n## Command Implementation\n\n```python\n@app.command(name=\"api-serve\")\ndef serve_api(\n    host: str = typer.Option(\"127.0.0.1\", \"--host\", \"-h\",\n        help=\"Host to bind to\"),\n    port: int = typer.Option(8000, \"--port\", \"-p\",\n        help=\"Port to bind to\"),\n    db_path: str = typer.Option(\"data/mcf_jobs.db\", \"--db\",\n        help=\"Path to SQLite database\"),\n    index_dir: str = typer.Option(\"data/embeddings\", \"--index-dir\",\n        help=\"Path to FAISS indexes\"),\n    reload: bool = typer.Option(False, \"--reload\",\n        help=\"Enable auto-reload for development\"),\n    workers: int = typer.Option(1, \"--workers\", \"-w\",\n        help=\"Number of worker processes (production)\"),\n    cors_origins: str = typer.Option(\n        \"http://localhost:3000,http://localhost:5173\",\n        \"--cors\",\n        help=\"Comma-separated CORS origins\"\n    ),\n) -\u003e None:\n    \"\"\"\n    Start the semantic search API server.\n    \n    Examples:\n        mcf api-serve                     # Start on localhost:8000\n        mcf api-serve --port 9000         # Custom port\n        mcf api-serve --reload            # Auto-reload for dev\n        mcf api-serve --workers 4         # Production mode\n    \n    After starting, visit http://localhost:8000/docs for API documentation.\n    \"\"\"\n    import uvicorn\n    \n    # Parse CORS origins\n    origins = [o.strip() for o in cors_origins.split(\",\") if o.strip()]\n    \n    console = Console()\n    \n    # Check prerequisites\n    if not Path(db_path).exists():\n        console.print(f\"[red]Error:[/red] Database not found: {db_path}\")\n        console.print(\"Run 'mcf scrape' first to populate the database.\")\n        raise typer.Exit(1)\n    \n    index_path = Path(index_dir)\n    if not (index_path / \"jobs.index\").exists():\n        console.print(f\"[yellow]Warning:[/yellow] FAISS index not found in {index_dir}\")\n        console.print(\"API will run in degraded mode (keyword search only).\")\n        console.print(\"Run 'mcf embed-generate' to enable semantic search.\")\n    \n    # Display startup info\n    console.print(\"\\n[bold green]Starting MCF Semantic Search API[/bold green]\")\n    console.print(f\"  Database:   {db_path}\")\n    console.print(f\"  Index dir:  {index_dir}\")\n    console.print(f\"  Endpoint:   http://{host}:{port}\")\n    console.print(f\"  API docs:   http://{host}:{port}/docs\")\n    console.print(f\"  CORS:       {', '.join(origins)}\")\n    if reload:\n        console.print(\"  Mode:       [yellow]Development (auto-reload)[/yellow]\")\n    else:\n        console.print(f\"  Mode:       Production ({workers} workers)\")\n    console.print()\n    \n    # Configure and start uvicorn\n    config = {\n        \"app\": \"src.api.app:app\",\n        \"host\": host,\n        \"port\": port,\n        \"reload\": reload,\n        \"workers\": 1 if reload else workers,\n        \"log_level\": \"info\",\n    }\n    \n    # Pass config to app via environment (cleaner than module-level globals)\n    import os\n    os.environ[\"MCF_DB_PATH\"] = db_path\n    os.environ[\"MCF_INDEX_DIR\"] = index_dir\n    os.environ[\"MCF_CORS_ORIGINS\"] = cors_origins\n    \n    uvicorn.run(**config)\n```\n\n## App Configuration from Environment\n\n```python\n# In src/api/app.py, update create_app:\n\ndef create_app() -\u003e FastAPI:\n    \"\"\"Create FastAPI app with config from environment.\"\"\"\n    import os\n    \n    db_path = os.environ.get(\"MCF_DB_PATH\", \"data/mcf_jobs.db\")\n    index_dir = os.environ.get(\"MCF_INDEX_DIR\", \"data/embeddings\")\n    cors_origins_str = os.environ.get(\"MCF_CORS_ORIGINS\", \"http://localhost:3000\")\n    cors_origins = [o.strip() for o in cors_origins_str.split(\",\")]\n    \n    app = FastAPI(\n        title=\"MCF Semantic Search API\",\n        description=\"Semantic job search with hybrid BM25 + vector ranking\",\n        version=\"1.0.0\",\n        lifespan=lifespan\n    )\n    \n    app.state.db_path = db_path\n    app.state.index_dir = index_dir\n    \n    # ... rest of middleware setup\n    \n    return app\n```\n\n## Testing\n\n```bash\n# Basic startup\nmcf api-serve\n\n# Check health endpoint\ncurl http://localhost:8000/health\n\n# Check docs\nopen http://localhost:8000/docs\n\n# Test search\ncurl -X POST http://localhost:8000/api/search \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"python developer\", \"limit\": 5}'\n\n# Development mode with auto-reload\nmcf api-serve --reload\n\n# Production mode\nmcf api-serve --workers 4 --host 0.0.0.0\n```\n\n## Graceful Shutdown\n\nThe uvicorn server handles SIGINT/SIGTERM for graceful shutdown. The lifespan handler in app.py cleans up resources.\n\n## Dependencies\n- Requires FastAPI app (MyCareersFuture-qwl.4.1)\n- Requires uvicorn (added in dependencies)","status":"closed","priority":1,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T18:38:10.357487+08:00","created_by":"David Ten","updated_at":"2026-02-06T08:30:05.11862+08:00","closed_at":"2026-02-06T08:30:05.11862+08:00","close_reason":"Added api-serve CLI command with host/port/reload/workers/cors options. Updated create_app() to read MCF_DB_PATH, MCF_INDEX_DIR, MCF_CORS_ORIGINS env vars as fallback defaults for uvicorn worker process compatibility.","dependencies":[{"issue_id":"MyCareersFuture-qwl.4.4","depends_on_id":"MyCareersFuture-qwl.4","type":"parent-child","created_at":"2026-02-05T18:38:10.358922+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-qwl.4.4","depends_on_id":"MyCareersFuture-qwl.4.1","type":"blocks","created_at":"2026-02-05T18:38:35.064973+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-qwl.5","title":"Phase 5: Skill and Company Features","description":"# Phase 5: Skill and Company Features\n\n## Purpose\nImplement the specialized search features: skill-based job search and company similarity. These go beyond basic semantic search to provide deeper insights into the job market.\n\n## Why This Phase Follows Phase 4\nThe API endpoints for these features are defined in Phase 4, but require specialized embedding generation (skill clustering, company multi-centroids) that builds on the core infrastructure. This phase implements the backend logic those endpoints call.\n\n## Key Features\n\n### 5.1 Skill Clustering and Search\n**User Story**: As a job seeker, I want to search by skills and find jobs requiring related skills, so I can discover opportunities I might have missed.\n\n**How It Works**:\n1. Extract all unique skills from job postings\n2. Generate embeddings for each skill\n3. Cluster skills by embedding similarity (e.g., \"Python\", \"Pandas\", \"NumPy\" cluster together)\n4. At search time: skill query → find related skills → find jobs with those skills\n\n**Example**:\n- User searches: \"Python\"\n- System expands to: Python, Pandas, NumPy, Scikit-learn, Jupyter\n- Returns jobs requiring any of these skills, ranked by relevance\n\n### 5.2 Company Similarity (Multi-Centroid)\n**User Story**: As a job seeker, I want to find companies similar to ones I like, so I can discover new opportunities that match my preferences.\n\n**How It Works**:\n1. For each company, collect all their job embeddings\n2. Cluster into k centroids (job families) using k-means\n3. At search time: query company → get centroids → find companies with similar centroids\n\n**Example**:\n- User queries: \"Google\"\n- Google has 3 centroids: ML/AI jobs, DevOps jobs, Product jobs\n- System finds companies with similar centroid profiles\n- Returns: Microsoft, Meta, Amazon (tech giants with similar job mix)\n\n## Technical Components\n\n### Skill Clustering Pipeline\n```python\ndef cluster_skills(db: MCFDatabase, generator: EmbeddingGenerator) -\u003e dict:\n    \"\"\"\n    Build skill clusters for query expansion.\n    \n    Returns:\n        skill_clusters: Dict[cluster_id, list[skill_names]]\n        skill_to_cluster: Dict[skill_name, cluster_id]\n    \"\"\"\n    # Extract unique skills\n    skills = db.get_all_unique_skills()  # Parse comma-separated skill fields\n    \n    # Generate embeddings\n    skill_embeddings = generator.generate_skill_embeddings_batch(skills)\n    \n    # Cluster\n    from sklearn.cluster import AgglomerativeClustering\n    \n    n_clusters = min(100, len(skills) // 5)  # ~5 skills per cluster\n    clustering = AgglomerativeClustering(\n        n_clusters=n_clusters,\n        metric='cosine',\n        linkage='average'\n    )\n    labels = clustering.fit_predict(skill_embeddings)\n    \n    # Build mappings\n    clusters = defaultdict(list)\n    skill_to_cluster = {}\n    \n    for skill, label in zip(skills, labels):\n        clusters[label].append(skill)\n        skill_to_cluster[skill] = label\n    \n    return {\n        'skill_clusters': dict(clusters),\n        'skill_to_cluster': skill_to_cluster\n    }\n```\n\n### Company Multi-Centroid Pipeline\n```python\ndef build_company_centroids(\n    db: MCFDatabase, \n    k: int = 3,\n    min_jobs: int = 10\n) -\u003e dict[str, list[np.ndarray]]:\n    \"\"\"\n    Build multi-centroid representation for each company.\n    \n    Args:\n        db: Database instance\n        k: Number of centroids for large companies\n        min_jobs: Minimum jobs to qualify for multi-centroid\n    \n    Returns:\n        Dict mapping company_name -\u003e list of centroid vectors\n    \"\"\"\n    company_centroids = {}\n    \n    for company_name in db.get_all_companies():\n        # Get job embeddings for this company\n        job_uuids = db.get_jobs_by_company(company_name)\n        embeddings = db.get_embeddings_for_uuids(job_uuids)\n        \n        if len(embeddings) \u003c min_jobs:\n            # Too few jobs - use weighted single centroid\n            weights = compute_recency_weights(job_uuids, db)\n            centroid = np.average(embeddings, axis=0, weights=weights)\n            company_centroids[company_name] = [centroid]\n        else:\n            # Enough jobs - use k-means\n            actual_k = min(k, len(embeddings) // 3)\n            kmeans = KMeans(n_clusters=actual_k, random_state=42)\n            kmeans.fit(embeddings)\n            company_centroids[company_name] = list(kmeans.cluster_centers_)\n    \n    return company_centroids\n```\n\n### Skill Search Implementation\n```python\nasync def search_by_skill(self, request: SkillSearchRequest) -\u003e SearchResponse:\n    \"\"\"\n    Search jobs by skill similarity.\n    \n    Process:\n    1. Find similar skills using skill embeddings\n    2. Expand to include clustered skills\n    3. Find jobs with any of these skills\n    4. Rank by semantic similarity to skill\n    \"\"\"\n    # Get skill embedding\n    skill_embedding = self.generator.encode(request.skill)\n    \n    # Find similar skills from index\n    similar_skills = self.index_manager.search_skills(skill_embedding, k=10)\n    skill_names = [name for name, score in similar_skills]\n    \n    # Add cluster members\n    if request.skill in self.skill_to_cluster:\n        cluster_id = self.skill_to_cluster[request.skill]\n        cluster_skills = self.skill_clusters[cluster_id]\n        skill_names.extend(cluster_skills)\n    \n    skill_names = list(set(skill_names))  # Deduplicate\n    \n    # Find jobs with these skills\n    candidate_uuids = set()\n    for skill in skill_names:\n        jobs = self.db.search_jobs(keyword=skill, limit=1000)\n        candidate_uuids.update(job['uuid'] for job in jobs)\n    \n    # Apply additional filters\n    if request.salary_min or request.salary_max or request.employment_type:\n        filtered = self.db.search_jobs(\n            salary_min=request.salary_min,\n            salary_max=request.salary_max,\n            employment_type=request.employment_type,\n            limit=100000\n        )\n        candidate_uuids \u0026= set(job['uuid'] for job in filtered)\n    \n    # Rank by semantic similarity to skill\n    results = self.index_manager.search_jobs_filtered(\n        skill_embedding, candidate_uuids, k=request.limit\n    )\n    \n    return SearchResponse(\n        results=self._enrich_results(results),\n        total_candidates=len(candidate_uuids),\n        query_expansion=skill_names[:10]  # Show expanded skills\n    )\n```\n\n## Files to Create/Modify\n- Extend `src/mcf/embeddings/generator.py`: Skill clustering methods\n- Extend `src/mcf/embeddings/search_engine.py`: Skill search implementation\n- Extend `src/mcf/database.py`: Skill extraction queries\n\n## Exit Criteria\n- [ ] Skill clustering produces sensible groupings\n- [ ] Skill search returns relevant jobs\n- [ ] Company multi-centroids capture job diversity\n- [ ] Company similarity finds genuinely similar companies\n- [ ] Integration with API endpoints works correctly","status":"closed","priority":2,"issue_type":"epic","owner":"davidten7@gmail.com","created_at":"2026-02-05T18:40:31.709979+08:00","created_by":"David Ten","updated_at":"2026-02-06T15:33:27.727543+08:00","closed_at":"2026-02-06T15:33:27.727543+08:00","close_reason":"All children complete: skill search endpoint (5.1) and company multi-centroid embeddings (5.2) both implemented","dependencies":[{"issue_id":"MyCareersFuture-qwl.5","depends_on_id":"MyCareersFuture-qwl","type":"parent-child","created_at":"2026-02-05T18:40:31.710959+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-qwl.5","depends_on_id":"MyCareersFuture-qwl.4","type":"blocks","created_at":"2026-02-05T18:45:26.175409+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-qwl.5.1","title":"Add skill search endpoint and skill cloud API","description":"# Task: Add Skill Search Endpoint and Skill Cloud API\n\n## What\nAdd advanced skill-based search features to the API. Basic skill extraction and clustering is now handled in Phase 1 (EmbeddingGenerator). This task adds:\n\n1. `/api/search/skills` endpoint with query expansion\n2. `/api/skills/cloud` endpoint for skill visualization\n3. `/api/skills/related` endpoint for skill exploration\n\n**Note:** Basic skill clustering is done during `embed-generate` (Phase 1.3). This task uses that data.\n\n## New API Endpoints\n\n### 1. Skill Search Endpoint (enhanced)\n```python\n@app.post(\"/api/search/skills\", response_model=SearchResponse)\nasync def skill_search(\n    request: SkillSearchRequest,\n    engine: SemanticSearchEngine = Depends(get_engine)\n) -\u003e SearchResponse:\n    \"\"\"\n    Search jobs by skill with automatic query expansion.\n    \n    Different from semantic search:\n    - Uses skill embedding directly (not query embedding)\n    - Expands to related skills from same cluster\n    - Shows which skills matched\n    \"\"\"\n    # Load skill clusters\n    skill_embedding = engine.get_skill_embedding(request.skill)\n    if skill_embedding is None:\n        raise HTTPException(404, f\"Unknown skill: {request.skill}\")\n    \n    # Get related skills from cluster\n    related_skills = engine.query_expander.get_related_skills(request.skill, k=5)\n    \n    # Search using skill embedding\n    results = engine.index_manager.search_jobs(skill_embedding, k=request.limit * 2)\n    \n    # Filter by SQL constraints if any\n    if request.salary_min or request.salary_max or request.employment_type:\n        results = engine._filter_results(results, request)\n    \n    response = engine._build_response(results[:request.limit])\n    response.skill_expansion = related_skills\n    \n    return response\n```\n\n### 2. Skill Cloud Endpoint (NEW)\n```python\n@app.get(\"/api/skills/cloud\")\nasync def skill_cloud(\n    min_jobs: int = 10,\n    limit: int = 100,\n    engine: SemanticSearchEngine = Depends(get_engine)\n) -\u003e list[SkillCloudItem]:\n    \"\"\"\n    Get skill frequency data for visualization.\n    \n    Returns skills with job counts for word cloud or bar chart.\n    \"\"\"\n    skills = engine.db.get_skill_frequencies(min_jobs=min_jobs, limit=limit)\n    \n    return [\n        SkillCloudItem(\n            skill=skill,\n            job_count=count,\n            cluster_id=engine.query_expander.skill_to_cluster.get(skill)\n        )\n        for skill, count in skills\n    ]\n\nclass SkillCloudItem(BaseModel):\n    skill: str\n    job_count: int\n    cluster_id: Optional[int] = Field(\n        None, description=\"Cluster ID for color coding\"\n    )\n```\n\n### 3. Related Skills Endpoint (NEW)\n```python\n@app.get(\"/api/skills/related/{skill}\")\nasync def related_skills(\n    skill: str,\n    k: int = 10,\n    engine: SemanticSearchEngine = Depends(get_engine)\n) -\u003e list[RelatedSkill]:\n    \"\"\"\n    Get skills related to a given skill.\n    \n    Uses embedding similarity within and across clusters.\n    \"\"\"\n    skill_embedding = engine.get_skill_embedding(skill)\n    if skill_embedding is None:\n        raise HTTPException(404, f\"Unknown skill: {skill}\")\n    \n    # Search skill index\n    similar = engine.index_manager.search_skills(skill_embedding, k=k+1)\n    \n    return [\n        RelatedSkill(\n            skill=s,\n            similarity=score,\n            same_cluster=(\n                engine.query_expander.skill_to_cluster.get(s) ==\n                engine.query_expander.skill_to_cluster.get(skill)\n            )\n        )\n        for s, score in similar if s != skill\n    ][:k]\n\nclass RelatedSkill(BaseModel):\n    skill: str\n    similarity: float\n    same_cluster: bool = Field(\n        description=\"True if in same cluster (strong synonym)\"\n    )\n```\n\n## Database Method: Skill Frequencies\n```python\n# Add to src/mcf/database.py\n\ndef get_skill_frequencies(self, min_jobs: int = 1, \n                         limit: int = 100) -\u003e list[tuple[str, int]]:\n    \"\"\"\n    Get skill frequencies for visualization.\n    \n    Returns list of (skill_name, job_count) tuples.\n    \"\"\"\n    with self._connection() as conn:\n        # Get all skills columns\n        rows = conn.execute(\n            \"SELECT skills FROM jobs WHERE skills IS NOT NULL AND skills != ''\"\n        ).fetchall()\n    \n    # Count skill occurrences\n    from collections import Counter\n    skill_counts = Counter()\n    for row in rows:\n        skills = [s.strip() for s in row[0].split(',')]\n        skill_counts.update(s for s in skills if s)\n    \n    # Filter and sort\n    filtered = [(skill, count) for skill, count in skill_counts.items()\n                if count \u003e= min_jobs]\n    filtered.sort(key=lambda x: x[1], reverse=True)\n    \n    return filtered[:limit]\n```\n\n## Pydantic Models\n```python\n# Add to src/api/models.py\n\nclass SkillSearchRequest(BaseModel):\n    \"\"\"Request for skill-based job search.\"\"\"\n    skill: str = Field(..., min_length=1, max_length=100)\n    limit: int = Field(20, ge=1, le=100)\n    include_related: bool = Field(True, \n        description=\"Include jobs matching related skills\")\n    \n    # Optional filters\n    salary_min: Optional[int] = None\n    salary_max: Optional[int] = None\n    employment_type: Optional[str] = None\n\nclass SkillCloudItem(BaseModel):\n    \"\"\"Item in skill cloud response.\"\"\"\n    skill: str\n    job_count: int\n    cluster_id: Optional[int] = None\n\nclass RelatedSkill(BaseModel):\n    \"\"\"A related skill.\"\"\"\n    skill: str\n    similarity: float\n    same_cluster: bool\n```\n\n## Testing\n\n```python\ndef test_skill_search():\n    client = TestClient(app)\n    \n    response = client.post(\"/api/search/skills\", json={\n        \"skill\": \"Python\",\n        \"limit\": 10\n    })\n    \n    assert response.status_code == 200\n    data = response.json()\n    assert \"results\" in data\n    assert \"skill_expansion\" in data\n\ndef test_skill_cloud():\n    client = TestClient(app)\n    \n    response = client.get(\"/api/skills/cloud?min_jobs=100\u0026limit=50\")\n    \n    assert response.status_code == 200\n    data = response.json()\n    assert len(data) \u003c= 50\n    assert all(\"skill\" in item and \"job_count\" in item for item in data)\n\ndef test_related_skills():\n    client = TestClient(app)\n    \n    response = client.get(\"/api/skills/related/Python?k=5\")\n    \n    assert response.status_code == 200\n    data = response.json()\n    assert len(data) \u003c= 5\n```\n\n## Dependencies\n- Requires skill cluster data from Phase 1.3 (embed-generate)\n- Requires SemanticSearchEngine (Phase 3)\n- Requires FastAPI app (Phase 4)","design":"## Implementation Plan\n\nTwo new GET endpoints wiring existing infrastructure:\n\n### New Files/Changes\n1. **models.py**: 4 Pydantic models (SkillCloudItem, SkillCloudResponse, RelatedSkill, RelatedSkillsResponse)\n2. **search_engine.py**: 2 methods (get_skill_cloud, get_related_skills) \n3. **app.py**: 2 route handlers (GET /api/skills/cloud, GET /api/skills/related/{skill})\n4. **tests**: Endpoint + model validation tests\n\n### Key Decisions\n- Engine methods return plain dicts (like get_stats pattern), no internal dataclasses needed\n- Related skills uses FAISS embedding similarity (primary) with cluster annotation, falls back to cluster-only\n- SkillCloudResponse wraps list with total_unique_skills metadata","status":"closed","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T18:42:06.265297+08:00","created_by":"David Ten","updated_at":"2026-02-06T10:44:53.07076+08:00","closed_at":"2026-02-06T10:44:53.07076+08:00","close_reason":"Added GET /api/skills/cloud and GET /api/skills/related/{skill} endpoints. 4 Pydantic models, 2 engine methods, 2 route handlers, full test coverage (10 new tests, 290 total pass).","dependencies":[{"issue_id":"MyCareersFuture-qwl.5.1","depends_on_id":"MyCareersFuture-qwl.5","type":"parent-child","created_at":"2026-02-05T18:42:06.266742+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-qwl.5.2","title":"Implement company multi-centroid embeddings","description":"# Task: Implement Company Multi-Centroid Embeddings\n\n## What\nCreate multi-centroid representations for companies to capture their diverse job families, enabling meaningful company-to-company similarity search.\n\n## Problem Statement\nA simple average of all job embeddings loses information:\n- Google posts ML jobs, DevOps jobs, and Sales jobs\n- Averaging these produces a \"blurry\" embedding that doesn't represent any category well\n- Similar average for Microsoft might not match well, even though both have similar job mixes\n\n## Solution: Multi-Centroid Representation\nUse k-means to find k centroids (job families) per company:\n- Google: [ML centroid, DevOps centroid, Sales centroid]\n- Microsoft: [ML centroid, Cloud centroid, Product centroid]\n- Similarity = max(sim(Google_i, Microsoft_j)) for all centroid pairs\n\n## Implementation\n\n### Generator Method\n```python\n# Add to src/mcf/embeddings/generator.py\n\ndef generate_company_centroids(\n    self,\n    db: MCFDatabase,\n    k: int = 3,\n    min_jobs_for_multi: int = 10,\n    max_jobs_per_company: int = 1000\n) -\u003e dict[str, list[np.ndarray]]:\n    \"\"\"\n    Generate multi-centroid embeddings for companies.\n    \n    For each company:\n    - If \u003c min_jobs_for_multi jobs: single weighted centroid\n    - Otherwise: k-means clustering to find k job families\n    \n    Args:\n        db: Database instance\n        k: Target number of centroids for large companies\n        min_jobs_for_multi: Minimum jobs for multi-centroid\n        max_jobs_per_company: Limit jobs to consider (most recent)\n        \n    Returns:\n        Dict mapping company_name -\u003e list of centroid arrays\n    \"\"\"\n    from sklearn.cluster import KMeans\n    \n    company_centroids = {}\n    companies = db.get_all_companies()\n    \n    logger.info(f\"Generating centroids for {len(companies)} companies...\")\n    \n    for company_name in tqdm(companies, desc=\"Companies\"):\n        # Get job UUIDs for this company (most recent first)\n        jobs = db.search_jobs(\n            company_name=company_name,\n            limit=max_jobs_per_company\n        )\n        \n        if not jobs:\n            continue\n        \n        uuids = [job['uuid'] for job in jobs]\n        \n        # Get embeddings\n        embeddings_dict = db.get_embeddings_for_uuids(uuids)\n        embeddings = np.array([\n            embeddings_dict[uuid] for uuid in uuids\n            if uuid in embeddings_dict\n        ])\n        \n        if len(embeddings) == 0:\n            continue\n        \n        if len(embeddings) \u003c min_jobs_for_multi:\n            # Single weighted centroid with recency weighting\n            weights = self._compute_recency_weights(jobs)\n            centroid = np.average(embeddings, axis=0, weights=weights[:len(embeddings)])\n            company_centroids[company_name] = [centroid]\n        else:\n            # K-means for multiple centroids\n            actual_k = min(k, len(embeddings) // 3)  # At least 3 jobs per centroid\n            actual_k = max(actual_k, 1)\n            \n            kmeans = KMeans(n_clusters=actual_k, random_state=42, n_init=10)\n            kmeans.fit(embeddings)\n            \n            company_centroids[company_name] = list(kmeans.cluster_centers_)\n    \n    return company_centroids\n\ndef _compute_recency_weights(self, jobs: list[dict], decay_days: int = 365) -\u003e np.ndarray:\n    \"\"\"\n    Compute recency weights for jobs.\n    \n    More recent jobs get higher weights using exponential decay.\n    \n    Args:\n        jobs: List of job dicts with 'posted_date'\n        decay_days: Days for weight to decay to 1/e\n        \n    Returns:\n        Array of weights (sum to 1)\n    \"\"\"\n    from datetime import datetime\n    \n    today = datetime.now().date()\n    weights = []\n    \n    for job in jobs:\n        if job.get('posted_date'):\n            try:\n                if isinstance(job['posted_date'], str):\n                    posted = datetime.fromisoformat(job['posted_date']).date()\n                else:\n                    posted = job['posted_date']\n                days_old = (today - posted).days\n                weight = np.exp(-days_old / decay_days)\n            except:\n                weight = 0.5  # Default for parse errors\n        else:\n            weight = 0.5  # Default for missing dates\n        \n        weights.append(weight)\n    \n    weights = np.array(weights)\n    return weights / weights.sum()  # Normalize to sum to 1\n```\n\n### Index Manager Extension\n```python\n# Add to src/mcf/embeddings/index_manager.py\n\ndef build_company_index(\n    self,\n    company_centroids: dict[str, list[np.ndarray]]\n) -\u003e None:\n    \"\"\"\n    Build index for company multi-centroids.\n    \n    Stores all centroids in a flat index with mapping back to companies.\n    \"\"\"\n    # Flatten centroids\n    all_centroids = []\n    company_names = []\n    centroid_to_company = []  # Which company each centroid belongs to\n    company_centroid_ranges = {}  # company -\u003e (start_idx, end_idx)\n    \n    for company_name, centroids in company_centroids.items():\n        start_idx = len(all_centroids)\n        for centroid in centroids:\n            all_centroids.append(centroid)\n            centroid_to_company.append(company_name)\n        end_idx = len(all_centroids)\n        company_centroid_ranges[company_name] = (start_idx, end_idx)\n        company_names.append(company_name)\n    \n    # Build flat index (companies are much fewer than jobs)\n    embeddings = np.array(all_centroids).astype(np.float32)\n    index = faiss.IndexFlatIP(embeddings.shape[1])  # Inner product = cosine for normalized\n    faiss.normalize_L2(embeddings)  # Normalize for cosine similarity\n    index.add(embeddings)\n    \n    self.indexes['companies'] = index\n    self._company_names = company_names\n    self._centroid_to_company = centroid_to_company\n    self._company_centroid_ranges = company_centroid_ranges\n    self._all_company_centroids = embeddings\n\ndef search_companies(\n    self,\n    query_vector: np.ndarray,\n    k: int = 10\n) -\u003e list[tuple[str, float]]:\n    \"\"\"\n    Find similar companies to a query vector.\n    \n    For each company, score is max similarity across their centroids.\n    \"\"\"\n    # Normalize query\n    query = query_vector.copy()\n    faiss.normalize_L2(query.reshape(1, -1))\n    \n    # Search all centroids\n    n_centroids = len(self._centroid_to_company)\n    similarities, indices = self.indexes['companies'].search(\n        query.reshape(1, -1), n_centroids\n    )\n    \n    # Group by company, take max\n    company_max_scores = {}\n    for sim, idx in zip(similarities[0], indices[0]):\n        if idx \u003e= 0:\n            company = self._centroid_to_company[idx]\n            if company not in company_max_scores:\n                company_max_scores[company] = float(sim)\n            else:\n                company_max_scores[company] = max(company_max_scores[company], float(sim))\n    \n    # Sort and return top k\n    sorted_companies = sorted(\n        company_max_scores.items(),\n        key=lambda x: x[1],\n        reverse=True\n    )[:k]\n    \n    return sorted_companies\n\ndef get_company_centroids(self, company_name: str) -\u003e Optional[np.ndarray]:\n    \"\"\"Get centroids for a specific company.\"\"\"\n    if company_name not in self._company_centroid_ranges:\n        return None\n    \n    start, end = self._company_centroid_ranges[company_name]\n    return self._all_company_centroids[start:end]\n```\n\n### Search Engine Integration\n```python\n# Add to src/mcf/embeddings/search_engine.py\n\nasync def find_similar_companies(\n    self,\n    request: CompanySimilarityRequest\n) -\u003e list[CompanySimilarity]:\n    \"\"\"\n    Find companies with similar job profiles.\n    \"\"\"\n    # Get source company centroids\n    source_centroids = self.index_manager.get_company_centroids(request.company_name)\n    \n    if source_centroids is None:\n        raise ValueError(f\"No embeddings found for company: {request.company_name}\")\n    \n    # Search using each centroid, aggregate scores\n    company_scores = {}\n    \n    for centroid in source_centroids:\n        results = self.index_manager.search_companies(centroid, k=request.limit * 3)\n        \n        for company, score in results:\n            if company == request.company_name:\n                continue  # Skip self\n            \n            if company not in company_scores:\n                company_scores[company] = score\n            else:\n                company_scores[company] = max(company_scores[company], score)\n    \n    # Sort by score\n    sorted_companies = sorted(\n        company_scores.items(),\n        key=lambda x: x[1],\n        reverse=True\n    )[:request.limit]\n    \n    # Enrich with stats\n    results = []\n    for company_name, score in sorted_companies:\n        stats = self.db.get_company_stats(company_name)\n        results.append(CompanySimilarity(\n            company_name=company_name,\n            similarity_score=score,\n            job_count=stats.get('job_count', 0),\n            avg_salary=stats.get('avg_salary'),\n            top_skills=stats.get('top_skills', [])[:5]\n        ))\n    \n    return results\n```\n\n### Database Helper\n```python\n# Add to src/mcf/database.py\n\ndef get_all_companies(self) -\u003e list[str]:\n    \"\"\"Get list of all company names.\"\"\"\n    with self._connection() as conn:\n        rows = conn.execute(\n            \"\"\"\n            SELECT DISTINCT company_name \n            FROM jobs \n            WHERE company_name IS NOT NULL AND company_name != ''\n            ORDER BY company_name\n            \"\"\"\n        ).fetchall()\n    return [row[0] for row in rows]\n\ndef get_company_stats(self, company_name: str) -\u003e dict:\n    \"\"\"Get statistics for a company.\"\"\"\n    with self._connection() as conn:\n        row = conn.execute(\n            \"\"\"\n            SELECT \n                COUNT(*) as job_count,\n                AVG(salary_min) as avg_salary_min,\n                AVG(salary_max) as avg_salary_max\n            FROM jobs\n            WHERE company_name = ?\n            \"\"\",\n            (company_name,)\n        ).fetchone()\n        \n        # Get top skills\n        skills_row = conn.execute(\n            \"\"\"\n            SELECT skills FROM jobs\n            WHERE company_name = ? AND skills IS NOT NULL\n            LIMIT 100\n            \"\"\",\n            (company_name,)\n        ).fetchall()\n    \n    # Aggregate skills\n    from collections import Counter\n    skill_counter = Counter()\n    for row in skills_row:\n        for skill in row[0].split(','):\n            skill = skill.strip()\n            if skill:\n                skill_counter[skill] += 1\n    \n    top_skills = [s for s, _ in skill_counter.most_common(10)]\n    \n    return {\n        'job_count': row[0] or 0,\n        'avg_salary': int((row[1] or 0 + row[2] or 0) / 2) if row[1] else None,\n        'top_skills': top_skills\n    }\n```\n\n## Testing\n\n```python\ndef test_company_centroids():\n    generator = EmbeddingGenerator()\n    db = MCFDatabase(\"test.db\")\n    \n    centroids = generator.generate_company_centroids(db, k=3)\n    \n    # Should have entries for companies\n    assert len(centroids) \u003e 0\n    \n    # Each entry should have 1-3 centroids\n    for company, centroid_list in centroids.items():\n        assert 1 \u003c= len(centroid_list) \u003c= 3\n        for centroid in centroid_list:\n            assert centroid.shape == (384,)\n\ndef test_company_similarity():\n    engine = SemanticSearchEngine(\"test.db\", Path(\"test_embeddings\"))\n    await engine.load()\n    \n    results = await engine.find_similar_companies(\n        CompanySimilarityRequest(company_name=\"Google\", limit=5)\n    )\n    \n    assert len(results) \u003c= 5\n    assert all(r.company_name != \"Google\" for r in results)\n    assert all(0 \u003c= r.similarity_score \u003c= 1 for r in results)\n```\n\n## Dependencies\n- Requires job embeddings (Phase 1)\n- Requires FAISS index manager (Phase 2)\n- Used by company similarity API endpoint (Phase 4)","status":"closed","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T18:43:16.531463+08:00","created_by":"David Ten","updated_at":"2026-02-06T14:29:10.199113+08:00","closed_at":"2026-02-06T14:29:10.199113+08:00","close_reason":"Already fully implemented in commit 2e96bb0. All 22 tests pass. Multi-centroid company embeddings with k-means clustering, FAISS index, search engine integration, API endpoint, and CLI wiring are complete.","dependencies":[{"issue_id":"MyCareersFuture-qwl.5.2","depends_on_id":"MyCareersFuture-qwl.5","type":"parent-child","created_at":"2026-02-05T18:43:16.532739+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-qwl.5.2","depends_on_id":"MyCareersFuture-qwl.5.1","type":"blocks","created_at":"2026-02-05T18:45:26.375663+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-qwl.6","title":"Phase 6: React Frontend","description":"# Phase 6: React Frontend\n\n## Purpose\nBuild an interactive web interface for the semantic search system. Users will be able to search for jobs, apply filters, view results, and discover similar jobs and companies - all through a modern, responsive UI.\n\n## Why React + TypeScript?\n- **React**: Most popular frontend framework, large ecosystem, component-based\n- **TypeScript**: Type safety catches bugs early, better IDE support, self-documenting\n- **Vite**: Fast build tool, instant HMR, better than Create React App\n\n## Key Features\n\n### 1. Search Interface\n- Search bar with query input\n- Filter panel (collapsible on mobile)\n- Results grid/list with job cards\n- Pagination or infinite scroll\n\n### 2. Job Card Component\n- Title, company, salary range\n- Skills tags\n- Posted date badge\n- Similarity score indicator\n- \"Find Similar\" button\n\n### 3. Filter Panel\n- Salary range slider\n- Employment type checkboxes\n- Region/location dropdown\n- Company search\n- Clear filters button\n\n### 4. Similar Jobs\n- \"Find Similar\" opens modal or sidebar\n- Shows top 5-10 similar jobs\n- Option to exclude same company\n\n### 5. Company Similarity\n- Company search input\n- Shows similar companies with stats\n- Job count, avg salary, top skills per company\n\n## Technical Architecture\n\n### Project Structure\n```\nsrc/frontend/\n├── public/\n│   └── index.html\n├── src/\n│   ├── components/\n│   │   ├── SearchBar.tsx\n│   │   ├── FilterPanel.tsx\n│   │   ├── JobCard.tsx\n│   │   ├── JobList.tsx\n│   │   ├── SimilarJobsModal.tsx\n│   │   ├── CompanySearch.tsx\n│   │   ├── LoadingSpinner.tsx\n│   │   └── DegradedBanner.tsx\n│   ├── hooks/\n│   │   ├── useSearch.ts\n│   │   ├── useSimilarJobs.ts\n│   │   └── useFilters.ts\n│   ├── services/\n│   │   └── api.ts\n│   ├── types/\n│   │   └── index.ts\n│   ├── App.tsx\n│   ├── main.tsx\n│   └── index.css\n├── package.json\n├── tsconfig.json\n├── vite.config.ts\n└── README.md\n```\n\n### Tech Stack\n- **React 18**: Latest features (concurrent rendering, suspense)\n- **TypeScript 5**: Strict type checking\n- **Vite**: Build tool\n- **Tailwind CSS**: Utility-first styling\n- **React Query (TanStack Query)**: Server state management\n- **React Router**: Client-side routing\n- **Axios**: HTTP client\n\n## Component Specifications\n\n### SearchBar\n```tsx\ninterface SearchBarProps {\n  value: string;\n  onChange: (query: string) =\u003e void;\n  onSearch: () =\u003e void;\n  placeholder?: string;\n}\n\nconst SearchBar: React.FC\u003cSearchBarProps\u003e = (props) =\u003e {\n  // Text input with search icon\n  // Enter key triggers search\n  // Clear button when has value\n};\n```\n\n### FilterPanel\n```tsx\ninterface Filters {\n  salaryMin?: number;\n  salaryMax?: number;\n  employmentType?: string[];\n  region?: string;\n  company?: string;\n}\n\ninterface FilterPanelProps {\n  filters: Filters;\n  onChange: (filters: Filters) =\u003e void;\n  onClear: () =\u003e void;\n  collapsed?: boolean;\n  onToggleCollapse?: () =\u003e void;\n}\n```\n\n### JobCard\n```tsx\ninterface JobCardProps {\n  job: JobResult;\n  onFindSimilar: (uuid: string) =\u003e void;\n}\n\nconst JobCard: React.FC\u003cJobCardProps\u003e = ({ job, onFindSimilar }) =\u003e {\n  // Card layout with:\n  // - Title (clickable to job URL)\n  // - Company name\n  // - Salary range\n  // - Skills tags (first 5)\n  // - Posted date\n  // - Similarity score badge\n  // - Find Similar button\n};\n```\n\n## API Integration\n\n### api.ts\n```typescript\nimport axios from 'axios';\n\nconst API_BASE = import.meta.env.VITE_API_URL || 'http://localhost:8000';\n\nexport interface SearchRequest {\n  query: string;\n  limit?: number;\n  salary_min?: number;\n  salary_max?: number;\n  employment_type?: string;\n  alpha?: number;\n  expand_query?: boolean;\n}\n\nexport interface SearchResponse {\n  results: JobResult[];\n  total_candidates: number;\n  search_time_ms: number;\n  query_expansion?: string[];\n  degraded: boolean;\n  cache_hit: boolean;\n}\n\nexport const api = {\n  search: async (request: SearchRequest): Promise\u003cSearchResponse\u003e =\u003e {\n    const response = await axios.post(`${API_BASE}/api/search`, request);\n    return response.data;\n  },\n  \n  findSimilar: async (jobUuid: string, limit = 10): Promise\u003cSearchResponse\u003e =\u003e {\n    const response = await axios.post(`${API_BASE}/api/similar`, {\n      job_uuid: jobUuid,\n      limit,\n      exclude_same_company: false\n    });\n    return response.data;\n  },\n  \n  searchSkills: async (skill: string, limit = 20): Promise\u003cSearchResponse\u003e =\u003e {\n    const response = await axios.post(`${API_BASE}/api/search/skills`, {\n      skill,\n      limit\n    });\n    return response.data;\n  },\n  \n  findSimilarCompanies: async (companyName: string, limit = 10) =\u003e {\n    const response = await axios.post(`${API_BASE}/api/companies/similar`, {\n      company_name: companyName,\n      limit\n    });\n    return response.data;\n  },\n  \n  getHealth: async () =\u003e {\n    const response = await axios.get(`${API_BASE}/health`);\n    return response.data;\n  }\n};\n```\n\n### React Query Hooks\n```typescript\n// hooks/useSearch.ts\nimport { useQuery, useMutation } from '@tanstack/react-query';\nimport { api, SearchRequest } from '../services/api';\n\nexport function useSearch() {\n  const mutation = useMutation({\n    mutationFn: (request: SearchRequest) =\u003e api.search(request),\n  });\n  \n  return {\n    search: mutation.mutate,\n    data: mutation.data,\n    isLoading: mutation.isPending,\n    error: mutation.error\n  };\n}\n\n// hooks/useSimilarJobs.ts\nexport function useSimilarJobs(jobUuid: string | null) {\n  return useQuery({\n    queryKey: ['similar', jobUuid],\n    queryFn: () =\u003e api.findSimilar(jobUuid!),\n    enabled: !!jobUuid\n  });\n}\n```\n\n## UI/UX Considerations\n\n### Responsive Design\n- Desktop: Side-by-side filter panel and results\n- Tablet: Collapsible filter panel\n- Mobile: Stacked layout, full-width cards\n\n### Loading States\n- Skeleton loaders for job cards while loading\n- Search button disabled during search\n- \"Searching...\" text feedback\n\n### Error Handling\n- Toast notifications for API errors\n- Retry button for failed requests\n- Graceful degradation banner when API is degraded\n\n### Performance\n- Debounce filter changes (300ms)\n- Virtual scrolling for large result sets\n- Memoize expensive computations\n\n## Setup Instructions\n\n```bash\n# Create project\ncd src\nnpm create vite@latest frontend -- --template react-ts\n\n# Install dependencies\ncd frontend\nnpm install\nnpm install axios @tanstack/react-query react-router-dom\nnpm install -D tailwindcss postcss autoprefixer\nnpx tailwindcss init -p\n\n# Configure Tailwind (tailwind.config.js)\n# content: ['./index.html', './src/**/*.{js,ts,jsx,tsx}']\n\n# Start development\nnpm run dev\n```\n\n## Files to Create\n- `src/frontend/` - New React application\n- See project structure above for file list\n\n## Exit Criteria\n- [ ] Search UI works end-to-end\n- [ ] Filter panel applies filters correctly\n- [ ] Job cards display all relevant info\n- [ ] Similar jobs modal works\n- [ ] Company search works\n- [ ] Responsive on mobile/tablet/desktop\n- [ ] Handles loading and error states\n- [ ] Degraded mode banner shows when API is degraded","status":"closed","priority":2,"issue_type":"epic","owner":"davidten7@gmail.com","created_at":"2026-02-05T18:43:52.549383+08:00","created_by":"David Ten","updated_at":"2026-02-06T15:33:28.398402+08:00","closed_at":"2026-02-06T15:33:28.398402+08:00","close_reason":"All children complete: React project initialized (6.1) and core UI components built (6.2)","dependencies":[{"issue_id":"MyCareersFuture-qwl.6","depends_on_id":"MyCareersFuture-qwl","type":"parent-child","created_at":"2026-02-05T18:43:52.550259+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-qwl.6","depends_on_id":"MyCareersFuture-qwl.4","type":"blocks","created_at":"2026-02-05T18:45:26.571732+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-qwl.6.1","title":"Initialize React project with Vite and dependencies","description":"# Task: Initialize React Project\n\n## What\nSet up the React frontend project with Vite, TypeScript, and all required dependencies.\n\n## Steps\n\n### 1. Create Vite Project\n```bash\ncd src\nnpm create vite@latest frontend -- --template react-ts\ncd frontend\n```\n\n### 2. Install Dependencies\n```bash\n# Core dependencies\nnpm install axios @tanstack/react-query react-router-dom\n\n# UI dependencies\nnpm install @headlessui/react @heroicons/react\n\n# Development dependencies\nnpm install -D tailwindcss postcss autoprefixer\nnpm install -D @types/node\n```\n\n### 3. Configure Tailwind CSS\n```bash\nnpx tailwindcss init -p\n```\n\n**tailwind.config.js:**\n```javascript\n/** @type {import('tailwindcss').Config} */\nexport default {\n  content: [\n    \"./index.html\",\n    \"./src/**/*.{js,ts,jsx,tsx}\",\n  ],\n  theme: {\n    extend: {},\n  },\n  plugins: [],\n}\n```\n\n**src/index.css:**\n```css\n@tailwind base;\n@tailwind components;\n@tailwind utilities;\n```\n\n### 4. Configure Vite for API Proxy\n**vite.config.ts:**\n```typescript\nimport { defineConfig } from 'vite';\nimport react from '@vitejs/plugin-react';\n\nexport default defineConfig({\n  plugins: [react()],\n  server: {\n    port: 3000,\n    proxy: {\n      '/api': {\n        target: 'http://localhost:8000',\n        changeOrigin: true,\n      },\n    },\n  },\n});\n```\n\n### 5. Configure TypeScript\n**tsconfig.json:**\n```json\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2020\",\n    \"useDefineForClassFields\": true,\n    \"lib\": [\"ES2020\", \"DOM\", \"DOM.Iterable\"],\n    \"module\": \"ESNext\",\n    \"skipLibCheck\": true,\n    \"moduleResolution\": \"bundler\",\n    \"allowImportingTsExtensions\": true,\n    \"resolveJsonModule\": true,\n    \"isolatedModules\": true,\n    \"noEmit\": true,\n    \"jsx\": \"react-jsx\",\n    \"strict\": true,\n    \"noUnusedLocals\": true,\n    \"noUnusedParameters\": true,\n    \"noFallthroughCasesInSwitch\": true,\n    \"baseUrl\": \".\",\n    \"paths\": {\n      \"@/*\": [\"src/*\"]\n    }\n  },\n  \"include\": [\"src\"],\n  \"references\": [{ \"path\": \"./tsconfig.node.json\" }]\n}\n```\n\n### 6. Create Directory Structure\n```bash\nmkdir -p src/components src/hooks src/services src/types\n```\n\n### 7. Create Type Definitions\n**src/types/index.ts:**\n```typescript\nexport interface JobResult {\n  uuid: string;\n  title: string;\n  company_name: string | null;\n  description: string;\n  salary_min: number | null;\n  salary_max: number | null;\n  employment_type: string | null;\n  skills: string | null;\n  location: string | null;\n  posted_date: string | null;\n  job_url: string;\n  similarity_score: number;\n  bm25_score?: number;\n  freshness_score?: number;\n}\n\nexport interface SearchResponse {\n  results: JobResult[];\n  total_candidates: number;\n  search_time_ms: number;\n  query_expansion?: string[];\n  degraded: boolean;\n  cache_hit: boolean;\n}\n\nexport interface Filters {\n  salaryMin?: number;\n  salaryMax?: number;\n  employmentType?: string;\n  region?: string;\n  company?: string;\n}\n\nexport interface CompanySimilarity {\n  company_name: string;\n  similarity_score: number;\n  job_count: number;\n  avg_salary: number | null;\n  top_skills: string[];\n}\n\nexport interface HealthResponse {\n  status: 'healthy' | 'degraded';\n  index_loaded: boolean;\n  degraded: boolean;\n}\n```\n\n### 8. Create API Service\n**src/services/api.ts:**\n```typescript\nimport axios from 'axios';\nimport type { SearchResponse, CompanySimilarity, HealthResponse, Filters } from '../types';\n\nconst API_BASE = import.meta.env.VITE_API_URL || '';\n\nconst client = axios.create({\n  baseURL: API_BASE,\n  timeout: 10000,\n});\n\nexport interface SearchRequest {\n  query: string;\n  limit?: number;\n  salary_min?: number;\n  salary_max?: number;\n  employment_type?: string;\n  region?: string;\n  company?: string;\n  alpha?: number;\n  expand_query?: boolean;\n}\n\nexport const api = {\n  search: async (request: SearchRequest): Promise\u003cSearchResponse\u003e =\u003e {\n    const response = await client.post('/api/search', request);\n    return response.data;\n  },\n  \n  findSimilar: async (jobUuid: string, limit = 10, excludeSameCompany = false): Promise\u003cSearchResponse\u003e =\u003e {\n    const response = await client.post('/api/similar', {\n      job_uuid: jobUuid,\n      limit,\n      exclude_same_company: excludeSameCompany,\n    });\n    return response.data;\n  },\n  \n  searchSkills: async (skill: string, limit = 20, filters?: Filters): Promise\u003cSearchResponse\u003e =\u003e {\n    const response = await client.post('/api/search/skills', {\n      skill,\n      limit,\n      ...filters,\n    });\n    return response.data;\n  },\n  \n  findSimilarCompanies: async (companyName: string, limit = 10): Promise\u003cCompanySimilarity[]\u003e =\u003e {\n    const response = await client.post('/api/companies/similar', {\n      company_name: companyName,\n      limit,\n    });\n    return response.data;\n  },\n  \n  getHealth: async (): Promise\u003cHealthResponse\u003e =\u003e {\n    const response = await client.get('/health');\n    return response.data;\n  },\n};\n\nexport default api;\n```\n\n### 9. Set Up React Query Provider\n**src/main.tsx:**\n```typescript\nimport React from 'react';\nimport ReactDOM from 'react-dom/client';\nimport { QueryClient, QueryClientProvider } from '@tanstack/react-query';\nimport App from './App';\nimport './index.css';\n\nconst queryClient = new QueryClient({\n  defaultOptions: {\n    queries: {\n      refetchOnWindowFocus: false,\n      retry: 1,\n    },\n  },\n});\n\nReactDOM.createRoot(document.getElementById('root')!).render(\n  \u003cReact.StrictMode\u003e\n    \u003cQueryClientProvider client={queryClient}\u003e\n      \u003cApp /\u003e\n    \u003c/QueryClientProvider\u003e\n  \u003c/React.StrictMode\u003e\n);\n```\n\n## Verification\n```bash\n# Start development server\nnpm run dev\n\n# Should see Vite dev server at http://localhost:3000\n# API calls should proxy to http://localhost:8000\n```\n\n## Dependencies\n- None (first task in Phase 6)\n- API server should be running for full testing","status":"closed","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T18:44:27.860236+08:00","created_by":"David Ten","updated_at":"2026-02-06T12:58:35.630408+08:00","closed_at":"2026-02-06T12:58:35.630408+08:00","close_reason":"Scaffolded Vite + React + TypeScript frontend with Tailwind v4, React Query, React Router, Axios API client (11 endpoints), TypeScript types matching all Pydantic models, and dev proxy to backend. TypeScript compiles clean, dev server runs on port 3000.","dependencies":[{"issue_id":"MyCareersFuture-qwl.6.1","depends_on_id":"MyCareersFuture-qwl.6","type":"parent-child","created_at":"2026-02-05T18:44:27.861223+08:00","created_by":"David Ten"}]}
{"id":"MyCareersFuture-qwl.6.2","title":"Build core UI components (SearchBar, FilterPanel, JobCard)","description":"# Task: Build Core UI Components\n\n## What\nCreate all UI components for the search interface including:\n- SearchBar, FilterPanel, JobCard, JobList (core)\n- SimilarJobsModal (recommendations)\n- SkillCloud (skill visualization)\n- CompanySearch (company similarity)\n- SearchStats (analytics display)\n- DegradedBanner (error state)\n\n## Components to Create\n\n### 1. SearchBar.tsx\n```tsx\nimport { useState, KeyboardEvent } from 'react';\nimport { MagnifyingGlassIcon, XMarkIcon } from '@heroicons/react/24/outline';\n\ninterface SearchBarProps {\n  value: string;\n  onChange: (value: string) =\u003e void;\n  onSearch: () =\u003e void;\n  isLoading?: boolean;\n  placeholder?: string;\n}\n\nexport function SearchBar({ value, onChange, onSearch, isLoading, placeholder }: SearchBarProps) {\n  const handleKeyDown = (e: KeyboardEvent) =\u003e {\n    if (e.key === 'Enter' \u0026\u0026 !isLoading) {\n      onSearch();\n    }\n  };\n\n  return (\n    \u003cdiv className=\"relative flex items-center\"\u003e\n      \u003cMagnifyingGlassIcon className=\"absolute left-3 h-5 w-5 text-gray-400\" /\u003e\n      \u003cinput\n        type=\"text\"\n        value={value}\n        onChange={(e) =\u003e onChange(e.target.value)}\n        onKeyDown={handleKeyDown}\n        placeholder={placeholder || \"Search jobs semantically...\"}\n        className=\"w-full pl-10 pr-10 py-3 border border-gray-300 rounded-lg \n                   focus:ring-2 focus:ring-blue-500 focus:border-transparent\"\n        disabled={isLoading}\n      /\u003e\n      {value \u0026\u0026 (\n        \u003cbutton onClick={() =\u003e onChange('')} className=\"absolute right-14 text-gray-400\"\u003e\n          \u003cXMarkIcon className=\"h-5 w-5\" /\u003e\n        \u003c/button\u003e\n      )}\n      \u003cbutton\n        onClick={onSearch}\n        disabled={isLoading || !value.trim()}\n        className=\"ml-2 px-6 py-3 bg-blue-600 text-white rounded-lg \n                   hover:bg-blue-700 disabled:bg-gray-300 transition-colors\"\n      \u003e\n        {isLoading ? 'Searching...' : 'Search'}\n      \u003c/button\u003e\n    \u003c/div\u003e\n  );\n}\n```\n\n### 2. FilterPanel.tsx\n```tsx\ninterface FilterPanelProps {\n  filters: Filters;\n  onChange: (filters: Filters) =\u003e void;\n  onClear: () =\u003e void;\n}\n\nexport function FilterPanel({ filters, onChange, onClear }: FilterPanelProps) {\n  const updateFilter = \u003cK extends keyof Filters\u003e(key: K, value: Filters[K]) =\u003e {\n    onChange({ ...filters, [key]: value });\n  };\n\n  return (\n    \u003cdiv className=\"bg-white p-4 rounded-lg shadow-sm border space-y-4\"\u003e\n      \u003cdiv className=\"flex justify-between items-center\"\u003e\n        \u003ch3 className=\"font-semibold text-gray-700\"\u003eFilters\u003c/h3\u003e\n        \u003cbutton onClick={onClear} className=\"text-sm text-blue-600 hover:text-blue-800\"\u003e\n          Clear all\n        \u003c/button\u003e\n      \u003c/div\u003e\n\n      {/* Salary Range */}\n      \u003cdiv className=\"grid grid-cols-2 gap-2\"\u003e\n        \u003cdiv\u003e\n          \u003clabel className=\"block text-sm font-medium text-gray-600 mb-1\"\u003eMin Salary\u003c/label\u003e\n          \u003cinput\n            type=\"number\"\n            value={filters.salaryMin || ''}\n            onChange={(e) =\u003e updateFilter('salaryMin', e.target.value ? parseInt(e.target.value) : undefined)}\n            placeholder=\"e.g., 5000\"\n            className=\"w-full px-3 py-2 border rounded-md text-sm\"\n          /\u003e\n        \u003c/div\u003e\n        \u003cdiv\u003e\n          \u003clabel className=\"block text-sm font-medium text-gray-600 mb-1\"\u003eMax Salary\u003c/label\u003e\n          \u003cinput\n            type=\"number\"\n            value={filters.salaryMax || ''}\n            onChange={(e) =\u003e updateFilter('salaryMax', e.target.value ? parseInt(e.target.value) : undefined)}\n            className=\"w-full px-3 py-2 border rounded-md text-sm\"\n          /\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n\n      {/* Employment Type */}\n      \u003cdiv\u003e\n        \u003clabel className=\"block text-sm font-medium text-gray-600 mb-1\"\u003eEmployment Type\u003c/label\u003e\n        \u003cselect\n          value={filters.employmentType || ''}\n          onChange={(e) =\u003e updateFilter('employmentType', e.target.value || undefined)}\n          className=\"w-full px-3 py-2 border rounded-md text-sm\"\n        \u003e\n          \u003coption value=\"\"\u003eAll types\u003c/option\u003e\n          \u003coption value=\"Full Time\"\u003eFull Time\u003c/option\u003e\n          \u003coption value=\"Part Time\"\u003ePart Time\u003c/option\u003e\n          \u003coption value=\"Contract\"\u003eContract\u003c/option\u003e\n        \u003c/select\u003e\n      \u003c/div\u003e\n\n      {/* Company */}\n      \u003cdiv\u003e\n        \u003clabel className=\"block text-sm font-medium text-gray-600 mb-1\"\u003eCompany\u003c/label\u003e\n        \u003cinput\n          type=\"text\"\n          value={filters.company || ''}\n          onChange={(e) =\u003e updateFilter('company', e.target.value || undefined)}\n          placeholder=\"Company name\"\n          className=\"w-full px-3 py-2 border rounded-md text-sm\"\n        /\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  );\n}\n```\n\n### 3. JobCard.tsx\n```tsx\ninterface JobCardProps {\n  job: JobResult;\n  onFindSimilar: (uuid: string) =\u003e void;\n}\n\nexport function JobCard({ job, onFindSimilar }: JobCardProps) {\n  const formatSalary = () =\u003e {\n    if (job.salary_min \u0026\u0026 job.salary_max) {\n      return `$${job.salary_min.toLocaleString()} - $${job.salary_max.toLocaleString()}`;\n    } else if (job.salary_min) {\n      return `$${job.salary_min.toLocaleString()}+`;\n    }\n    return 'Salary not specified';\n  };\n\n  const skills = job.skills?.split(',').slice(0, 5) || [];\n  const similarityPercent = Math.round(job.similarity_score * 100);\n\n  return (\n    \u003cdiv className=\"bg-white p-4 rounded-lg shadow-sm border hover:shadow-md transition-shadow\"\u003e\n      \u003cdiv className=\"flex justify-between items-start mb-2\"\u003e\n        \u003cdiv className=\"flex-1\"\u003e\n          \u003ca href={job.job_url} target=\"_blank\" rel=\"noopener noreferrer\"\n             className=\"text-lg font-semibold text-blue-600 hover:underline\"\u003e\n            {job.title}\n          \u003c/a\u003e\n          \u003cp className=\"text-gray-600\"\u003e{job.company_name || 'Unknown Company'}\u003c/p\u003e\n        \u003c/div\u003e\n        \n        {/* Similarity Score Badge */}\n        \u003cdiv className=\"flex items-center space-x-1 bg-green-100 text-green-800 px-2 py-1 rounded-full text-sm\"\u003e\n          \u003cSparklesIcon className=\"h-4 w-4\" /\u003e\n          \u003cspan\u003e{similarityPercent}% match\u003c/span\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n\n      \u003cdiv className=\"text-sm text-gray-500 mb-3 space-y-1\"\u003e\n        \u003cp\u003e{formatSalary()}\u003c/p\u003e\n        {job.location \u0026\u0026 \u003cp\u003e{job.location}\u003c/p\u003e}\n      \u003c/div\u003e\n\n      {/* Skills */}\n      {skills.length \u003e 0 \u0026\u0026 (\n        \u003cdiv className=\"flex flex-wrap gap-1 mb-3\"\u003e\n          {skills.map((skill, i) =\u003e (\n            \u003cspan key={i} className=\"bg-gray-100 text-gray-700 px-2 py-0.5 rounded text-xs\"\u003e\n              {skill.trim()}\n            \u003c/span\u003e\n          ))}\n        \u003c/div\u003e\n      )}\n\n      \u003cp className=\"text-sm text-gray-600 line-clamp-2 mb-3\"\u003e{job.description}\u003c/p\u003e\n\n      \u003cdiv className=\"flex justify-end\"\u003e\n        \u003cbutton\n          onClick={() =\u003e onFindSimilar(job.uuid)}\n          className=\"text-sm text-blue-600 hover:text-blue-800 flex items-center space-x-1\"\n        \u003e\n          \u003cSparklesIcon className=\"h-4 w-4\" /\u003e\n          \u003cspan\u003eFind Similar\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  );\n}\n```\n\n### 4. SimilarJobsModal.tsx (NEW)\n```tsx\ninterface SimilarJobsModalProps {\n  isOpen: boolean;\n  onClose: () =\u003e void;\n  sourceJob: JobResult | null;\n  similarJobs: JobResult[];\n  isLoading: boolean;\n}\n\nexport function SimilarJobsModal({ isOpen, onClose, sourceJob, similarJobs, isLoading }: SimilarJobsModalProps) {\n  if (!isOpen) return null;\n\n  return (\n    \u003cdiv className=\"fixed inset-0 bg-black bg-opacity-50 flex items-center justify-center z-50\"\u003e\n      \u003cdiv className=\"bg-white rounded-lg shadow-xl max-w-2xl w-full max-h-[80vh] overflow-hidden\"\u003e\n        \u003cdiv className=\"p-4 border-b flex justify-between items-center\"\u003e\n          \u003cdiv\u003e\n            \u003ch2 className=\"text-lg font-semibold\"\u003eSimilar Jobs\u003c/h2\u003e\n            {sourceJob \u0026\u0026 (\n              \u003cp className=\"text-sm text-gray-500\"\u003eJobs similar to: {sourceJob.title}\u003c/p\u003e\n            )}\n          \u003c/div\u003e\n          \u003cbutton onClick={onClose} className=\"text-gray-400 hover:text-gray-600\"\u003e\n            \u003cXMarkIcon className=\"h-6 w-6\" /\u003e\n          \u003c/button\u003e\n        \u003c/div\u003e\n\n        \u003cdiv className=\"p-4 overflow-y-auto max-h-[60vh]\"\u003e\n          {isLoading ? (\n            \u003cdiv className=\"flex justify-center py-8\"\u003e\n              \u003cdiv className=\"animate-spin rounded-full h-8 w-8 border-b-2 border-blue-600\" /\u003e\n            \u003c/div\u003e\n          ) : similarJobs.length === 0 ? (\n            \u003cp className=\"text-center text-gray-500 py-8\"\u003eNo similar jobs found.\u003c/p\u003e\n          ) : (\n            \u003cdiv className=\"space-y-3\"\u003e\n              {similarJobs.map((job) =\u003e (\n                \u003cdiv key={job.uuid} className=\"p-3 border rounded-lg hover:bg-gray-50\"\u003e\n                  \u003cdiv className=\"flex justify-between items-start\"\u003e\n                    \u003cdiv\u003e\n                      \u003ca href={job.job_url} target=\"_blank\" rel=\"noopener noreferrer\"\n                         className=\"font-medium text-blue-600 hover:underline\"\u003e\n                        {job.title}\n                      \u003c/a\u003e\n                      \u003cp className=\"text-sm text-gray-500\"\u003e{job.company_name}\u003c/p\u003e\n                    \u003c/div\u003e\n                    \u003cspan className=\"text-sm text-green-600\"\u003e\n                      {Math.round(job.similarity_score * 100)}% match\n                    \u003c/span\u003e\n                  \u003c/div\u003e\n                \u003c/div\u003e\n              ))}\n            \u003c/div\u003e\n          )}\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  );\n}\n```\n\n### 5. SkillCloud.tsx (NEW)\n```tsx\ninterface SkillCloudItem {\n  skill: string;\n  job_count: number;\n  cluster_id?: number;\n}\n\ninterface SkillCloudProps {\n  skills: SkillCloudItem[];\n  onSkillClick: (skill: string) =\u003e void;\n  isLoading: boolean;\n}\n\n// Color palette for clusters\nconst CLUSTER_COLORS = [\n  'bg-blue-100 text-blue-800',\n  'bg-green-100 text-green-800',\n  'bg-purple-100 text-purple-800',\n  'bg-orange-100 text-orange-800',\n  'bg-pink-100 text-pink-800',\n  'bg-teal-100 text-teal-800',\n];\n\nexport function SkillCloud({ skills, onSkillClick, isLoading }: SkillCloudProps) {\n  if (isLoading) {\n    return \u003cdiv className=\"animate-pulse h-32 bg-gray-100 rounded-lg\" /\u003e;\n  }\n\n  // Calculate font sizes based on frequency\n  const maxCount = Math.max(...skills.map(s =\u003e s.job_count));\n  const minCount = Math.min(...skills.map(s =\u003e s.job_count));\n  \n  const getFontSize = (count: number) =\u003e {\n    const normalized = (count - minCount) / (maxCount - minCount || 1);\n    return 0.75 + normalized * 1; // 0.75rem to 1.75rem\n  };\n\n  const getColor = (clusterId?: number) =\u003e {\n    if (clusterId === undefined) return 'bg-gray-100 text-gray-700';\n    return CLUSTER_COLORS[clusterId % CLUSTER_COLORS.length];\n  };\n\n  return (\n    \u003cdiv className=\"bg-white p-4 rounded-lg shadow-sm border\"\u003e\n      \u003ch3 className=\"font-semibold text-gray-700 mb-3\"\u003ePopular Skills\u003c/h3\u003e\n      \u003cdiv className=\"flex flex-wrap gap-2\"\u003e\n        {skills.map((skill) =\u003e (\n          \u003cbutton\n            key={skill.skill}\n            onClick={() =\u003e onSkillClick(skill.skill)}\n            style={{ fontSize: `${getFontSize(skill.job_count)}rem` }}\n            className={`px-2 py-1 rounded-full hover:opacity-80 transition-opacity ${getColor(skill.cluster_id)}`}\n            title={`${skill.job_count} jobs`}\n          \u003e\n            {skill.skill}\n          \u003c/button\u003e\n        ))}\n      \u003c/div\u003e\n    \u003c/div\u003e\n  );\n}\n```\n\n### 6. CompanySearch.tsx (NEW)\n```tsx\ninterface CompanySimilarity {\n  company_name: string;\n  similarity_score: number;\n  job_count: number;\n  avg_salary?: number;\n  top_skills: string[];\n}\n\ninterface CompanySearchProps {\n  onSearch: (companyName: string) =\u003e void;\n  results: CompanySimilarity[];\n  isLoading: boolean;\n}\n\nexport function CompanySearch({ onSearch, results, isLoading }: CompanySearchProps) {\n  const [companyName, setCompanyName] = useState('');\n\n  const handleSearch = () =\u003e {\n    if (companyName.trim()) {\n      onSearch(companyName.trim());\n    }\n  };\n\n  return (\n    \u003cdiv className=\"bg-white p-4 rounded-lg shadow-sm border\"\u003e\n      \u003ch3 className=\"font-semibold text-gray-700 mb-3\"\u003eFind Similar Companies\u003c/h3\u003e\n      \n      \u003cdiv className=\"flex gap-2 mb-4\"\u003e\n        \u003cinput\n          type=\"text\"\n          value={companyName}\n          onChange={(e) =\u003e setCompanyName(e.target.value)}\n          onKeyDown={(e) =\u003e e.key === 'Enter' \u0026\u0026 handleSearch()}\n          placeholder=\"Enter company name...\"\n          className=\"flex-1 px-3 py-2 border rounded-md text-sm\"\n        /\u003e\n        \u003cbutton\n          onClick={handleSearch}\n          disabled={isLoading || !companyName.trim()}\n          className=\"px-4 py-2 bg-blue-600 text-white rounded-md text-sm disabled:bg-gray-300\"\n        \u003e\n          {isLoading ? '...' : 'Find'}\n        \u003c/button\u003e\n      \u003c/div\u003e\n\n      {results.length \u003e 0 \u0026\u0026 (\n        \u003cdiv className=\"space-y-2\"\u003e\n          {results.map((company) =\u003e (\n            \u003cdiv key={company.company_name} className=\"p-3 border rounded-lg\"\u003e\n              \u003cdiv className=\"flex justify-between items-start\"\u003e\n                \u003cdiv\u003e\n                  \u003cp className=\"font-medium\"\u003e{company.company_name}\u003c/p\u003e\n                  \u003cp className=\"text-sm text-gray-500\"\u003e\n                    {company.job_count} jobs\n                    {company.avg_salary \u0026\u0026 ` · Avg $${company.avg_salary.toLocaleString()}`}\n                  \u003c/p\u003e\n                \u003c/div\u003e\n                \u003cspan className=\"text-sm text-green-600\"\u003e\n                  {Math.round(company.similarity_score * 100)}% similar\n                \u003c/span\u003e\n              \u003c/div\u003e\n              {company.top_skills.length \u003e 0 \u0026\u0026 (\n                \u003cdiv className=\"mt-2 flex flex-wrap gap-1\"\u003e\n                  {company.top_skills.slice(0, 5).map((skill) =\u003e (\n                    \u003cspan key={skill} className=\"bg-gray-100 text-xs px-1.5 py-0.5 rounded\"\u003e\n                      {skill}\n                    \u003c/span\u003e\n                  ))}\n                \u003c/div\u003e\n              )}\n            \u003c/div\u003e\n          ))}\n        \u003c/div\u003e\n      )}\n    \u003c/div\u003e\n  );\n}\n```\n\n### 7. SearchStats.tsx (NEW)\n```tsx\ninterface SearchStatsProps {\n  totalCandidates: number;\n  searchTimeMs: number;\n  queryExpansion?: string[];\n  cacheHit: boolean;\n}\n\nexport function SearchStats({ totalCandidates, searchTimeMs, queryExpansion, cacheHit }: SearchStatsProps) {\n  return (\n    \u003cdiv className=\"text-sm text-gray-500 flex items-center gap-4\"\u003e\n      \u003cspan\u003e{totalCandidates.toLocaleString()} matching jobs\u003c/span\u003e\n      \u003cspan\u003e·\u003c/span\u003e\n      \u003cspan\u003e{searchTimeMs.toFixed(0)}ms\u003c/span\u003e\n      {cacheHit \u0026\u0026 (\n        \u003c\u003e\n          \u003cspan\u003e·\u003c/span\u003e\n          \u003cspan className=\"text-green-600\"\u003ecached\u003c/span\u003e\n        \u003c/\u003e\n      )}\n      {queryExpansion \u0026\u0026 queryExpansion.length \u003e 1 \u0026\u0026 (\n        \u003c\u003e\n          \u003cspan\u003e·\u003c/span\u003e\n          \u003cspan title={queryExpansion.join(', ')}\u003e\n            Expanded to {queryExpansion.length} terms\n          \u003c/span\u003e\n        \u003c/\u003e\n      )}\n    \u003c/div\u003e\n  );\n}\n```\n\n### 8. DegradedBanner.tsx\n```tsx\nexport function DegradedBanner({ show }: { show: boolean }) {\n  if (!show) return null;\n\n  return (\n    \u003cdiv className=\"bg-yellow-50 border-l-4 border-yellow-400 p-4 mb-4\"\u003e\n      \u003cdiv className=\"flex items-center\"\u003e\n        \u003cExclamationTriangleIcon className=\"h-5 w-5 text-yellow-400 mr-2\" /\u003e\n        \u003cp className=\"text-sm text-yellow-700\"\u003e\n          \u003cstrong\u003eLimited Mode:\u003c/strong\u003e Semantic search unavailable. Using keyword matching only.\n        \u003c/p\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  );\n}\n```\n\n## Main App Integration\n\n```tsx\n// App.tsx\nfunction App() {\n  const [query, setQuery] = useState('');\n  const [filters, setFilters] = useState\u003cFilters\u003e({});\n  const [results, setResults] = useState\u003cSearchResponse | null\u003e(null);\n  const [isLoading, setIsLoading] = useState(false);\n  const [similarModal, setSimilarModal] = useState\u003c{open: boolean, job: JobResult | null}\u003e({open: false, job: null});\n  const [similarJobs, setSimilarJobs] = useState\u003cJobResult[]\u003e([]);\n  const [skillCloud, setSkillCloud] = useState\u003cSkillCloudItem[]\u003e([]);\n\n  useEffect(() =\u003e {\n    // Load skill cloud on mount\n    fetch('/api/skills/cloud?limit=50')\n      .then(res =\u003e res.json())\n      .then(setSkillCloud);\n  }, []);\n\n  const handleSearch = async () =\u003e {\n    setIsLoading(true);\n    const res = await fetch('/api/search', {\n      method: 'POST',\n      headers: {'Content-Type': 'application/json'},\n      body: JSON.stringify({ query, ...filters })\n    });\n    setResults(await res.json());\n    setIsLoading(false);\n  };\n\n  const handleFindSimilar = async (uuid: string) =\u003e {\n    const job = results?.results.find(j =\u003e j.uuid === uuid);\n    setSimilarModal({open: true, job: job || null});\n    \n    const res = await fetch('/api/similar', {\n      method: 'POST',\n      headers: {'Content-Type': 'application/json'},\n      body: JSON.stringify({ job_uuid: uuid, limit: 10 })\n    });\n    const data = await res.json();\n    setSimilarJobs(data.results);\n  };\n\n  return (\n    \u003cdiv className=\"min-h-screen bg-gray-50\"\u003e\n      \u003cdiv className=\"max-w-6xl mx-auto p-4\"\u003e\n        \u003ch1 className=\"text-2xl font-bold mb-6\"\u003eMCF Job Search\u003c/h1\u003e\n        \n        \u003cDegradedBanner show={results?.degraded || false} /\u003e\n        \n        \u003cSearchBar value={query} onChange={setQuery} onSearch={handleSearch} isLoading={isLoading} /\u003e\n        \n        \u003cdiv className=\"mt-4 grid grid-cols-4 gap-4\"\u003e\n          \u003cdiv className=\"col-span-1 space-y-4\"\u003e\n            \u003cFilterPanel filters={filters} onChange={setFilters} onClear={() =\u003e setFilters({})} /\u003e\n            \u003cSkillCloud skills={skillCloud} onSkillClick={(s) =\u003e {setQuery(s); handleSearch();}} isLoading={false} /\u003e\n          \u003c/div\u003e\n          \n          \u003cdiv className=\"col-span-3\"\u003e\n            {results \u0026\u0026 \u003cSearchStats {...results} /\u003e}\n            \u003cJobList jobs={results?.results || []} isLoading={isLoading} onFindSimilar={handleFindSimilar} /\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n        \n        \u003cSimilarJobsModal\n          isOpen={similarModal.open}\n          onClose={() =\u003e setSimilarModal({open: false, job: null})}\n          sourceJob={similarModal.job}\n          similarJobs={similarJobs}\n          isLoading={false}\n        /\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  );\n}\n```\n\n## File Structure\n```\nsrc/frontend/src/\n├── components/\n│   ├── SearchBar.tsx\n│   ├── FilterPanel.tsx\n│   ├── JobCard.tsx\n│   ├── JobList.tsx\n│   ├── SimilarJobsModal.tsx\n│   ├── SkillCloud.tsx\n│   ├── CompanySearch.tsx\n│   ├── SearchStats.tsx\n│   └── DegradedBanner.tsx\n├── types/\n│   └── index.ts\n├── hooks/\n│   └── useSearch.ts\n├── App.tsx\n└── main.tsx\n```\n\n## Testing\n1. Start API: `mcf api-serve`\n2. Start frontend: `cd src/frontend \u0026\u0026 npm run dev`\n3. Test all components interactively\n4. Test responsive layout\n5. Test error states and loading states","status":"closed","priority":2,"issue_type":"task","owner":"davidten7@gmail.com","created_at":"2026-02-05T18:45:17.889406+08:00","created_by":"David Ten","updated_at":"2026-02-06T14:20:33.440809+08:00","closed_at":"2026-02-06T14:20:33.440809+08:00","close_reason":"All 13 files implemented: types/api.ts (Filters interface), 9 components (DegradedBanner, SearchStats, SearchBar, SkeletonCard, JobCard, FilterPanel, SkillCloud, JobList, SimilarJobsModal, CompanySearch), useSearch hook, App.tsx rewrite. Also fixed erasableSyntaxOnly error in api.ts. tsc, lint, and build all pass clean.","dependencies":[{"issue_id":"MyCareersFuture-qwl.6.2","depends_on_id":"MyCareersFuture-qwl.6","type":"parent-child","created_at":"2026-02-05T18:45:17.890972+08:00","created_by":"David Ten"},{"issue_id":"MyCareersFuture-qwl.6.2","depends_on_id":"MyCareersFuture-qwl.6.1","type":"blocks","created_at":"2026-02-05T18:45:26.785329+08:00","created_by":"David Ten"}]}
